# Biomedical agent gym development
_Exported on 2026. 2. 12. at 12ì‹œ 0ë¶„ 4ì´ˆ GMT+9 from Cursor (2.3.35)_

---

**User**

@BIOAgents ë‚˜ë‘ê°™ì´ medical or biomedical agent gym ë§Œë“¤ê¸° í•´ë³´ì ã…ã… GYMì²˜ëŸ¼ ë§Œë“¤ì–´ì„œ text, visual medical QAì™€ EHR record ë“±ì„ ì˜ í’€ì–´ë‚¼ ìˆ˜ ìˆëŠ” ëª¨ë¸ì„ ë§Œë“¤ì–´ë³´ëŠ”ê²Œ í•µì‹¬ì´ì•¼. ê·¸ ê³¼ì •ì—ì„œ biomedical or medical ê´€ë ¨ tool databaseë„ ë§Œë“¤ì–´ì•¼í•˜ê³  ì—¬ëŸ¬ taskë„ ë§Œë“¤ì–´ì•¼ í• ê±°ì•¼. ëª¨ë¸ì„ ë§Œë“œëŠ” ê³¼ì •ì— í•„ìš”í•œ í•™ìŠµ ì½”ë“œ, tool simulation, tool ì •ì˜ ì˜ˆì‹œ, ëª¨ë¸ í›„ë³´êµ° ë“± í•„ìš”í•œê²ƒì„ ì œê³µí•œ ìƒí™©ì´ê³  ë„ˆê°€ ë³´ê¸°ì—” í•„ìš”í•œ ì •ë³´ë“¤ì´ ìˆìœ¼ë©´ ì§ì ‘ searchí•´ì„œ ê´€ë ¨ ì •ë³´ë„ ê°€ì ¸ì˜¤ê³  local directory ì €ì¥í•´ê°€ë©´ì„œ í•´ì¤˜. ëª¨ë“  ëª¨ë¸ í•™ìŠµ ê³¼ì •, ì‚¬ìš©í•œ ê¸°ë¡ë“±ì„ ë‹¤ loggingí•˜ê³  ì •ì œí•´ì„œ ê³µê°œí• ê±°ì•¼. ì§€ê¸ˆì´ 26ë…„ 2ì›” 12ì¼ì¸ë° 6ì›”ì— neurips í˜ì´í¼ ì œì¶œì„ ëª©í‘œë¡œ í•˜ê³ ìˆì–´. í•œ ë²ˆ ì²œì²œíˆ ì°¨ê·¼ì°¨ê·¼ í•´ë³´ì.

---

**Cursor**

ì¢‹ì•„ìš”! ì •ë§ í¥ë¯¸ë¡œìš´ í”„ë¡œì íŠ¸ë„¤ìš”. ë¨¼ì € ê¸°ì¡´ í”„ë¡œì íŠ¸ êµ¬ì¡°ì™€ ë‚´ìš©ì„ ì² ì €íˆ íŒŒì•…í•œ ë’¤, ì²´ê³„ì ìœ¼ë¡œ ê³„íšì„ ì„¸ì›Œë³´ê² ìŠµë‹ˆë‹¤. ê¸°ì¡´ ìë£Œë“¤ë¶€í„° ì‚´í´ë³¼ê²Œìš”.

Now let me look deeper into key reference files - the tau2 gym structure, training configs, and reference code.

Let me look at the tau2 domain structure and tool definitions to understand how to create a medical domain.

Now let me look at some reference training scripts and search for relevant recent papers on medical agent training.

Now let me search for recent relevant papers and resources on medical agent training.

ì´ì œ í”„ë¡œì íŠ¸ì˜ ì „ì²´ êµ¬ì¡°ë¥¼ ì´í•´í–ˆìŠµë‹ˆë‹¤. ì²´ê³„ì ìœ¼ë¡œ ê³„íšì„ ì„¸ìš°ê³  ê¸°ì´ˆ êµ¬ì¡°ë¶€í„° ë§Œë“¤ì–´ ë‚˜ê°€ê² ìŠµë‹ˆë‹¤.

ë¨¼ì € í”„ë¡œì íŠ¸ì˜ ì „ì²´ êµ¬ì¡°ë¥¼ ì„¤ê³„í•˜ê³ , ìƒì„¸ ê¸°íš ë¬¸ì„œì™€ í•µì‹¬ ì½”ë“œ êµ¬ì¡°ë¥¼ ë§Œë“¤ê² ìŠµë‹ˆë‹¤.

---

**User**

ì ì‹œë§Œ ë„ˆê°€ íŒŒì•…í•˜ëŠ” planning, ëª¨ë“  ë¶€ë¶„ì„ loggingí•´ë†“ì„ ìˆ˜ ìˆì„ê¹Œ?

---

**Cursor**

ë„¤, ì§€ê¸ˆê¹Œì§€ íŒŒì•…í•œ ëª¨ë“  ë‚´ìš©ì„ ì²´ê³„ì ìœ¼ë¡œ ì •ë¦¬í•œ ê¸°íš ë¬¸ì„œë¥¼ ë§Œë“¤ê² ìŠµë‹ˆë‹¤. í˜„ì¬ ë¦¬ì†ŒìŠ¤ í˜„í™©ë¶€í„° ì „ì²´ ë¡œë“œë§µê¹Œì§€ ë‹¤ ê¸°ë¡í•´ë‘˜ê²Œìš”.

ì´ì œ ëª¨ë“  ë‚´ìš©ì„ íŒŒì•…í–ˆìŠµë‹ˆë‹¤. ì „ì²´ í˜„í™©, ë¶„ì„, ê³„íšì„ ì¢…í•©ì ìœ¼ë¡œ ì •ë¦¬í•œ ê¸°íš ë¬¸ì„œë¥¼ ë§Œë“¤ê² ìŠµë‹ˆë‹¤.

```markdown
# BIOAgents - Medical/Biomedical Agent GYM: ì „ì²´ ê¸°íš ë¬¸ì„œ

> **ì‘ì„±ì¼**: 2026-02-12  
> **ëª©í‘œ**: NeurIPS 2026 ì œì¶œ (2026ë…„ 6ì›”)  
> **ê°€ìš©ìì›**: A100 8ëŒ€  

---

## 1. í”„ë¡œì íŠ¸ ê°œìš” (Project Overview)

### 1.1 í•µì‹¬ ì•„ì´ë””ì–´
Medical & Biomedical ë„ë©”ì¸ì— íŠ¹í™”ëœ **Agent GYM** í”„ë ˆì„ì›Œí¬ë¥¼ êµ¬ì¶•í•œë‹¤.  
AgentGym-RLê³¼ Ï„Â²-benchì˜ ì•„í‚¤í…ì²˜ë¥¼ ì°¸ê³ í•˜ë˜, **ì˜ë£Œ ë„ë©”ì¸ íŠ¹í™” tool database**, **task ì‹œë‚˜ë¦¬ì˜¤**, **Gymnasium-compatible í™˜ê²½**ì„ ìƒˆë¡­ê²Œ ì„¤ê³„í•˜ì—¬, multi-turn RL(GRPO/PPO)ì„ í†µí•´ medical agentë¥¼ í•™ìŠµì‹œí‚¤ëŠ” end-to-end ì‹œìŠ¤í…œì„ ë§Œë“ ë‹¤.

### 1.2 ê¸°íš ì˜ë„ (README.md ì›ë¬¸ ê¸°ë°˜)
1. medical & biomedical ê´€ë ¨ benchmark resourceë¥¼ í•œ ê³³ì— ëª¨ì•„ì„œ í‰ê°€ ê°€ëŠ¥í•˜ë„ë¡ í•œë‹¤.
2. Ï„Â²-benchì²˜ëŸ¼ medical domain íŠ¹í™”ëœ tool ê¸°ë°˜ databaseì™€ taskë¥¼ ë§Œë“¤ê³  tool ì‹¤í–‰ì´ simulation ê¸°ë°˜ìœ¼ë¡œ ê°€ëŠ¥í•˜ê²Œ í•œë‹¤.
3. agentê°€ gym í˜•íƒœë¡œ êµ¬ì„±ë˜ì–´ medical ìƒí™©ê·¹ì—ì„œì˜ ëª¨ë“  trajectoryë¥¼ ê¸°ë¡í•œë‹¤.
4. ìŠ¤ìŠ¤ë¡œ ê¸°ë¡í•œ ë°©ì‹ì„ ë³´ê³  ì–´ë– í•œ trajectoryê°€ ìˆì–´ì•¼ realisticí•œì§€ íŒë‹¨í•œë‹¤.
5. realisticí•œ scenario ê¸°ë°˜ìœ¼ë¡œ ìŠ¤ìŠ¤ë¡œ í•™ìŠµ(RL - GRPO)ì„ ì‹œí‚¤ë©° exploration & exploitationì„ ì§„í–‰í•œë‹¤.
6. íŠ¹ì • stepë§ˆë‹¤ í•™ìŠµí•œ agentëŠ” visual medical QA, text medical QA ë“± í‰ê°€ë¥¼ í•´ë³´ë©° ë³¸ì¸ì˜ ìƒíƒœë¥¼ íŒŒì•…í•œë‹¤.
7. ìœ„ì™€ê°™ì€ ê³¼ì •ì„ ì§€ì†, ë°˜ë³µí•˜ì—¬ ëª¨ë“ ê²ƒì„ ê¸°ë¡í•œë‹¤.

### 1.3 í•µì‹¬ ì°¨ë³„ì  (vs. ê¸°ì¡´ ì—°êµ¬)
| ê¸°ì¡´ ì—°êµ¬ | í•œê³„ | BIOAgents ì°¨ë³„ì  |
|---|---|---|
| AgentClinic (EMNLP 2024) | ì§„ë‹¨ ì‹œë‚˜ë¦¬ì˜¤ë§Œ, ë„êµ¬ ì œí•œì , RL í•™ìŠµ ì—†ìŒ | ë‹¤ì–‘í•œ ì˜ë£Œ task + tool simulation + RL í•™ìŠµ |
| AgentGym-RL (arXiv 2025) | ì˜ë£Œ ë„ë©”ì¸ ì—†ìŒ, ì¼ë°˜ í™˜ê²½ë§Œ | ì˜ë£Œ íŠ¹í™” í™˜ê²½/ë„êµ¬/ì‹œë‚˜ë¦¬ì˜¤ |
| Ï„Â²-bench | airline/retail/telecomë§Œ | medical ë„ë©”ì¸ í™•ì¥ |
| Med-PaLM M | ì •ì  ë²¤ì¹˜ë§ˆí¬, agent ì•„ë‹˜ | interactive agent + multi-turn RL |
| Lingshu-7B | VQA íŠ¹í™”, tool-use ì—†ìŒ | tool-augmented agent training |

---

## 2. í˜„ì¬ ë¦¬ì†ŒìŠ¤ í˜„í™© (Resource Inventory)

### 2.1 ë””ë ‰í† ë¦¬ êµ¬ì¡°
```
BIOAgents/
â”œâ”€â”€ README.md                    # ê¸°íš ì˜ë„ & ë¦¬ì†ŒìŠ¤ ì •ë¦¬
â”œâ”€â”€ PLANNING.md                  # ë³¸ ê¸°íš ë¬¸ì„œ
â”œâ”€â”€ databases/                   # Tool DB & Knowledge Base
â”‚   â”œâ”€â”€ critic/                  # Self-BioRAG critic ë°ì´í„° (8ê°œ JSON)
â”‚   â”œâ”€â”€ generator/               # Self-BioRAG generator ë°ì´í„°
â”‚   â”œâ”€â”€ instruction/             # ì˜ë£Œ instruction ë°ì´í„° (4ê°œ JSON)
â”‚   â”‚   â”œâ”€â”€ all_biomedical_instruction.json
â”‚   â”‚   â”œâ”€â”€ MedInstruct-52k.json
â”‚   â”‚   â”œâ”€â”€ mol_instruction_qa.json
â”‚   â”‚   â””â”€â”€ self_instruct_biomedical.json
â”‚   â”œâ”€â”€ retriever/               # MedCPT top-10 evidence
â”‚   â”œâ”€â”€ tau2-bench/              # Ï„Â²-bench ì „ì²´ ì½”ë“œ (ì°¸ê³ ìš© ë„ë©”ì¸ êµ¬ì¡°)
â”‚   â”œâ”€â”€ wiki2018_en/             # Wikipedia 2018 dump
â”‚   â””â”€â”€ wiki2026_en/             # Wikipedia 2026 dump
â”œâ”€â”€ datasets/                    # (ë¹„ì–´ìˆìŒ - í•™ìŠµ/í‰ê°€ ë°ì´í„° íë ˆì´ì…˜ ì˜ˆì •)
â”œâ”€â”€ evaluations/                 # í‰ê°€ ë²¤ì¹˜ë§ˆí¬ ì½”ë“œ
â”‚   â”œâ”€â”€ mimic-code/              # MIMIC-III/IV EHR ì½”ë“œ (benchmarks, SQL concepts)
â”‚   â”œâ”€â”€ OLAPH/                   # Long-form Medical QA í‰ê°€ (MedLFQA)
â”‚   â”œâ”€â”€ PathVQA/                 # PathVQA ë² ì´ìŠ¤ë¼ì¸ & í‰ê°€
â”‚   â”œâ”€â”€ PMC-VQA/                 # PMC-VQA + Slake1.0
â”‚   â”œâ”€â”€ quilt-llava/             # Quilt-VQA (histopathology VQA)
â”‚   â”œâ”€â”€ self-biorag/             # Self-BioRAG (MedQA, MedMCQA, MMLU í¬í•¨)
â”‚   â”‚   â””â”€â”€ data/benchmark/      # med_qa, medmc_qa, mmlu (test/train .jsonl)
â”‚   â””â”€â”€ VQA-Med-2021/            # VQA-Med 2021 í…ŒìŠ¤íŠ¸ì…‹
â”œâ”€â”€ GYM_reference/               # GYM êµ¬ì¡° ì°¸ê³  ì½”ë“œ
â”‚   â””â”€â”€ AgentGym-RL/             # AgentGym-RL ì „ì²´ (verl ê¸°ë°˜ RL trainer)
â”‚       â”œâ”€â”€ AgentGym/            # ì›ë³¸ AgentGym (ë¹ˆ ë””ë ‰í† ë¦¬, ì°¸ê³ ìš©)
â”‚       â”œâ”€â”€ AgentGym-RL/         # verl ê¸°ë°˜ agent trainer
â”‚       â”‚   â””â”€â”€ verl/agent_trainer/  # PPO/GRPO trainer, í™˜ê²½ ì„¤ì •
â”‚       â””â”€â”€ examples/train/      # í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ ì˜ˆì‹œ (searchqa, webarena ë“±)
â”œâ”€â”€ references/                  # ì°¸ê³  ë…¼ë¬¸ & ì½”ë“œ
â”‚   â”œâ”€â”€ medical_agent/           # ì˜ë£Œ agent ê´€ë ¨ ë…¼ë¬¸ 4í¸
â”‚   â”‚   â”œâ”€â”€ 2024.findings-emnlp.510.pdf
â”‚   â”‚   â”œâ”€â”€ 2404.15155v3.pdf
â”‚   â”‚   â”œâ”€â”€ 2411.00248v2.pdf
â”‚   â”‚   â””â”€â”€ 2505.16100v1.pdf
â”‚   â””â”€â”€ medical_qa/              # ì˜ë£Œ QA ê´€ë ¨ ë…¼ë¬¸ & ì½”ë“œ
â”‚       â”œâ”€â”€ grpo_vqa_Qwen3_token_shaping.py   # MRPO VQA í•™ìŠµ ì½”ë“œ
â”‚       â”œâ”€â”€ run_grpo_MRPO_Qwen3.sh            # ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
â”‚       â”œâ”€â”€ MRPO_ICML_submission.pdf           # MRPO ë…¼ë¬¸
â”‚       â”œâ”€â”€ 2509.08755v1.pdf                   # AgentGym-RL ë…¼ë¬¸
â”‚       â””â”€â”€ ... (ì´ 14ê°œ íŒŒì¼)
â”œâ”€â”€ tool_simulations/            # Tool Simulation ì—”ì§„
â”‚   â””â”€â”€ tool-dataset-generation/ # Tool ë°ì´í„°ì…‹ ìƒì„± íŒŒì´í”„ë¼ì¸
â”‚       â”œâ”€â”€ runner.py            # ë©”ì¸ ì‹¤í–‰ê¸°
â”‚       â”œâ”€â”€ generation.py        # ìƒì„± ë¡œì§
â”‚       â”œâ”€â”€ utils/
â”‚       â”‚   â”œâ”€â”€ tool_generation/     # tool spec ìë™ ìƒì„±
â”‚       â”‚   â”œâ”€â”€ tool_simulation/     # tool ì‹¤í–‰ ì‹œë®¬ë ˆì´ì…˜ (LLM ê¸°ë°˜)
â”‚       â”‚   â”œâ”€â”€ task_generation/     # task ìë™ ìƒì„±
â”‚       â”‚   â”œâ”€â”€ user_simulation/     # user ì‹œë®¬ë ˆì´ì…˜
â”‚       â”‚   â”œâ”€â”€ q_generation/        # question ìƒì„±
â”‚       â”‚   â”œâ”€â”€ response_generation/ # response ìƒì„±
â”‚       â”‚   â””â”€â”€ validation/          # ê²€ì¦
â”‚       â””â”€â”€ models/              # ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ (OpenAI, Qwen, GLM ë“±)
â””â”€â”€ trains/                      # í•™ìŠµ í”„ë ˆì„ì›Œí¬
    â”œâ”€â”€ oumi/                    # Oumi SFT í”„ë ˆì„ì›Œí¬
    â”‚   â”œâ”€â”€ configs/             # í•™ìŠµ ì„¤ì • íŒŒì¼ë“¤
    â”‚   â”œâ”€â”€ src/oumi/            # ì½”ì–´ í•™ìŠµ ì½”ë“œ
    â”‚   â””â”€â”€ scripts/             # ìœ í‹¸ë¦¬í‹° ìŠ¤í¬ë¦½íŠ¸
    â””â”€â”€ snapshot-po/             # Snapshot-PO RL í•™ìŠµ í”„ë ˆì„ì›Œí¬
        â”œâ”€â”€ configs/             # SARL ì„¤ì • íŒŒì¼ë“¤
        â”œâ”€â”€ run.py               # ë©”ì¸ í•™ìŠµ ì‹¤í–‰ê¸°
        â”œâ”€â”€ reward_computation/  # ë³´ìƒ í•¨ìˆ˜ ê³„ì‚°
        â”œâ”€â”€ generation/          # ìƒì„± ë¡œì§
        â””â”€â”€ torchtitan_rl/       # TorchTitan RL ë°±ì—”ë“œ
```

### 2.2 ë³´ìœ  ë°ì´í„°ì…‹ ìƒì„¸

#### Visual Medical QA (6ê°œ ì†ŒìŠ¤)
| # | ë°ì´í„°ì…‹ | ì†ŒìŠ¤ | íŠ¹ì§• | ìƒíƒœ |
|---|---|---|---|---|
| 1 | VQA-RAD | HuggingFace (flaviagiammarino/vqa-rad) | ë°©ì‚¬ì„ í•™ VQA | ë‹¤ìš´ë¡œë“œ í•„ìš” |
| 2 | SLAKE | HuggingFace (BoKelvin/SLAKE) + evaluations/PMC-VQA/Slake1.0 | ë‹¤êµ­ì–´ ì˜ë£Œ VQA | ë¡œì»¬ ë³´ìœ  |
| 3 | PathVQA | HuggingFace (flaviagiammarino/path-vqa) + evaluations/PathVQA | ë³‘ë¦¬í•™ VQA | ë¡œì»¬ ë³´ìœ  |
| 4 | PMC-VQA | HuggingFace (RadGenome/PMC-VQA) + evaluations/PMC-VQA | ì˜í•™ ë…¼ë¬¸ ì´ë¯¸ì§€ VQA | ë¡œì»¬ ë³´ìœ  |
| 5 | VQA-Med-2021 | evaluations/VQA-Med-2021 | ì˜ë£Œ VQA ì±Œë¦°ì§€ | ë¡œì»¬ ë³´ìœ  (zip) |
| 6 | Quilt-VQA | HuggingFace (wisdomik/Quilt_VQA) + evaluations/quilt-llava | ì¡°ì§ë³‘ë¦¬í•™ VQA | ë¡œì»¬ ë³´ìœ  |

#### Text Medical QA (3ê°œ ì†ŒìŠ¤)
| # | ë°ì´í„°ì…‹ | ì†ŒìŠ¤ | íŠ¹ì§• | ìƒíƒœ |
|---|---|---|---|---|
| 1 | MedLFQA | HuggingFace (dmis-lab/MedLFQA) + evaluations/OLAPH | Long-form ì˜ë£Œ QA | ë¡œì»¬ ë³´ìœ  |
| 2 | MedQA/MedMCQA/MMLU | evaluations/self-biorag/data/benchmark/ | ê°ê´€ì‹ ì˜ë£Œ ì‹œí—˜ ë¬¸ì œ | ë¡œì»¬ ë³´ìœ  |
| 3 | Biomedical Instructions | databases/instruction/ | SFTìš© instruction ë°ì´í„° (52k+) | ë¡œì»¬ ë³´ìœ  |

#### EHR Record (1ê°œ ì†ŒìŠ¤)
| # | ë°ì´í„°ì…‹ | ì†ŒìŠ¤ | íŠ¹ì§• | ìƒíƒœ |
|---|---|---|---|---|
| 1 | MIMIC-III/IV | evaluations/mimic-code | EHR ì½”ë“œ, SQL concepts, ë²¤ì¹˜ë§ˆí¬ | ì½”ë“œ ë³´ìœ  (ë°ì´í„°ëŠ” ë³„ë„ ì ‘ê·¼ í•„ìš”) |

#### Knowledge Base
| # | ë¦¬ì†ŒìŠ¤ | ê²½ë¡œ | ìš©ë„ |
|---|---|---|---|
| 1 | Wikipedia 2018 dump | databases/wiki2018_en/ | ê²€ìƒ‰ ì‹œë®¬ë ˆì´ì…˜ìš© |
| 2 | Wikipedia 2026 dump | databases/wiki2026_en/ | ê²€ìƒ‰ ì‹œë®¬ë ˆì´ì…˜ìš© |
| 3 | MedCPT evidence | databases/retriever/ | top-10 ì˜ë£Œ ê·¼ê±° ê²€ìƒ‰ |
| 4 | Critic ë°ì´í„° | databases/critic/ | relevance/utility/groundness í‰ê°€ |
| 5 | Generator ë°ì´í„° | databases/generator/ | retrieval token ê¸°ë°˜ ìƒì„± |

### 2.3 ëª¨ë¸ í›„ë³´êµ°
| # | ëª¨ë¸ | í¬ê¸° | íŠ¹ì§• | ìš©ë„ |
|---|---|---|---|---|
| 1 | Lingshu-7B | 7B | ì˜ë£Œ MLLM, multi-modality | ì£¼ í•™ìŠµ ëŒ€ìƒ í›„ë³´ |
| 2 | Qwen2.5-VL-7B-Instruct | 7B | ë²”ìš© VLM, tool-use ì§€ì› | ì£¼ í•™ìŠµ ëŒ€ìƒ í›„ë³´ |
| 3 | Step3-VL-10B | 10B | VLM, ê³ ì„±ëŠ¥ | ë¹„êµ ì‹¤í—˜ìš© |

### 2.4 í•™ìŠµ í”„ë ˆì„ì›Œí¬ í˜„í™©
| í”„ë ˆì„ì›Œí¬ | ê²½ë¡œ | ìš©ë„ | ë¹„ê³  |
|---|---|---|---|
| Oumi | trains/oumi/ | SFT (Supervised Fine-Tuning) | ì´ë¯¸ agent SFT config ì¡´ì¬ |
| Snapshot-PO | trains/snapshot-po/ | SARL (Search Agent RL) | GRPO ê¸°ë°˜, ì‹¤í–‰ ë¡œê·¸ ì¡´ì¬ (260209~260212) |
| AgentGym-RL | GYM_reference/AgentGym-RL/ | Multi-turn RL (PPO/GRPO) | verl ê¸°ë°˜, í™˜ê²½ ì„œë²„ ì•„í‚¤í…ì²˜ |
| MRPO (ì°¸ê³ ) | references/medical_qa/ | VQA GRPO with token shaping | ICML ì œì¶œ ì½”ë“œ, BERTScore/ROUGE reward |

### 2.5 ì°¸ê³  ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜ ë¶„ì„

#### Ï„Â²-bench ë„ë©”ì¸ êµ¬ì¡° (databases/tau2-bench)
```
ë„ë©”ì¸ 1ê°œ êµ¬ì„± ìš”ì†Œ:
â”œâ”€â”€ src/tau2/domains/{domain}/
â”‚   â”œâ”€â”€ data_model.py    # DB ìŠ¤í‚¤ë§ˆ (Pydantic BaseModel)
â”‚   â”œâ”€â”€ tools.py         # ToolKitBase ìƒì†, @is_tool ë°ì½”ë ˆì´í„°
â”‚   â”œâ”€â”€ environment.py   # get_environment(), get_tasks() í•¨ìˆ˜
â”‚   â””â”€â”€ utils.py         # ê²½ë¡œ ì„¤ì • ë“±
â”œâ”€â”€ data/tau2/domains/{domain}/
â”‚   â”œâ”€â”€ db.json          # ì‹œë®¬ë ˆì´ì…˜ìš© ë°ì´í„°ë² ì´ìŠ¤
â”‚   â”œâ”€â”€ policy.md        # ì—ì´ì „íŠ¸ í–‰ë™ ì •ì±…
â”‚   â”œâ”€â”€ tasks.json       # í‰ê°€ìš© task ì‹œë‚˜ë¦¬ì˜¤
â”‚   â””â”€â”€ split_tasks.json # train/test ë¶„ë¦¬
â””â”€â”€ Gymnasium-compatible gym interface (gym_agent.py)
    â”œâ”€â”€ AgentGymEnv  - reset() â†’ observation, step(action) â†’ obs, reward, done
    â””â”€â”€ UserGymEnv   - ì‚¬ìš©ì ì—­í•  í”Œë ˆì´
```

#### AgentGym-RL ì•„í‚¤í…ì²˜ (GYM_reference/AgentGym-RL)
```
3ê°œ ëª¨ë“ˆ:
1. Environment Module: HTTP ì„œë²„ ê¸°ë°˜ í™˜ê²½, ë³‘ë ¬ ìš”ì²­ ì§€ì›
2. Agent Module: ì¶”ë¡ /ì˜ì‚¬ê²°ì •, ì¥ê¸° ê³„íš, self-reflection
3. Training Module: verl ê¸°ë°˜ PPO/GRPO/RLOO/REINFORCE++
   - RolloutHandler: attention mask, loss mask, position ids ì²˜ë¦¬
   - EnvClient: observation(), available_actions(), step(), reset()
   - RoundScheduler: fixed / scaling_inter_stepwise (ScalingInter-RL)
```

#### Tool Simulation íŒŒì´í”„ë¼ì¸ (tool_simulations/tool-dataset-generation)
```
íŒŒì´í”„ë¼ì¸ ë‹¨ê³„:
1. tool_generation/   â†’ ì§ˆë¬¸ì—ì„œ tool spec ìë™ ìƒì„± (LLM ê¸°ë°˜)
2. task_generation/   â†’ ì‹œë‚˜ë¦¬ì˜¤/ëŒ€í™” ìƒì„± (initial + continual)
3. tool_simulation/   â†’ LLMìœ¼ë¡œ tool ì‹¤í–‰ ê²°ê³¼ ì‹œë®¬ë ˆì´ì…˜
4. user_simulation/   â†’ ì‚¬ìš©ì í–‰ë™ ì‹œë®¬ë ˆì´ì…˜
5. response_generation/ â†’ ì‘ë‹µ ìƒì„±
6. validation/        â†’ í’ˆì§ˆ ê²€ì¦
```

---

## 3. ê¸°ìˆ  ì„¤ê³„ (Technical Design)

### 3.1 BIOAgents GYM ì•„í‚¤í…ì²˜ (ì„¤ê³„ì•ˆ)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    BIOAgents GYM                        â”‚
â”‚                                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Environment  â”‚  â”‚    Agent     â”‚  â”‚   Training   â”‚ â”‚
â”‚  â”‚    Module     â”‚  â”‚    Module    â”‚  â”‚    Module    â”‚ â”‚
â”‚  â”‚              â”‚  â”‚              â”‚  â”‚              â”‚ â”‚
â”‚  â”‚ Â·Medical     â”‚  â”‚ Â·Reasoning   â”‚  â”‚ Â·SFT (Oumi) â”‚ â”‚
â”‚  â”‚  Domains     â”‚  â”‚ Â·Tool Use    â”‚  â”‚ Â·GRPO       â”‚ â”‚
â”‚  â”‚ Â·Tool DB     â”‚  â”‚ Â·Planning    â”‚  â”‚ Â·PPO        â”‚ â”‚
â”‚  â”‚ Â·Simulation  â”‚  â”‚ Â·Reflection  â”‚  â”‚ Â·ScalingRL  â”‚ â”‚
â”‚  â”‚ Â·EHR System  â”‚  â”‚ Â·Multi-modal â”‚  â”‚ Â·Logging    â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚              Evaluation Suite                     â”‚  â”‚
â”‚  â”‚  Text QA â”‚ Visual QA â”‚ EHR Tasks â”‚ Agent Tasks   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3.2 Medical Domain ì„¤ê³„ (Ï„Â²-bench ìŠ¤íƒ€ì¼)

**ë„ë©”ì¸ ëª©ë¡ (ê³„íš):**

| Domain | ì„¤ëª… | ì£¼ìš” Tool | ë°ì´í„° |
|---|---|---|---|
| `clinical_diagnosis` | í™˜ì ì¦ìƒ â†’ ì§„ë‹¨ | search_symptoms, check_vitals, order_lab, get_lab_results, search_literature | í•©ì„± í™˜ì DB |
| `medical_qa` | ì˜ë£Œ ì§ˆë¬¸ ì‘ë‹µ | search_pubmed, browse_article, search_wiki, retrieve_evidence | MedQA/MMLU |
| `visual_diagnosis` | ì˜ë£Œ ì´ë¯¸ì§€ ë¶„ì„ | analyze_image, get_image_report, compare_images, search_similar_cases | VQA-RAD/SLAKE |
| `ehr_management` | EHR ì¡°íšŒ/ë¶„ì„ | query_patient_records, get_lab_trends, get_medications, check_interactions | MIMIC-III ê¸°ë°˜ |
| `drug_interaction` | ì•½ë¬¼ ìƒí˜¸ì‘ìš© ê²€ì¦ | search_drug, check_interaction, get_dosage, search_alternatives | ì•½ë¬¼ DB |

### 3.3 Medical Tool Database ì„¤ê³„ (ìƒì„¸)

```python
# ê³„íšëœ Tool ì¹´í…Œê³ ë¦¬ (ì´ ~25ê°œ tool)

# Category 1: Medical Knowledge Search
- search_pubmed(queries: list[str]) â†’ list[{title, abstract, pmid, url}]
- browse_article(pmid: str, query: str) â†’ str
- search_medical_wiki(queries: list[str]) â†’ list[{title, url, snippet}]
- browse_medical_wiki(url: str, query: str) â†’ str
- search_clinical_guidelines(condition: str) â†’ list[{guideline, source}]

# Category 2: Patient Record (EHR) Tools
- get_patient_info(patient_id: str) â†’ {demographics, conditions, allergies}
- get_lab_results(patient_id: str, test_type: str) â†’ list[{test, value, unit, date}]
- get_medication_list(patient_id: str) â†’ list[{drug, dose, frequency, start_date}]
- get_vital_signs(patient_id: str) â†’ {bp, hr, temp, spo2, rr}
- get_clinical_notes(patient_id: str, note_type: str) â†’ list[{date, content}]

# Category 3: Diagnostic Tools
- check_drug_interaction(drug_a: str, drug_b: str) â†’ {severity, description}
- calculate_clinical_score(score_type: str, params: dict) â†’ {score, interpretation}
- get_differential_diagnosis(symptoms: list[str]) â†’ list[{condition, probability}]
- order_lab_test(patient_id: str, test_type: str) â†’ {order_id, status}

# Category 4: Medical Image Analysis (ì‹œë®¬ë ˆì´ì…˜)
- analyze_medical_image(image_path: str, modality: str) â†’ {findings, confidence}
- get_image_report(image_id: str) â†’ {report, impression}
- compare_with_prior(current_id: str, prior_id: str) â†’ {changes, assessment}

# Category 5: Communication & Workflow
- transfer_to_specialist(summary: str, specialty: str) â†’ str
- schedule_followup(patient_id: str, reason: str) â†’ {appointment_id, date}
- send_patient_message(patient_id: str, message: str) â†’ str
```

### 3.4 Reward Function ì„¤ê³„

```python
# Multi-signal Reward (ì°¸ê³ : MRPO token shaping + AgentGym-RL)

# 1. Task Completion Reward (binary)
#    - ì˜ë£Œ taskë¥¼ ì˜¬ë°”ë¥´ê²Œ ì™„ë£Œí–ˆëŠ”ì§€ (ì •í™•í•œ ì§„ë‹¨, ì˜¬ë°”ë¥¸ ë‹µë³€ ë“±)

# 2. Tool Usage Reward (structured)
#    - ì˜¬ë°”ë¥¸ toolì„ ì˜¬ë°”ë¥¸ ìˆœì„œë¡œ ì‚¬ìš©í–ˆëŠ”ì§€
#    - ë¶ˆí•„ìš”í•œ tool callì´ ì—†ëŠ”ì§€

# 3. Medical Accuracy Reward (soft)
#    - ROUGE-1 + BLEU-1 + BERTScore (BiomedBERT ê¸°ë°˜)
#    - ì˜ë£Œ ì „ë¬¸ ìš©ì–´ì˜ ì •í™•ì„±

# 4. Process Reward (LLM-as-judge)
#    - Alignment: gold reasoningê³¼ ì¼ì¹˜í•˜ëŠ”ì§€
#    - Relevance: í™˜ì ì‚¬ë¡€ì— êµ¬ì²´ì ì¸ì§€
#    - Factuality: ì˜í•™ì ìœ¼ë¡œ ì‚¬ì‹¤ì¸ì§€

# 5. Format Reward (binary)
#    - <think>...</think> <answer>...</answer> í˜•ì‹ ì¤€ìˆ˜
```

---

## 4. íƒ€ì„ë¼ì¸ & ë¡œë“œë§µ

### Phase 1: ê¸°ë°˜ êµ¬ì¶• (2026.02.12 ~ 2026.03.15) [4ì£¼]

| ì£¼ì°¨ | ì‘ì—… | ì‚°ì¶œë¬¼ | ìƒíƒœ |
|---|---|---|---|
| W1 (02/12~02/18) | í”„ë¡œì íŠ¸ êµ¬ì¡° ì„¤ê³„ & ê¸°íš ë¬¸ì„œ | PLANNING.md, ë””ë ‰í† ë¦¬ êµ¬ì¡° | ğŸ”„ ì§„í–‰ì¤‘ |
| W1 | Medical Tool Database ìŠ¤í‚¤ë§ˆ ì„¤ê³„ | tools.py, data_model.py ì´ˆì•ˆ | â¬œ ëŒ€ê¸° |
| W2 (02/19~02/25) | Medical Domain í™˜ê²½ êµ¬í˜„ (clinical_diagnosis) | environment.py, policy.md | â¬œ ëŒ€ê¸° |
| W2 | Tool Simulation ì—”ì§„ ì˜ë£Œ ë„ë©”ì¸ ì ìš© | tool simulation prompts | â¬œ ëŒ€ê¸° |
| W3 (02/26~03/04) | Task ì‹œë‚˜ë¦¬ì˜¤ ìƒì„± (50+ tasks) | tasks.json, db.json | â¬œ ëŒ€ê¸° |
| W3 | GYM ì¸í„°í˜ì´ìŠ¤ êµ¬í˜„ (Gymnasium-compatible) | gym_agent.py | â¬œ ëŒ€ê¸° |
| W4 (03/05~03/15) | ë°ì´í„°ì…‹ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ | datasets/ êµ¬ì„± | â¬œ ëŒ€ê¸° |
| W4 | ê¸°ë³¸ í‰ê°€ íŒŒì´í”„ë¼ì¸ êµ¬ì¶• | eval scripts | â¬œ ëŒ€ê¸° |

### Phase 2: í•™ìŠµ íŒŒì´í”„ë¼ì¸ (2026.03.15 ~ 2026.04.15) [4ì£¼]

| ì£¼ì°¨ | ì‘ì—… | ì‚°ì¶œë¬¼ | ìƒíƒœ |
|---|---|---|---|
| W5 (03/15~03/22) | SFT ë°ì´í„° êµ¬ì„± (instruction + tool-use) | SFT jsonl ë°ì´í„° | â¬œ ëŒ€ê¸° |
| W5 | ëª¨ë¸ ì„ ì • & baseline í‰ê°€ | baseline ê²°ê³¼ ë¡œê·¸ | â¬œ ëŒ€ê¸° |
| W6 (03/22~03/29) | SFT í•™ìŠµ (Oumi) | SFT ì²´í¬í¬ì¸íŠ¸ | â¬œ ëŒ€ê¸° |
| W6 | additional Medical Domain êµ¬í˜„ (medical_qa, visual_diagnosis) | ì¶”ê°€ ë„ë©”ì¸ ì½”ë“œ | â¬œ ëŒ€ê¸° |
| W7 (03/29~04/05) | RL í•™ìŠµ ì‹œì‘ (GRPO, GYM í™˜ê²½ ì—°ë™) | RL ì²´í¬í¬ì¸íŠ¸ | â¬œ ëŒ€ê¸° |
| W7 | Trajectory ë¡œê¹… ì‹œìŠ¤í…œ êµ¬ì¶• | trajectory íŒŒì¼ë“¤ | â¬œ ëŒ€ê¸° |
| W8 (04/05~04/15) | ScalingInter-RL ì ìš© ì‹¤í—˜ | í•™ìŠµ ê³¡ì„ , ë¹„êµ ê²°ê³¼ | â¬œ ëŒ€ê¸° |
| W8 | ì¤‘ê°„ í‰ê°€ (Text QA + Visual QA) | ì¤‘ê°„ ê²°ê³¼ ë¦¬í¬íŠ¸ | â¬œ ëŒ€ê¸° |

### Phase 3: ë°˜ë³µ ê°œì„  (2026.04.15 ~ 2026.05.15) [4ì£¼]

| ì£¼ì°¨ | ì‘ì—… | ì‚°ì¶œë¬¼ | ìƒíƒœ |
|---|---|---|---|
| W9 | EHR Domain êµ¬í˜„ & í•™ìŠµ | EHR ë„ë©”ì¸ ì½”ë“œ | â¬œ ëŒ€ê¸° |
| W10 | Reward function ê°œì„  & ì‹¤í—˜ | ablation ê²°ê³¼ | â¬œ ëŒ€ê¸° |
| W11 | Multi-domain í†µí•© í•™ìŠµ | í†µí•© ì²´í¬í¬ì¸íŠ¸ | â¬œ ëŒ€ê¸° |
| W12 | ì „ì²´ ë²¤ì¹˜ë§ˆí¬ í‰ê°€ | ìµœì¢… ê²°ê³¼ í…Œì´ë¸” | â¬œ ëŒ€ê¸° |

### Phase 4: ë…¼ë¬¸ ì‘ì„± (2026.05.15 ~ 2026.06.01) [2ì£¼]

| ì£¼ì°¨ | ì‘ì—… | ì‚°ì¶œë¬¼ | ìƒíƒœ |
|---|---|---|---|
| W13 | ë…¼ë¬¸ ì´ˆì•ˆ ì‘ì„± | paper draft | â¬œ ëŒ€ê¸° |
| W14 | ì¶”ê°€ ì‹¤í—˜ + ë…¼ë¬¸ ì™„ì„± | final paper | â¬œ ëŒ€ê¸° |

---

## 5. Related Work ë¶„ì„

### 5.1 ê°€ì¥ ê´€ë ¨ëœ ê¸°ì¡´ ì—°êµ¬

#### (1) AgentClinic (EMNLP 2024 Findings)
- **ë…¼ë¬¸**: `references/medical_agent/2024.findings-emnlp.510.pdf`
- **í•µì‹¬**: í™˜ì/ì˜ì‚¬/ì¸¡ì •/ì¡°ì • ì—ì´ì „íŠ¸ë¡œ êµ¬ì„±ëœ ì„ìƒ ì‹œë®¬ë ˆì´ì…˜
- **í•œê³„**: ì§„ë‹¨ ì‹œë‚˜ë¦¬ì˜¤ì— í•œì •, tool ë‹¤ì–‘ì„± ë¶€ì¡±, RL í•™ìŠµ ì—†ìŒ
- **ì°¸ê³ ì **: ë‹¤ì¤‘ ì—­í•  ì—ì´ì „íŠ¸ êµ¬ì¡°, í¸í–¥ ì‹œë®¬ë ˆì´ì…˜

#### (2) AgentGym-RL (arXiv:2509.08755)
- **ë…¼ë¬¸**: `references/medical_qa/2509.08755v1.pdf`
- **í•µì‹¬**: verl ê¸°ë°˜ multi-turn RL, ScalingInter-RL ì•Œê³ ë¦¬ì¦˜
- **í•œê³„**: ì˜ë£Œ ë„ë©”ì¸ í™˜ê²½ ì—†ìŒ
- **ì°¸ê³ ì **: ì•„í‚¤í…ì²˜, í•™ìŠµ íŒŒì´í”„ë¼ì¸, RoundScheduler

#### (3) Ï„Â²-bench (arXiv:2506.07982)
- **ì½”ë“œ**: `databases/tau2-bench/`
- **í•µì‹¬**: ë„ë©”ì¸ë³„ tool+DB+policy+task êµ¬ì¡°, Gymnasium ì¸í„°í˜ì´ìŠ¤
- **í•œê³„**: airline/retail/telecomë§Œ
- **ì°¸ê³ ì **: ë„ë©”ì¸ êµ¬ì¡° íŒ¨í„´, í‰ê°€ ì²´ê³„

#### (4) Self-BioRAG (arXiv:2305.10415)
- **ì½”ë“œ**: `evaluations/self-biorag/`
- **í•µì‹¬**: ì˜ë£Œ RAG, retrieval critic, MedQA/MMLU í‰ê°€
- **ì°¸ê³ ì **: ì˜ë£Œ ì§€ì‹ ê²€ìƒ‰ êµ¬ì¡°, ë²¤ì¹˜ë§ˆí¬ ë°ì´í„°

#### (5) MRPO (ICML submission)
- **ì½”ë“œ**: `references/medical_qa/grpo_vqa_Qwen3_token_shaping.py`
- **í•µì‹¬**: Medical VQAì— GRPO + token shaping ì ìš©
- **ì°¸ê³ ì **: process reward (Alignment/Relevance/Factuality), BERTScore reward

#### (6) Lingshu
- **URL**: https://huggingface.co/lingshu-medical-mllm/Lingshu-7B
- **í•µì‹¬**: ì˜ë£Œ íŠ¹í™” MLLM, MedEvalKit
- **ì°¸ê³ ì **: ëª¨ë¸ í›„ë³´, í‰ê°€ í”„ë ˆì„ì›Œí¬

### 5.2 ì¶”ê°€ ì°¸ê³  ë…¼ë¬¸ (references/ ë‚´)
| íŒŒì¼ëª… | ì¶”ì • ë‚´ìš© |
|---|---|
| `medical_agent/2404.15155v3.pdf` | ì˜ë£Œ agent ê´€ë ¨ (2024) |
| `medical_agent/2411.00248v2.pdf` | ì˜ë£Œ agent ê´€ë ¨ (2024) |
| `medical_agent/2505.16100v1.pdf` | ì˜ë£Œ agent ê´€ë ¨ (2025) |
| `medical_qa/2003.10286v1.pdf` | ì˜ë£Œ QA ê´€ë ¨ (2020) |
| `medical_qa/2009.13081v1.pdf` | ì˜ë£Œ QA ê´€ë ¨ (2020) |
| `medical_qa/2309.11080v1.pdf` | ì˜ë£Œ QA ê´€ë ¨ (2023) |
| `medical_qa/2405.12701v3.pdf` | ì˜ë£Œ QA ê´€ë ¨ (2024) |
| `medical_qa/2506.09513v3.pdf` | ì˜ë£Œ QA ê´€ë ¨ (2025) |
| `medical_qa/2508.19096v1.pdf` | ì˜ë£Œ QA ê´€ë ¨ (2025) |
| `medical_qa/sdata2018251.pdf` | ì˜ë£Œ ë°ì´í„° ê´€ë ¨ |
| `medical_qa/SLAKE.pdf` | SLAKE ë°ì´í„°ì…‹ ë…¼ë¬¸ |

---

## 6. í•µì‹¬ ê¸°ìˆ  ê²°ì • ì‚¬í•­

### 6.1 ê²°ì •ëœ ì‚¬í•­
- [x] GYM êµ¬ì¡°: Ï„Â²-bench ìŠ¤íƒ€ì¼ ë„ë©”ì¸ êµ¬ì¡° + Gymnasium ì¸í„°í˜ì´ìŠ¤
- [x] RL ì•Œê³ ë¦¬ì¦˜: GRPO (ì£¼), PPO (ë¹„êµ), ScalingInter-RL (ì‹¤í—˜)
- [x] Tool Simulation: LLM ê¸°ë°˜ ì‹œë®¬ë ˆì´ì…˜ (tool-dataset-generation ì°¸ê³ )
- [x] í‰ê°€ ë²¤ì¹˜ë§ˆí¬: MedQA, MedMCQA, MMLU, VQA-RAD, SLAKE, PathVQA, PMC-VQA
- [x] ê°€ìš© ìì›: A100 8ëŒ€

### 6.2 ê²°ì • í•„ìš” ì‚¬í•­
- [ ] **ì£¼ ëª¨ë¸ ì„ íƒ**: Qwen2.5-VL-7B vs Lingshu-7B vs ë‘˜ ë‹¤?
- [ ] **EHR ë°ì´í„° ì ‘ê·¼**: MIMIC-III/IV ë°ì´í„° ì‹¤ì œ ì ‘ê·¼ ê°€ëŠ¥ ì—¬ë¶€
- [ ] **ë„ë©”ì¸ ìš°ì„ ìˆœìœ„**: ì–´ë–¤ ë„ë©”ì¸ë¶€í„° êµ¬í˜„í•  ê²ƒì¸ê°€?
- [ ] **Tool Simulation vs Real API**: ì–´ë””ê¹Œì§€ ì‹œë®¬ë ˆì´ì…˜, ì–´ë””ë¶€í„° ì‹¤ì œ API?
- [ ] **ë…¼ë¬¸ í¬ì§€ì…”ë‹**: framework paper vs method paper vs benchmark paper?

---

## 7. ì‹¤í—˜ ë¡œê·¸ (Experiment Log)

### [2026-02-12] í”„ë¡œì íŠ¸ ì‹œì‘
- í”„ë¡œì íŠ¸ êµ¬ì¡° ë¶„ì„ ì™„ë£Œ
- ê¸°íš ë¬¸ì„œ ì´ˆì•ˆ ì‘ì„±
- ë³´ìœ  ë¦¬ì†ŒìŠ¤ ì „ìˆ˜ ì¡°ì‚¬ ì™„ë£Œ
- Related work ì„œë² ì´ ì‹œì‘

### í–¥í›„ ê¸°ë¡ í˜•ì‹
```
### [YYYY-MM-DD] ì‘ì—… ì œëª©
- **ì‘ì—… ë‚´ìš©**: ìˆ˜í–‰í•œ ì‘ì—… ìƒì„¸
- **ì‚¬ìš© ëª¨ë¸/ë°ì´í„°**: 
- **ê²°ê³¼ ìš”ì•½**: 
- **ë‹¤ìŒ ë‹¨ê³„**: 
- **ê´€ë ¨ íŒŒì¼**: ê²½ë¡œ ëª©ë¡
```

---

## 8. ë¦¬ìŠ¤í¬ & ëŒ€ì‘ ì „ëµ

| ë¦¬ìŠ¤í¬ | ì˜í–¥ | í™•ë¥  | ëŒ€ì‘ |
|---|---|---|---|
| MIMIC ë°ì´í„° ì ‘ê·¼ ë¶ˆê°€ | EHR ë„ë©”ì¸ êµ¬í˜„ ë¶ˆê°€ | ì¤‘ | í•©ì„± EHR ë°ì´í„°ë¡œ ëŒ€ì²´ |
| RL í•™ìŠµ ë¶ˆì•ˆì • | ì„±ëŠ¥ ì €í•˜ | ê³  | SFT warmup + KL penalty + ScalingInter |
| A100 8ëŒ€ ë¦¬ì†ŒìŠ¤ ë¶€ì¡± | ëŒ€í˜• ëª¨ë¸ í•™ìŠµ ë¶ˆê°€ | ì¤‘ | 7B ëª¨ë¸ ì§‘ì¤‘, LoRA/QLoRA ì ìš© |
| NeurIPS ë§ˆê° (6ì›”) | ì‹œê°„ ë¶€ì¡± | ì¤‘ | Phase 1-2 ì—„ê²© ê´€ë¦¬, MVP ìš°ì„  |
| Tool simulation í’ˆì§ˆ | ë¹„í˜„ì‹¤ì  ê²°ê³¼ | ì¤‘ | GPT-5/Claudeë¡œ ê³ í’ˆì§ˆ simulation |

---

## 9. ì½”ë“œ ì»¨ë²¤ì…˜ & ë¡œê¹… ê·œì¹™

### 9.1 ë””ë ‰í† ë¦¬ ê·œì¹™
- ëª¨ë“  ì‹¤í—˜ ê²°ê³¼ëŠ” `logs/` ë””ë ‰í† ë¦¬ì— ë‚ ì§œë³„ ì €ì¥
- ì²´í¬í¬ì¸íŠ¸ëŠ” `checkpoints/` ë””ë ‰í† ë¦¬ì— ì‹¤í—˜ëª…_ë‚ ì§œë¡œ ì €ì¥
- í•™ìŠµ ì„¤ì •ì€ `configs/` ë””ë ‰í† ë¦¬ì— YAMLë¡œ ê´€ë¦¬

### 9.2 ë¡œê¹… ê·œì¹™
- ëª¨ë“  í•™ìŠµì€ W&B (Weights & Biases)ì— ê¸°ë¡
- TrajectoryëŠ” JSON í˜•ì‹ìœ¼ë¡œ ì „ì²´ ì €ì¥
- í‰ê°€ ê²°ê³¼ëŠ” í‘œì¤€ JSON í˜•ì‹ìœ¼ë¡œ ì €ì¥
- ì½”ë“œ ë³€ê²½ì€ Git commitìœ¼ë¡œ ì¶”ì 

### 9.3 íŒŒì¼ ëª…ëª… ê·œì¹™
- ë°ì´í„°: `{domain}_{split}_{version}.json`
- ì„¤ì •: `{model}_{method}_{date}.yaml`
- ë¡œê·¸: `{experiment_name}_{date}_log.txt`
- ì²´í¬í¬ì¸íŠ¸: `{model}_{method}_{step}/`

---

*ì´ ë¬¸ì„œëŠ” í”„ë¡œì íŠ¸ ì§„í–‰ì— ë”°ë¼ ì§€ì†ì ìœ¼ë¡œ ì—…ë°ì´íŠ¸ë©ë‹ˆë‹¤.*

```

`PLANNING.md` ì‘ì„± ì™„ë£Œí–ˆìŠµë‹ˆë‹¤! ì •ë¦¬ëœ ë‚´ìš©ì„ ìš”ì•½í•˜ë©´:

---

### ğŸ“‹ ê¸°íš ë¬¸ì„œì— ë‹´ê¸´ ë‚´ìš© (`PLANNING.md`)

**1. í”„ë¡œì íŠ¸ ê°œìš”** â€” í•µì‹¬ ì•„ì´ë””ì–´, ê¸°íš ì˜ë„, ê¸°ì¡´ ì—°êµ¬ ëŒ€ë¹„ ì°¨ë³„ì  (AgentClinic, AgentGym-RL, Ï„Â²-bench, Med-PaLM M, Lingshuì™€ì˜ ë¹„êµí‘œ)

**2. í˜„ì¬ ë¦¬ì†ŒìŠ¤ í˜„í™© (ì „ìˆ˜ ì¡°ì‚¬)**
- ğŸ“ ì „ì²´ ë””ë ‰í† ë¦¬ êµ¬ì¡° íŠ¸ë¦¬ë§µ (ëª¨ë“  í•˜ìœ„ í´ë” ì„¤ëª… í¬í•¨)
- ğŸ“Š Visual Medical QA 6ê°œ, Text Medical QA 3ê°œ, EHR 1ê°œ ë°ì´í„°ì…‹ ìƒì„¸ í‘œ
- ğŸ§  Knowledge Base 5ê°œ ì†ŒìŠ¤ (wiki dump, MedCPT, critic/generator)
- ğŸ¤– ëª¨ë¸ í›„ë³´êµ° 3ê°œ (Lingshu-7B, Qwen2.5-VL-7B, Step3-VL-10B)
- ğŸ”§ í•™ìŠµ í”„ë ˆì„ì›Œí¬ 4ê°œ ë¶„ì„ (Oumi, Snapshot-PO, AgentGym-RL, MRPO)
- ğŸ—ï¸ ì°¸ê³  ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜ ë¶„ì„ (Ï„Â²-bench ë„ë©”ì¸ íŒ¨í„´, AgentGym-RL ëª¨ë“ˆ, Tool Simulation íŒŒì´í”„ë¼ì¸)

**3. ê¸°ìˆ  ì„¤ê³„**
- GYM ì „ì²´ ì•„í‚¤í…ì²˜ ë‹¤ì´ì–´ê·¸ë¨
- Medical Domain 5ê°œ ì„¤ê³„ (clinical_diagnosis, medical_qa, visual_diagnosis, ehr_management, drug_interaction)
- Tool Database ì„¤ê³„ (~25ê°œ tool, 5ê°œ ì¹´í…Œê³ ë¦¬)
- Reward Function 5ê°€ì§€ signal ì„¤ê³„

**4. íƒ€ì„ë¼ì¸ (14ì£¼ ë¡œë“œë§µ)**
- Phase 1 (02/12~03/15): ê¸°ë°˜ êµ¬ì¶• â€” ë„ë©”ì¸/íˆ´/í™˜ê²½/GYM
- Phase 2 (03/15~04/15): í•™ìŠµ íŒŒì´í”„ë¼ì¸ â€” SFTâ†’RL
- Phase 3 (04/15~05/15): ë°˜ë³µ ê°œì„  â€” ë©€í‹°ë„ë©”ì¸, reward ê°œì„ 
- Phase 4 (05/15~06/01): ë…¼ë¬¸ ì‘ì„±

**5. Related Work ë¶„ì„** â€” ì°¸ê³  ë…¼ë¬¸ 11í¸+ ìƒì„¸ ë¶„ì„

**6. ë¯¸ê²°ì • ì‚¬í•­** â€” ëª¨ë¸ ì„ íƒ, EHR ì ‘ê·¼, ë„ë©”ì¸ ìš°ì„ ìˆœìœ„ ë“±

**7. ì‹¤í—˜ ë¡œê·¸ ì–‘ì‹** â€” ë‚ ì§œë³„ ê¸°ë¡ í¬ë§· ì •ì˜

**8. ë¦¬ìŠ¤í¬ & ëŒ€ì‘** â€” 5ê°€ì§€ ì£¼ìš” ë¦¬ìŠ¤í¬ì™€ ì™„í™” ì „ëµ

**9. ì½”ë“œ/ë¡œê¹… ì»¨ë²¤ì…˜** â€” íŒŒì¼ ëª…ëª…, W&B ë¡œê¹…, Git ì¶”ì  ê·œì¹™

---

ì´ì œ ë‹¤ìŒ ë‹¨ê³„ë¡œ ë„˜ì–´ê°ˆ ì¤€ë¹„ê°€ ë˜ì—ˆì–´ìš”. **ê²°ì •ì´ í•„ìš”í•œ ë¶€ë¶„**ì´ ëª‡ ê°€ì§€ ìˆëŠ”ë°:

1. **ì£¼ ëª¨ë¸**: Qwen2.5-VL-7B (ë²”ìš©, tool-use ê°•í•¨) vs Lingshu-7B (ì˜ë£Œ íŠ¹í™”) â€” ì–´ë–¤ ê±¸ ë©”ì¸ìœ¼ë¡œ?
2. **ì²« ë²ˆì§¸ ë„ë©”ì¸**: `clinical_diagnosis`ë¶€í„° ì‹œì‘í• ê¹Œìš”, `medical_qa`ë¶€í„°?
3. **ë…¼ë¬¸ í¬ì§€ì…”ë‹**: Framework paper (GYM ìì²´) vs Method paper (RL ì•Œê³ ë¦¬ì¦˜) vs Benchmark paper?

ì´ëŸ° ê²ƒë“¤ ì •í•˜ê³  ë°”ë¡œ ì½”ë“œ êµ¬í˜„ìœ¼ë¡œ ë“¤ì–´ê°€ë©´ ë©ë‹ˆë‹¤!

---

**User**

ë¨¼ì € ì˜ë£Œ íŠ¹í™” ëª¨ë¸ë¡œ ì‹œë„í•˜ê³  ë‚˜ì¤‘ì— qwen2.5-vlë¡œë„ í™œìš©í•´ë´.
ëª¨ë“  ë„ë©”ì¸ ì‘ì—…í• ê±°ë‹ˆê¹Œ clinical_diagnosisë¶€í„° ê°€ë³´ì.
ë…¼ë¬¸ì€ framework ìì²´ë¡œ ì‘ì„±í• ê±°ì•¼. paper ì‘ì„±ì€ ë‚´ê°€ í• í…Œë‹ˆê¹Œ ë„ˆëŠ” ëª¨ë“  ê³¼ì •ì„ í•­ìƒ ìƒˆë¡œìš´ í˜ì´ì§€ ê·¸ë‚  ë‚ ì§œì— ë§ì¶°ì„œ ê¸°ë¡í•˜ê³  êµ¬í˜„í•˜ê³  ì½”ë“œì‹¤í–‰ë„ í•˜ê³  ê²°ê³¼ë¶„ì„ë„ í•˜ê³  ë‹¤ í•´ì¤˜

---

**Cursor**

ì•Œê² ìŠµë‹ˆë‹¤! ê²°ì •ì‚¬í•­ ë°˜ì˜í•˜ê³  ë°”ë¡œ êµ¬í˜„ ì‹œì‘í• ê²Œìš”. ì˜¤ëŠ˜ì ì‘ì—… ë¡œê·¸ë„ ë§Œë“¤ë©´ì„œ ê°€ê² ìŠµë‹ˆë‹¤.

ë¨¼ì € ê²°ì •ì‚¬í•­ ë°˜ì˜ + ì¼ì¼ ë¡œê·¸ ì‹œìŠ¤í…œ + í”„ë¡œì íŠ¸ í•µì‹¬ ì½”ë“œ ë””ë ‰í† ë¦¬ êµ¬ì¡°ë¥¼ ìƒì„±í•˜ê² ìŠµë‹ˆë‹¤.

```markdown
# BIOAgents - Medical/Biomedical Agent GYM: ì „ì²´ ê¸°íš ë¬¸ì„œ

> **ì‘ì„±ì¼**: 2026-02-12  
> **ëª©í‘œ**: NeurIPS 2026 ì œì¶œ (2026ë…„ 6ì›”)  
> **ê°€ìš©ìì›**: A100 8ëŒ€  

---

## 1. í”„ë¡œì íŠ¸ ê°œìš” (Project Overview)

### 1.1 í•µì‹¬ ì•„ì´ë””ì–´
Medical & Biomedical ë„ë©”ì¸ì— íŠ¹í™”ëœ **Agent GYM** í”„ë ˆì„ì›Œí¬ë¥¼ êµ¬ì¶•í•œë‹¤.  
AgentGym-RLê³¼ Ï„Â²-benchì˜ ì•„í‚¤í…ì²˜ë¥¼ ì°¸ê³ í•˜ë˜, **ì˜ë£Œ ë„ë©”ì¸ íŠ¹í™” tool database**, **task ì‹œë‚˜ë¦¬ì˜¤**, **Gymnasium-compatible í™˜ê²½**ì„ ìƒˆë¡­ê²Œ ì„¤ê³„í•˜ì—¬, multi-turn RL(GRPO/PPO)ì„ í†µí•´ medical agentë¥¼ í•™ìŠµì‹œí‚¤ëŠ” end-to-end ì‹œìŠ¤í…œì„ ë§Œë“ ë‹¤.

### 1.2 ê¸°íš ì˜ë„ (README.md ì›ë¬¸ ê¸°ë°˜)
1. medical & biomedical ê´€ë ¨ benchmark resourceë¥¼ í•œ ê³³ì— ëª¨ì•„ì„œ í‰ê°€ ê°€ëŠ¥í•˜ë„ë¡ í•œë‹¤.
2. Ï„Â²-benchì²˜ëŸ¼ medical domain íŠ¹í™”ëœ tool ê¸°ë°˜ databaseì™€ taskë¥¼ ë§Œë“¤ê³  tool ì‹¤í–‰ì´ simulation ê¸°ë°˜ìœ¼ë¡œ ê°€ëŠ¥í•˜ê²Œ í•œë‹¤.
3. agentê°€ gym í˜•íƒœë¡œ êµ¬ì„±ë˜ì–´ medical ìƒí™©ê·¹ì—ì„œì˜ ëª¨ë“  trajectoryë¥¼ ê¸°ë¡í•œë‹¤.
4. ìŠ¤ìŠ¤ë¡œ ê¸°ë¡í•œ ë°©ì‹ì„ ë³´ê³  ì–´ë– í•œ trajectoryê°€ ìˆì–´ì•¼ realisticí•œì§€ íŒë‹¨í•œë‹¤.
5. realisticí•œ scenario ê¸°ë°˜ìœ¼ë¡œ ìŠ¤ìŠ¤ë¡œ í•™ìŠµ(RL - GRPO)ì„ ì‹œí‚¤ë©° exploration & exploitationì„ ì§„í–‰í•œë‹¤.
6. íŠ¹ì • stepë§ˆë‹¤ í•™ìŠµí•œ agentëŠ” visual medical QA, text medical QA ë“± í‰ê°€ë¥¼ í•´ë³´ë©° ë³¸ì¸ì˜ ìƒíƒœë¥¼ íŒŒì•…í•œë‹¤.
7. ìœ„ì™€ê°™ì€ ê³¼ì •ì„ ì§€ì†, ë°˜ë³µí•˜ì—¬ ëª¨ë“ ê²ƒì„ ê¸°ë¡í•œë‹¤.

### 1.3 í•µì‹¬ ì°¨ë³„ì  (vs. ê¸°ì¡´ ì—°êµ¬)
| ê¸°ì¡´ ì—°êµ¬ | í•œê³„ | BIOAgents ì°¨ë³„ì  |
|---|---|---|
| AgentClinic (EMNLP 2024) | ì§„ë‹¨ ì‹œë‚˜ë¦¬ì˜¤ë§Œ, ë„êµ¬ ì œí•œì , RL í•™ìŠµ ì—†ìŒ | ë‹¤ì–‘í•œ ì˜ë£Œ task + tool simulation + RL í•™ìŠµ |
| AgentGym-RL (arXiv 2025) | ì˜ë£Œ ë„ë©”ì¸ ì—†ìŒ, ì¼ë°˜ í™˜ê²½ë§Œ | ì˜ë£Œ íŠ¹í™” í™˜ê²½/ë„êµ¬/ì‹œë‚˜ë¦¬ì˜¤ |
| Ï„Â²-bench | airline/retail/telecomë§Œ | medical ë„ë©”ì¸ í™•ì¥ |
| Med-PaLM M | ì •ì  ë²¤ì¹˜ë§ˆí¬, agent ì•„ë‹˜ | interactive agent + multi-turn RL |
| Lingshu-7B | VQA íŠ¹í™”, tool-use ì—†ìŒ | tool-augmented agent training |

---

## 2. í˜„ì¬ ë¦¬ì†ŒìŠ¤ í˜„í™© (Resource Inventory)

### 2.1 ë””ë ‰í† ë¦¬ êµ¬ì¡°
```
BIOAgents/
â”œâ”€â”€ README.md                    # ê¸°íš ì˜ë„ & ë¦¬ì†ŒìŠ¤ ì •ë¦¬
â”œâ”€â”€ PLANNING.md                  # ë³¸ ê¸°íš ë¬¸ì„œ
â”œâ”€â”€ databases/                   # Tool DB & Knowledge Base
â”‚   â”œâ”€â”€ critic/                  # Self-BioRAG critic ë°ì´í„° (8ê°œ JSON)
â”‚   â”œâ”€â”€ generator/               # Self-BioRAG generator ë°ì´í„°
â”‚   â”œâ”€â”€ instruction/             # ì˜ë£Œ instruction ë°ì´í„° (4ê°œ JSON)
â”‚   â”‚   â”œâ”€â”€ all_biomedical_instruction.json
â”‚   â”‚   â”œâ”€â”€ MedInstruct-52k.json
â”‚   â”‚   â”œâ”€â”€ mol_instruction_qa.json
â”‚   â”‚   â””â”€â”€ self_instruct_biomedical.json
â”‚   â”œâ”€â”€ retriever/               # MedCPT top-10 evidence
â”‚   â”œâ”€â”€ tau2-bench/              # Ï„Â²-bench ì „ì²´ ì½”ë“œ (ì°¸ê³ ìš© ë„ë©”ì¸ êµ¬ì¡°)
â”‚   â”œâ”€â”€ wiki2018_en/             # Wikipedia 2018 dump
â”‚   â””â”€â”€ wiki2026_en/             # Wikipedia 2026 dump
â”œâ”€â”€ datasets/                    # (ë¹„ì–´ìˆìŒ - í•™ìŠµ/í‰ê°€ ë°ì´í„° íë ˆì´ì…˜ ì˜ˆì •)
â”œâ”€â”€ evaluations/                 # í‰ê°€ ë²¤ì¹˜ë§ˆí¬ ì½”ë“œ
â”‚   â”œâ”€â”€ mimic-code/              # MIMIC-III/IV EHR ì½”ë“œ (benchmarks, SQL concepts)
â”‚   â”œâ”€â”€ OLAPH/                   # Long-form Medical QA í‰ê°€ (MedLFQA)
â”‚   â”œâ”€â”€ PathVQA/                 # PathVQA ë² ì´ìŠ¤ë¼ì¸ & í‰ê°€
â”‚   â”œâ”€â”€ PMC-VQA/                 # PMC-VQA + Slake1.0
â”‚   â”œâ”€â”€ quilt-llava/             # Quilt-VQA (histopathology VQA)
â”‚   â”œâ”€â”€ self-biorag/             # Self-BioRAG (MedQA, MedMCQA, MMLU í¬í•¨)
â”‚   â”‚   â””â”€â”€ data/benchmark/      # med_qa, medmc_qa, mmlu (test/train .jsonl)
â”‚   â””â”€â”€ VQA-Med-2021/            # VQA-Med 2021 í…ŒìŠ¤íŠ¸ì…‹
â”œâ”€â”€ GYM_reference/               # GYM êµ¬ì¡° ì°¸ê³  ì½”ë“œ
â”‚   â””â”€â”€ AgentGym-RL/             # AgentGym-RL ì „ì²´ (verl ê¸°ë°˜ RL trainer)
â”‚       â”œâ”€â”€ AgentGym/            # ì›ë³¸ AgentGym (ë¹ˆ ë””ë ‰í† ë¦¬, ì°¸ê³ ìš©)
â”‚       â”œâ”€â”€ AgentGym-RL/         # verl ê¸°ë°˜ agent trainer
â”‚       â”‚   â””â”€â”€ verl/agent_trainer/  # PPO/GRPO trainer, í™˜ê²½ ì„¤ì •
â”‚       â””â”€â”€ examples/train/      # í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ ì˜ˆì‹œ (searchqa, webarena ë“±)
â”œâ”€â”€ references/                  # ì°¸ê³  ë…¼ë¬¸ & ì½”ë“œ
â”‚   â”œâ”€â”€ medical_agent/           # ì˜ë£Œ agent ê´€ë ¨ ë…¼ë¬¸ 4í¸
â”‚   â”‚   â”œâ”€â”€ 2024.findings-emnlp.510.pdf
â”‚   â”‚   â”œâ”€â”€ 2404.15155v3.pdf
â”‚   â”‚   â”œâ”€â”€ 2411.00248v2.pdf
â”‚   â”‚   â””â”€â”€ 2505.16100v1.pdf
â”‚   â””â”€â”€ medical_qa/              # ì˜ë£Œ QA ê´€ë ¨ ë…¼ë¬¸ & ì½”ë“œ
â”‚       â”œâ”€â”€ grpo_vqa_Qwen3_token_shaping.py   # MRPO VQA í•™ìŠµ ì½”ë“œ
â”‚       â”œâ”€â”€ run_grpo_MRPO_Qwen3.sh            # ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
â”‚       â”œâ”€â”€ MRPO_ICML_submission.pdf           # MRPO ë…¼ë¬¸
â”‚       â”œâ”€â”€ 2509.08755v1.pdf                   # AgentGym-RL ë…¼ë¬¸
â”‚       â””â”€â”€ ... (ì´ 14ê°œ íŒŒì¼)
â”œâ”€â”€ tool_simulations/            # Tool Simulation ì—”ì§„
â”‚   â””â”€â”€ tool-dataset-generation/ # Tool ë°ì´í„°ì…‹ ìƒì„± íŒŒì´í”„ë¼ì¸
â”‚       â”œâ”€â”€ runner.py            # ë©”ì¸ ì‹¤í–‰ê¸°
â”‚       â”œâ”€â”€ generation.py        # ìƒì„± ë¡œì§
â”‚       â”œâ”€â”€ utils/
â”‚       â”‚   â”œâ”€â”€ tool_generation/     # tool spec ìë™ ìƒì„±
â”‚       â”‚   â”œâ”€â”€ tool_simulation/     # tool ì‹¤í–‰ ì‹œë®¬ë ˆì´ì…˜ (LLM ê¸°ë°˜)
â”‚       â”‚   â”œâ”€â”€ task_generation/     # task ìë™ ìƒì„±
â”‚       â”‚   â”œâ”€â”€ user_simulation/     # user ì‹œë®¬ë ˆì´ì…˜
â”‚       â”‚   â”œâ”€â”€ q_generation/        # question ìƒì„±
â”‚       â”‚   â”œâ”€â”€ response_generation/ # response ìƒì„±
â”‚       â”‚   â””â”€â”€ validation/          # ê²€ì¦
â”‚       â””â”€â”€ models/              # ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ (OpenAI, Qwen, GLM ë“±)
â””â”€â”€ trains/                      # í•™ìŠµ í”„ë ˆì„ì›Œí¬
    â”œâ”€â”€ oumi/                    # Oumi SFT í”„ë ˆì„ì›Œí¬
    â”‚   â”œâ”€â”€ configs/             # í•™ìŠµ ì„¤ì • íŒŒì¼ë“¤
    â”‚   â”œâ”€â”€ src/oumi/            # ì½”ì–´ í•™ìŠµ ì½”ë“œ
    â”‚   â””â”€â”€ scripts/             # ìœ í‹¸ë¦¬í‹° ìŠ¤í¬ë¦½íŠ¸
    â””â”€â”€ snapshot-po/             # Snapshot-PO RL í•™ìŠµ í”„ë ˆì„ì›Œí¬
        â”œâ”€â”€ configs/             # SARL ì„¤ì • íŒŒì¼ë“¤
        â”œâ”€â”€ run.py               # ë©”ì¸ í•™ìŠµ ì‹¤í–‰ê¸°
        â”œâ”€â”€ reward_computation/  # ë³´ìƒ í•¨ìˆ˜ ê³„ì‚°
        â”œâ”€â”€ generation/          # ìƒì„± ë¡œì§
        â””â”€â”€ torchtitan_rl/       # TorchTitan RL ë°±ì—”ë“œ
```

### 2.2 ë³´ìœ  ë°ì´í„°ì…‹ ìƒì„¸

#### Visual Medical QA (6ê°œ ì†ŒìŠ¤)
| # | ë°ì´í„°ì…‹ | ì†ŒìŠ¤ | íŠ¹ì§• | ìƒíƒœ |
|---|---|---|---|---|
| 1 | VQA-RAD | HuggingFace (flaviagiammarino/vqa-rad) | ë°©ì‚¬ì„ í•™ VQA | ë‹¤ìš´ë¡œë“œ í•„ìš” |
| 2 | SLAKE | HuggingFace (BoKelvin/SLAKE) + evaluations/PMC-VQA/Slake1.0 | ë‹¤êµ­ì–´ ì˜ë£Œ VQA | ë¡œì»¬ ë³´ìœ  |
| 3 | PathVQA | HuggingFace (flaviagiammarino/path-vqa) + evaluations/PathVQA | ë³‘ë¦¬í•™ VQA | ë¡œì»¬ ë³´ìœ  |
| 4 | PMC-VQA | HuggingFace (RadGenome/PMC-VQA) + evaluations/PMC-VQA | ì˜í•™ ë…¼ë¬¸ ì´ë¯¸ì§€ VQA | ë¡œì»¬ ë³´ìœ  |
| 5 | VQA-Med-2021 | evaluations/VQA-Med-2021 | ì˜ë£Œ VQA ì±Œë¦°ì§€ | ë¡œì»¬ ë³´ìœ  (zip) |
| 6 | Quilt-VQA | HuggingFace (wisdomik/Quilt_VQA) + evaluations/quilt-llava | ì¡°ì§ë³‘ë¦¬í•™ VQA | ë¡œì»¬ ë³´ìœ  |

#### Text Medical QA (3ê°œ ì†ŒìŠ¤)
| # | ë°ì´í„°ì…‹ | ì†ŒìŠ¤ | íŠ¹ì§• | ìƒíƒœ |
|---|---|---|---|---|
| 1 | MedLFQA | HuggingFace (dmis-lab/MedLFQA) + evaluations/OLAPH | Long-form ì˜ë£Œ QA | ë¡œì»¬ ë³´ìœ  |
| 2 | MedQA/MedMCQA/MMLU | evaluations/self-biorag/data/benchmark/ | ê°ê´€ì‹ ì˜ë£Œ ì‹œí—˜ ë¬¸ì œ | ë¡œì»¬ ë³´ìœ  |
| 3 | Biomedical Instructions | databases/instruction/ | SFTìš© instruction ë°ì´í„° (52k+) | ë¡œì»¬ ë³´ìœ  |

#### EHR Record (1ê°œ ì†ŒìŠ¤)
| # | ë°ì´í„°ì…‹ | ì†ŒìŠ¤ | íŠ¹ì§• | ìƒíƒœ |
|---|---|---|---|---|
| 1 | MIMIC-III/IV | evaluations/mimic-code | EHR ì½”ë“œ, SQL concepts, ë²¤ì¹˜ë§ˆí¬ | ì½”ë“œ ë³´ìœ  (ë°ì´í„°ëŠ” ë³„ë„ ì ‘ê·¼ í•„ìš”) |

#### Knowledge Base
| # | ë¦¬ì†ŒìŠ¤ | ê²½ë¡œ | ìš©ë„ |
|---|---|---|---|
| 1 | Wikipedia 2018 dump | databases/wiki2018_en/ | ê²€ìƒ‰ ì‹œë®¬ë ˆì´ì…˜ìš© |
| 2 | Wikipedia 2026 dump | databases/wiki2026_en/ | ê²€ìƒ‰ ì‹œë®¬ë ˆì´ì…˜ìš© |
| 3 | MedCPT evidence | databases/retriever/ | top-10 ì˜ë£Œ ê·¼ê±° ê²€ìƒ‰ |
| 4 | Critic ë°ì´í„° | databases/critic/ | relevance/utility/groundness í‰ê°€ |
| 5 | Generator ë°ì´í„° | databases/generator/ | retrieval token ê¸°ë°˜ ìƒì„± |

### 2.3 ëª¨ë¸ í›„ë³´êµ°
| # | ëª¨ë¸ | í¬ê¸° | íŠ¹ì§• | ìš©ë„ |
|---|---|---|---|---|
| 1 | Lingshu-7B | 7B | ì˜ë£Œ MLLM, multi-modality | ì£¼ í•™ìŠµ ëŒ€ìƒ í›„ë³´ |
| 2 | Qwen2.5-VL-7B-Instruct | 7B | ë²”ìš© VLM, tool-use ì§€ì› | ì£¼ í•™ìŠµ ëŒ€ìƒ í›„ë³´ |
| 3 | Step3-VL-10B | 10B | VLM, ê³ ì„±ëŠ¥ | ë¹„êµ ì‹¤í—˜ìš© |

### 2.4 í•™ìŠµ í”„ë ˆì„ì›Œí¬ í˜„í™©
| í”„ë ˆì„ì›Œí¬ | ê²½ë¡œ | ìš©ë„ | ë¹„ê³  |
|---|---|---|---|
| Oumi | trains/oumi/ | SFT (Supervised Fine-Tuning) | ì´ë¯¸ agent SFT config ì¡´ì¬ |
| Snapshot-PO | trains/snapshot-po/ | SARL (Search Agent RL) | GRPO ê¸°ë°˜, ì‹¤í–‰ ë¡œê·¸ ì¡´ì¬ (260209~260212) |
| AgentGym-RL | GYM_reference/AgentGym-RL/ | Multi-turn RL (PPO/GRPO) | verl ê¸°ë°˜, í™˜ê²½ ì„œë²„ ì•„í‚¤í…ì²˜ |
| MRPO (ì°¸ê³ ) | references/medical_qa/ | VQA GRPO with token shaping | ICML ì œì¶œ ì½”ë“œ, BERTScore/ROUGE reward |

### 2.5 ì°¸ê³  ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜ ë¶„ì„

#### Ï„Â²-bench ë„ë©”ì¸ êµ¬ì¡° (databases/tau2-bench)
```
ë„ë©”ì¸ 1ê°œ êµ¬ì„± ìš”ì†Œ:
â”œâ”€â”€ src/tau2/domains/{domain}/
â”‚   â”œâ”€â”€ data_model.py    # DB ìŠ¤í‚¤ë§ˆ (Pydantic BaseModel)
â”‚   â”œâ”€â”€ tools.py         # ToolKitBase ìƒì†, @is_tool ë°ì½”ë ˆì´í„°
â”‚   â”œâ”€â”€ environment.py   # get_environment(), get_tasks() í•¨ìˆ˜
â”‚   â””â”€â”€ utils.py         # ê²½ë¡œ ì„¤ì • ë“±
â”œâ”€â”€ data/tau2/domains/{domain}/
â”‚   â”œâ”€â”€ db.json          # ì‹œë®¬ë ˆì´ì…˜ìš© ë°ì´í„°ë² ì´ìŠ¤
â”‚   â”œâ”€â”€ policy.md        # ì—ì´ì „íŠ¸ í–‰ë™ ì •ì±…
â”‚   â”œâ”€â”€ tasks.json       # í‰ê°€ìš© task ì‹œë‚˜ë¦¬ì˜¤
â”‚   â””â”€â”€ split_tasks.json # train/test ë¶„ë¦¬
â””â”€â”€ Gymnasium-compatible gym interface (gym_agent.py)
    â”œâ”€â”€ AgentGymEnv  - reset() â†’ observation, step(action) â†’ obs, reward, done
    â””â”€â”€ UserGymEnv   - ì‚¬ìš©ì ì—­í•  í”Œë ˆì´
```

#### AgentGym-RL ì•„í‚¤í…ì²˜ (GYM_reference/AgentGym-RL)
```
3ê°œ ëª¨ë“ˆ:
1. Environment Module: HTTP ì„œë²„ ê¸°ë°˜ í™˜ê²½, ë³‘ë ¬ ìš”ì²­ ì§€ì›
2. Agent Module: ì¶”ë¡ /ì˜ì‚¬ê²°ì •, ì¥ê¸° ê³„íš, self-reflection
3. Training Module: verl ê¸°ë°˜ PPO/GRPO/RLOO/REINFORCE++
   - RolloutHandler: attention mask, loss mask, position ids ì²˜ë¦¬
   - EnvClient: observation(), available_actions(), step(), reset()
   - RoundScheduler: fixed / scaling_inter_stepwise (ScalingInter-RL)
```

#### Tool Simulation íŒŒì´í”„ë¼ì¸ (tool_simulations/tool-dataset-generation)
```
íŒŒì´í”„ë¼ì¸ ë‹¨ê³„:
1. tool_generation/   â†’ ì§ˆë¬¸ì—ì„œ tool spec ìë™ ìƒì„± (LLM ê¸°ë°˜)
2. task_generation/   â†’ ì‹œë‚˜ë¦¬ì˜¤/ëŒ€í™” ìƒì„± (initial + continual)
3. tool_simulation/   â†’ LLMìœ¼ë¡œ tool ì‹¤í–‰ ê²°ê³¼ ì‹œë®¬ë ˆì´ì…˜
4. user_simulation/   â†’ ì‚¬ìš©ì í–‰ë™ ì‹œë®¬ë ˆì´ì…˜
5. response_generation/ â†’ ì‘ë‹µ ìƒì„±
6. validation/        â†’ í’ˆì§ˆ ê²€ì¦
```

---

## 3. ê¸°ìˆ  ì„¤ê³„ (Technical Design)

### 3.1 BIOAgents GYM ì•„í‚¤í…ì²˜ (ì„¤ê³„ì•ˆ)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    BIOAgents GYM                        â”‚
â”‚                                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Environment  â”‚  â”‚    Agent     â”‚  â”‚   Training   â”‚ â”‚
â”‚  â”‚    Module     â”‚  â”‚    Module    â”‚  â”‚    Module    â”‚ â”‚
â”‚  â”‚              â”‚  â”‚              â”‚  â”‚              â”‚ â”‚
â”‚  â”‚ Â·Medical     â”‚  â”‚ Â·Reasoning   â”‚  â”‚ Â·SFT (Oumi) â”‚ â”‚
â”‚  â”‚  Domains     â”‚  â”‚ Â·Tool Use    â”‚  â”‚ Â·GRPO       â”‚ â”‚
â”‚  â”‚ Â·Tool DB     â”‚  â”‚ Â·Planning    â”‚  â”‚ Â·PPO        â”‚ â”‚
â”‚  â”‚ Â·Simulation  â”‚  â”‚ Â·Reflection  â”‚  â”‚ Â·ScalingRL  â”‚ â”‚
â”‚  â”‚ Â·EHR System  â”‚  â”‚ Â·Multi-modal â”‚  â”‚ Â·Logging    â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚              Evaluation Suite                     â”‚  â”‚
â”‚  â”‚  Text QA â”‚ Visual QA â”‚ EHR Tasks â”‚ Agent Tasks   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3.2 Medical Domain ì„¤ê³„ (Ï„Â²-bench ìŠ¤íƒ€ì¼)

**ë„ë©”ì¸ ëª©ë¡ (ê³„íš):**

| Domain | ì„¤ëª… | ì£¼ìš” Tool | ë°ì´í„° |
|---|---|---|---|
| `clinical_diagnosis` | í™˜ì ì¦ìƒ â†’ ì§„ë‹¨ | search_symptoms, check_vitals, order_lab, get_lab_results, search_literature | í•©ì„± í™˜ì DB |
| `medical_qa` | ì˜ë£Œ ì§ˆë¬¸ ì‘ë‹µ | search_pubmed, browse_article, search_wiki, retrieve_evidence | MedQA/MMLU |
| `visual_diagnosis` | ì˜ë£Œ ì´ë¯¸ì§€ ë¶„ì„ | analyze_image, get_image_report, compare_images, search_similar_cases | VQA-RAD/SLAKE |
| `ehr_management` | EHR ì¡°íšŒ/ë¶„ì„ | query_patient_records, get_lab_trends, get_medications, check_interactions | MIMIC-III ê¸°ë°˜ |
| `drug_interaction` | ì•½ë¬¼ ìƒí˜¸ì‘ìš© ê²€ì¦ | search_drug, check_interaction, get_dosage, search_alternatives | ì•½ë¬¼ DB |

### 3.3 Medical Tool Database ì„¤ê³„ (ìƒì„¸)

```python
# ê³„íšëœ Tool ì¹´í…Œê³ ë¦¬ (ì´ ~25ê°œ tool)

# Category 1: Medical Knowledge Search
- search_pubmed(queries: list[str]) â†’ list[{title, abstract, pmid, url}]
- browse_article(pmid: str, query: str) â†’ str
- search_medical_wiki(queries: list[str]) â†’ list[{title, url, snippet}]
- browse_medical_wiki(url: str, query: str) â†’ str
- search_clinical_guidelines(condition: str) â†’ list[{guideline, source}]

# Category 2: Patient Record (EHR) Tools
- get_patient_info(patient_id: str) â†’ {demographics, conditions, allergies}
- get_lab_results(patient_id: str, test_type: str) â†’ list[{test, value, unit, date}]
- get_medication_list(patient_id: str) â†’ list[{drug, dose, frequency, start_date}]
- get_vital_signs(patient_id: str) â†’ {bp, hr, temp, spo2, rr}
- get_clinical_notes(patient_id: str, note_type: str) â†’ list[{date, content}]

# Category 3: Diagnostic Tools
- check_drug_interaction(drug_a: str, drug_b: str) â†’ {severity, description}
- calculate_clinical_score(score_type: str, params: dict) â†’ {score, interpretation}
- get_differential_diagnosis(symptoms: list[str]) â†’ list[{condition, probability}]
- order_lab_test(patient_id: str, test_type: str) â†’ {order_id, status}

# Category 4: Medical Image Analysis (ì‹œë®¬ë ˆì´ì…˜)
- analyze_medical_image(image_path: str, modality: str) â†’ {findings, confidence}
- get_image_report(image_id: str) â†’ {report, impression}
- compare_with_prior(current_id: str, prior_id: str) â†’ {changes, assessment}

# Category 5: Communication & Workflow
- transfer_to_specialist(summary: str, specialty: str) â†’ str
- schedule_followup(patient_id: str, reason: str) â†’ {appointment_id, date}
- send_patient_message(patient_id: str, message: str) â†’ str
```

### 3.4 Reward Function ì„¤ê³„

```python
# Multi-signal Reward (ì°¸ê³ : MRPO token shaping + AgentGym-RL)

# 1. Task Completion Reward (binary)
#    - ì˜ë£Œ taskë¥¼ ì˜¬ë°”ë¥´ê²Œ ì™„ë£Œí–ˆëŠ”ì§€ (ì •í™•í•œ ì§„ë‹¨, ì˜¬ë°”ë¥¸ ë‹µë³€ ë“±)

# 2. Tool Usage Reward (structured)
#    - ì˜¬ë°”ë¥¸ toolì„ ì˜¬ë°”ë¥¸ ìˆœì„œë¡œ ì‚¬ìš©í–ˆëŠ”ì§€
#    - ë¶ˆí•„ìš”í•œ tool callì´ ì—†ëŠ”ì§€

# 3. Medical Accuracy Reward (soft)
#    - ROUGE-1 + BLEU-1 + BERTScore (BiomedBERT ê¸°ë°˜)
#    - ì˜ë£Œ ì „ë¬¸ ìš©ì–´ì˜ ì •í™•ì„±

# 4. Process Reward (LLM-as-judge)
#    - Alignment: gold reasoningê³¼ ì¼ì¹˜í•˜ëŠ”ì§€
#    - Relevance: í™˜ì ì‚¬ë¡€ì— êµ¬ì²´ì ì¸ì§€
#    - Factuality: ì˜í•™ì ìœ¼ë¡œ ì‚¬ì‹¤ì¸ì§€

# 5. Format Reward (binary)
#    - <think>...</think> <answer>...</answer> í˜•ì‹ ì¤€ìˆ˜
```

---

## 4. íƒ€ì„ë¼ì¸ & ë¡œë“œë§µ

### Phase 1: ê¸°ë°˜ êµ¬ì¶• (2026.02.12 ~ 2026.03.15) [4ì£¼]

| ì£¼ì°¨ | ì‘ì—… | ì‚°ì¶œë¬¼ | ìƒíƒœ |
|---|---|---|---|
| W1 (02/12~02/18) | í”„ë¡œì íŠ¸ êµ¬ì¡° ì„¤ê³„ & ê¸°íš ë¬¸ì„œ | PLANNING.md, ë””ë ‰í† ë¦¬ êµ¬ì¡° | ğŸ”„ ì§„í–‰ì¤‘ |
| W1 | Medical Tool Database ìŠ¤í‚¤ë§ˆ ì„¤ê³„ | tools.py, data_model.py ì´ˆì•ˆ | â¬œ ëŒ€ê¸° |
| W2 (02/19~02/25) | Medical Domain í™˜ê²½ êµ¬í˜„ (clinical_diagnosis) | environment.py, policy.md | â¬œ ëŒ€ê¸° |
| W2 | Tool Simulation ì—”ì§„ ì˜ë£Œ ë„ë©”ì¸ ì ìš© | tool simulation prompts | â¬œ ëŒ€ê¸° |
| W3 (02/26~03/04) | Task ì‹œë‚˜ë¦¬ì˜¤ ìƒì„± (50+ tasks) | tasks.json, db.json | â¬œ ëŒ€ê¸° |
| W3 | GYM ì¸í„°í˜ì´ìŠ¤ êµ¬í˜„ (Gymnasium-compatible) | gym_agent.py | â¬œ ëŒ€ê¸° |
| W4 (03/05~03/15) | ë°ì´í„°ì…‹ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ | datasets/ êµ¬ì„± | â¬œ ëŒ€ê¸° |
| W4 | ê¸°ë³¸ í‰ê°€ íŒŒì´í”„ë¼ì¸ êµ¬ì¶• | eval scripts | â¬œ ëŒ€ê¸° |

### Phase 2: í•™ìŠµ íŒŒì´í”„ë¼ì¸ (2026.03.15 ~ 2026.04.15) [4ì£¼]

| ì£¼ì°¨ | ì‘ì—… | ì‚°ì¶œë¬¼ | ìƒíƒœ |
|---|---|---|---|
| W5 (03/15~03/22) | SFT ë°ì´í„° êµ¬ì„± (instruction + tool-use) | SFT jsonl ë°ì´í„° | â¬œ ëŒ€ê¸° |
| W5 | ëª¨ë¸ ì„ ì • & baseline í‰ê°€ | baseline ê²°ê³¼ ë¡œê·¸ | â¬œ ëŒ€ê¸° |
| W6 (03/22~03/29) | SFT í•™ìŠµ (Oumi) | SFT ì²´í¬í¬ì¸íŠ¸ | â¬œ ëŒ€ê¸° |
| W6 | additional Medical Domain êµ¬í˜„ (medical_qa, visual_diagnosis) | ì¶”ê°€ ë„ë©”ì¸ ì½”ë“œ | â¬œ ëŒ€ê¸° |
| W7 (03/29~04/05) | RL í•™ìŠµ ì‹œì‘ (GRPO, GYM í™˜ê²½ ì—°ë™) | RL ì²´í¬í¬ì¸íŠ¸ | â¬œ ëŒ€ê¸° |
| W7 | Trajectory ë¡œê¹… ì‹œìŠ¤í…œ êµ¬ì¶• | trajectory íŒŒì¼ë“¤ | â¬œ ëŒ€ê¸° |
| W8 (04/05~04/15) | ScalingInter-RL ì ìš© ì‹¤í—˜ | í•™ìŠµ ê³¡ì„ , ë¹„êµ ê²°ê³¼ | â¬œ ëŒ€ê¸° |
| W8 | ì¤‘ê°„ í‰ê°€ (Text QA + Visual QA) | ì¤‘ê°„ ê²°ê³¼ ë¦¬í¬íŠ¸ | â¬œ ëŒ€ê¸° |

### Phase 3: ë°˜ë³µ ê°œì„  (2026.04.15 ~ 2026.05.15) [4ì£¼]

| ì£¼ì°¨ | ì‘ì—… | ì‚°ì¶œë¬¼ | ìƒíƒœ |
|---|---|---|---|
| W9 | EHR Domain êµ¬í˜„ & í•™ìŠµ | EHR ë„ë©”ì¸ ì½”ë“œ | â¬œ ëŒ€ê¸° |
| W10 | Reward function ê°œì„  & ì‹¤í—˜ | ablation ê²°ê³¼ | â¬œ ëŒ€ê¸° |
| W11 | Multi-domain í†µí•© í•™ìŠµ | í†µí•© ì²´í¬í¬ì¸íŠ¸ | â¬œ ëŒ€ê¸° |
| W12 | ì „ì²´ ë²¤ì¹˜ë§ˆí¬ í‰ê°€ | ìµœì¢… ê²°ê³¼ í…Œì´ë¸” | â¬œ ëŒ€ê¸° |

### Phase 4: ë…¼ë¬¸ ì‘ì„± (2026.05.15 ~ 2026.06.01) [2ì£¼]

| ì£¼ì°¨ | ì‘ì—… | ì‚°ì¶œë¬¼ | ìƒíƒœ |
|---|---|---|---|
| W13 | ë…¼ë¬¸ ì´ˆì•ˆ ì‘ì„± | paper draft | â¬œ ëŒ€ê¸° |
| W14 | ì¶”ê°€ ì‹¤í—˜ + ë…¼ë¬¸ ì™„ì„± | final paper | â¬œ ëŒ€ê¸° |

---

## 5. Related Work ë¶„ì„

### 5.1 ê°€ì¥ ê´€ë ¨ëœ ê¸°ì¡´ ì—°êµ¬

#### (1) AgentClinic (EMNLP 2024 Findings)
- **ë…¼ë¬¸**: `references/medical_agent/2024.findings-emnlp.510.pdf`
- **í•µì‹¬**: í™˜ì/ì˜ì‚¬/ì¸¡ì •/ì¡°ì • ì—ì´ì „íŠ¸ë¡œ êµ¬ì„±ëœ ì„ìƒ ì‹œë®¬ë ˆì´ì…˜
- **í•œê³„**: ì§„ë‹¨ ì‹œë‚˜ë¦¬ì˜¤ì— í•œì •, tool ë‹¤ì–‘ì„± ë¶€ì¡±, RL í•™ìŠµ ì—†ìŒ
- **ì°¸ê³ ì **: ë‹¤ì¤‘ ì—­í•  ì—ì´ì „íŠ¸ êµ¬ì¡°, í¸í–¥ ì‹œë®¬ë ˆì´ì…˜

#### (2) AgentGym-RL (arXiv:2509.08755)
- **ë…¼ë¬¸**: `references/medical_qa/2509.08755v1.pdf`
- **í•µì‹¬**: verl ê¸°ë°˜ multi-turn RL, ScalingInter-RL ì•Œê³ ë¦¬ì¦˜
- **í•œê³„**: ì˜ë£Œ ë„ë©”ì¸ í™˜ê²½ ì—†ìŒ
- **ì°¸ê³ ì **: ì•„í‚¤í…ì²˜, í•™ìŠµ íŒŒì´í”„ë¼ì¸, RoundScheduler

#### (3) Ï„Â²-bench (arXiv:2506.07982)
- **ì½”ë“œ**: `databases/tau2-bench/`
- **í•µì‹¬**: ë„ë©”ì¸ë³„ tool+DB+policy+task êµ¬ì¡°, Gymnasium ì¸í„°í˜ì´ìŠ¤
- **í•œê³„**: airline/retail/telecomë§Œ
- **ì°¸ê³ ì **: ë„ë©”ì¸ êµ¬ì¡° íŒ¨í„´, í‰ê°€ ì²´ê³„

#### (4) Self-BioRAG (arXiv:2305.10415)
- **ì½”ë“œ**: `evaluations/self-biorag/`
- **í•µì‹¬**: ì˜ë£Œ RAG, retrieval critic, MedQA/MMLU í‰ê°€
- **ì°¸ê³ ì **: ì˜ë£Œ ì§€ì‹ ê²€ìƒ‰ êµ¬ì¡°, ë²¤ì¹˜ë§ˆí¬ ë°ì´í„°

#### (5) MRPO (ICML submission)
- **ì½”ë“œ**: `references/medical_qa/grpo_vqa_Qwen3_token_shaping.py`
- **í•µì‹¬**: Medical VQAì— GRPO + token shaping ì ìš©
- **ì°¸ê³ ì **: process reward (Alignment/Relevance/Factuality), BERTScore reward

#### (6) Lingshu
- **URL**: https://huggingface.co/lingshu-medical-mllm/Lingshu-7B
- **í•µì‹¬**: ì˜ë£Œ íŠ¹í™” MLLM, MedEvalKit
- **ì°¸ê³ ì **: ëª¨ë¸ í›„ë³´, í‰ê°€ í”„ë ˆì„ì›Œí¬

### 5.2 ì¶”ê°€ ì°¸ê³  ë…¼ë¬¸ (references/ ë‚´)
| íŒŒì¼ëª… | ì¶”ì • ë‚´ìš© |
|---|---|
| `medical_agent/2404.15155v3.pdf` | ì˜ë£Œ agent ê´€ë ¨ (2024) |
| `medical_agent/2411.00248v2.pdf` | ì˜ë£Œ agent ê´€ë ¨ (2024) |
| `medical_agent/2505.16100v1.pdf` | ì˜ë£Œ agent ê´€ë ¨ (2025) |
| `medical_qa/2003.10286v1.pdf` | ì˜ë£Œ QA ê´€ë ¨ (2020) |
| `medical_qa/2009.13081v1.pdf` | ì˜ë£Œ QA ê´€ë ¨ (2020) |
| `medical_qa/2309.11080v1.pdf` | ì˜ë£Œ QA ê´€ë ¨ (2023) |
| `medical_qa/2405.12701v3.pdf` | ì˜ë£Œ QA ê´€ë ¨ (2024) |
| `medical_qa/2506.09513v3.pdf` | ì˜ë£Œ QA ê´€ë ¨ (2025) |
| `medical_qa/2508.19096v1.pdf` | ì˜ë£Œ QA ê´€ë ¨ (2025) |
| `medical_qa/sdata2018251.pdf` | ì˜ë£Œ ë°ì´í„° ê´€ë ¨ |
| `medical_qa/SLAKE.pdf` | SLAKE ë°ì´í„°ì…‹ ë…¼ë¬¸ |

---

## 6. í•µì‹¬ ê¸°ìˆ  ê²°ì • ì‚¬í•­

### 6.1 ê²°ì •ëœ ì‚¬í•­
- [x] GYM êµ¬ì¡°: Ï„Â²-bench ìŠ¤íƒ€ì¼ ë„ë©”ì¸ êµ¬ì¡° + Gymnasium ì¸í„°í˜ì´ìŠ¤
- [x] RL ì•Œê³ ë¦¬ì¦˜: GRPO (ì£¼), PPO (ë¹„êµ), ScalingInter-RL (ì‹¤í—˜)
- [x] Tool Simulation: LLM ê¸°ë°˜ ì‹œë®¬ë ˆì´ì…˜ (tool-dataset-generation ì°¸ê³ )
- [x] í‰ê°€ ë²¤ì¹˜ë§ˆí¬: MedQA, MedMCQA, MMLU, VQA-RAD, SLAKE, PathVQA, PMC-VQA
- [x] ê°€ìš© ìì›: A100 8ëŒ€

### 6.2 ê²°ì • ì‚¬í•­ (2026-02-12 í™•ì •)
- [x] **ì£¼ ëª¨ë¸ ì„ íƒ**: Lingshu-7B (ì˜ë£Œ íŠ¹í™”) ìš°ì„  â†’ ì´í›„ Qwen2.5-VL-7Bë¡œ í™•ì¥
- [ ] **EHR ë°ì´í„° ì ‘ê·¼**: MIMIC-III/IV ë°ì´í„° ì‹¤ì œ ì ‘ê·¼ ê°€ëŠ¥ ì—¬ë¶€ (í™•ì¸ í•„ìš”)
- [x] **ë„ë©”ì¸ ìš°ì„ ìˆœìœ„**: clinical_diagnosis â†’ medical_qa â†’ visual_diagnosis â†’ ehr_management â†’ drug_interaction
- [ ] **Tool Simulation vs Real API**: ì–´ë””ê¹Œì§€ ì‹œë®¬ë ˆì´ì…˜, ì–´ë””ë¶€í„° ì‹¤ì œ API?
- [x] **ë…¼ë¬¸ í¬ì§€ì…”ë‹**: Framework paper (BIOAgents GYM ìì²´ê°€ contribution)
- [x] **ë…¼ë¬¸ ì‘ì„±**: ì‚¬ìš©ìê°€ ì§ì ‘ ì‘ì„±, AIëŠ” ëª¨ë“  ì‹¤í—˜/êµ¬í˜„/ë¶„ì„ ìˆ˜í–‰ ë° ê¸°ë¡

---

## 7. ì‹¤í—˜ ë¡œê·¸ (Experiment Log)

### [2026-02-12] í”„ë¡œì íŠ¸ ì‹œì‘
- í”„ë¡œì íŠ¸ êµ¬ì¡° ë¶„ì„ ì™„ë£Œ
- ê¸°íš ë¬¸ì„œ ì´ˆì•ˆ ì‘ì„±
- ë³´ìœ  ë¦¬ì†ŒìŠ¤ ì „ìˆ˜ ì¡°ì‚¬ ì™„ë£Œ
- Related work ì„œë² ì´ ì‹œì‘

### í–¥í›„ ê¸°ë¡ í˜•ì‹
```
### [YYYY-MM-DD] ì‘ì—… ì œëª©
- **ì‘ì—… ë‚´ìš©**: ìˆ˜í–‰í•œ ì‘ì—… ìƒì„¸
- **ì‚¬ìš© ëª¨ë¸/ë°ì´í„°**: 
- **ê²°ê³¼ ìš”ì•½**: 
- **ë‹¤ìŒ ë‹¨ê³„**: 
- **ê´€ë ¨ íŒŒì¼**: ê²½ë¡œ ëª©ë¡
```

---

## 8. ë¦¬ìŠ¤í¬ & ëŒ€ì‘ ì „ëµ

| ë¦¬ìŠ¤í¬ | ì˜í–¥ | í™•ë¥  | ëŒ€ì‘ |
|---|---|---|---|
| MIMIC ë°ì´í„° ì ‘ê·¼ ë¶ˆê°€ | EHR ë„ë©”ì¸ êµ¬í˜„ ë¶ˆê°€ | ì¤‘ | í•©ì„± EHR ë°ì´í„°ë¡œ ëŒ€ì²´ |
| RL í•™ìŠµ ë¶ˆì•ˆì • | ì„±ëŠ¥ ì €í•˜ | ê³  | SFT warmup + KL penalty + ScalingInter |
| A100 8ëŒ€ ë¦¬ì†ŒìŠ¤ ë¶€ì¡± | ëŒ€í˜• ëª¨ë¸ í•™ìŠµ ë¶ˆê°€ | ì¤‘ | 7B ëª¨ë¸ ì§‘ì¤‘, LoRA/QLoRA ì ìš© |
| NeurIPS ë§ˆê° (6ì›”) | ì‹œê°„ ë¶€ì¡± | ì¤‘ | Phase 1-2 ì—„ê²© ê´€ë¦¬, MVP ìš°ì„  |
| Tool simulation í’ˆì§ˆ | ë¹„í˜„ì‹¤ì  ê²°ê³¼ | ì¤‘ | GPT-5/Claudeë¡œ ê³ í’ˆì§ˆ simulation |

---

## 9. ì½”ë“œ ì»¨ë²¤ì…˜ & ë¡œê¹… ê·œì¹™

### 9.1 ë””ë ‰í† ë¦¬ ê·œì¹™
- ëª¨ë“  ì‹¤í—˜ ê²°ê³¼ëŠ” `logs/` ë””ë ‰í† ë¦¬ì— ë‚ ì§œë³„ ì €ì¥
- ì²´í¬í¬ì¸íŠ¸ëŠ” `checkpoints/` ë””ë ‰í† ë¦¬ì— ì‹¤í—˜ëª…_ë‚ ì§œë¡œ ì €ì¥
- í•™ìŠµ ì„¤ì •ì€ `configs/` ë””ë ‰í† ë¦¬ì— YAMLë¡œ ê´€ë¦¬

### 9.2 ë¡œê¹… ê·œì¹™
- ëª¨ë“  í•™ìŠµì€ W&B (Weights & Biases)ì— ê¸°ë¡
- TrajectoryëŠ” JSON í˜•ì‹ìœ¼ë¡œ ì „ì²´ ì €ì¥
- í‰ê°€ ê²°ê³¼ëŠ” í‘œì¤€ JSON í˜•ì‹ìœ¼ë¡œ ì €ì¥
- ì½”ë“œ ë³€ê²½ì€ Git commitìœ¼ë¡œ ì¶”ì 

### 9.3 íŒŒì¼ ëª…ëª… ê·œì¹™
- ë°ì´í„°: `{domain}_{split}_{version}.json`
- ì„¤ì •: `{model}_{method}_{date}.yaml`
- ë¡œê·¸: `{experiment_name}_{date}_log.txt`
- ì²´í¬í¬ì¸íŠ¸: `{model}_{method}_{step}/`

---

*ì´ ë¬¸ì„œëŠ” í”„ë¡œì íŠ¸ ì§„í–‰ì— ë”°ë¼ ì§€ì†ì ìœ¼ë¡œ ì—…ë°ì´íŠ¸ë©ë‹ˆë‹¤.*

```

ì´ì œ ì˜¤ëŠ˜ì ì¼ì¼ ë¡œê·¸ë¥¼ ìƒì„±í•˜ê³ , ë°”ë¡œ clinical_diagnosis ë„ë©”ì¸ êµ¬í˜„ì— ë“¤ì–´ê°‘ë‹ˆë‹¤.

```markdown
# Daily Log: 2026-02-12 (Day 1)

## ì˜¤ëŠ˜ì˜ ëª©í‘œ
- [x] í”„ë¡œì íŠ¸ ì „ì²´ ë¦¬ì†ŒìŠ¤ ë¶„ì„ & PLANNING.md ì‘ì„±
- [x] ê²°ì •ì‚¬í•­ í™•ì • (ëª¨ë¸: Lingshu-7B ìš°ì„ , ë„ë©”ì¸: clinical_diagnosis ìš°ì„ , ë…¼ë¬¸: framework paper)
- [x] í”„ë¡œì íŠ¸ ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„±
- [ ] clinical_diagnosis ë„ë©”ì¸ core êµ¬í˜„ ì‹œì‘
  - [ ] data_model.py (DB ìŠ¤í‚¤ë§ˆ)
  - [ ] tools.py (Medical tools)
  - [ ] environment.py (í™˜ê²½)
  - [ ] policy.md (ì—ì´ì „íŠ¸ ì •ì±…)
  - [ ] db.json (ì‹œë®¬ë ˆì´ì…˜ DB)
  - [ ] tasks.json (í‰ê°€ task)
- [ ] base environment & tool framework êµ¬í˜„

## ì‘ì—… ê¸°ë¡

### 09:00 - í”„ë¡œì íŠ¸ ë¶„ì„ ì™„ë£Œ
- **ìˆ˜í–‰**: ì „ì²´ ë””ë ‰í† ë¦¬ íŠ¸ë¦¬ ë¶„ì„, ë³´ìœ  ë°ì´í„°ì…‹/ëª¨ë¸/í”„ë ˆì„ì›Œí¬ ì „ìˆ˜ ì¡°ì‚¬
- **ë°œê²¬ì‚¬í•­**:
  - Ï„Â²-bench ë„ë©”ì¸ íŒ¨í„´ì´ ë§¤ìš° ê¹”ë” (data_model â†’ tools â†’ environment êµ¬ì¡°)
  - AgentGym-RLì˜ verl ê¸°ë°˜ RL trainer í™œìš© ê°€ëŠ¥
  - tool-dataset-generationì˜ tool simulation ì—”ì§„ ì¬í™œìš© ê°€ëŠ¥
  - MRPOì˜ BERTScore+ROUGE reward ì²´ê³„ ì°¸ê³  ê°€ëŠ¥
  - databases/instructionì— 52k+ ì˜ë£Œ instruction ë°ì´í„° ë³´ìœ 
- **ì‚°ì¶œë¬¼**: `PLANNING.md` (ì „ì²´ ê¸°íš ë¬¸ì„œ)

### 09:30 - ê²°ì •ì‚¬í•­ í™•ì •
- ì£¼ ëª¨ë¸: Lingshu-7B â†’ Qwen2.5-VL-7B ìˆœì„œ
- ë„ë©”ì¸: clinical_diagnosis ë¨¼ì €
- ë…¼ë¬¸: framework paper
- ì¼ì¼ ë¡œê·¸ ì‹œìŠ¤í…œ: logs/daily/YYYY-MM-DD.md

### 10:00 - ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„±
```
bioagents/                    # ë©”ì¸ íŒ¨í‚¤ì§€
â”œâ”€â”€ domains/                  # ë„ë©”ì¸ë³„ ì½”ë“œ
â”‚   â”œâ”€â”€ clinical_diagnosis/   # ì„ìƒ ì§„ë‹¨ ë„ë©”ì¸
â”‚   â”œâ”€â”€ medical_qa/           # ì˜ë£Œ QA ë„ë©”ì¸
â”‚   â”œâ”€â”€ visual_diagnosis/     # ì‹œê° ì§„ë‹¨ ë„ë©”ì¸
â”‚   â”œâ”€â”€ ehr_management/       # EHR ê´€ë¦¬ ë„ë©”ì¸
â”‚   â””â”€â”€ drug_interaction/     # ì•½ë¬¼ ìƒí˜¸ì‘ìš© ë„ë©”ì¸
â”œâ”€â”€ gym/                      # Gymnasium ì¸í„°í˜ì´ìŠ¤
â”œâ”€â”€ environment/              # í™˜ê²½ ë² ì´ìŠ¤ í´ë˜ìŠ¤
â”œâ”€â”€ tools/                    # ë„êµ¬ ë² ì´ìŠ¤ í´ë˜ìŠ¤
â”œâ”€â”€ evaluation/               # í‰ê°€ ëª¨ë“ˆ
â”œâ”€â”€ training/                 # í•™ìŠµ ëª¨ë“ˆ
â””â”€â”€ utils/                    # ìœ í‹¸ë¦¬í‹°
data/domains/                 # ë„ë©”ì¸ë³„ ë°ì´í„° (db.json, tasks.json, policy.md)
configs/                      # í•™ìŠµ/í‰ê°€ ì„¤ì •
checkpoints/                  # ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸
logs/daily/                   # ì¼ì¼ ì‘ì—… ë¡œê·¸
```

### 10:30 - clinical_diagnosis ë„ë©”ì¸ êµ¬í˜„ ì‹œì‘
(ì•„ë˜ ê³„ì† ê¸°ë¡)

---

## ì‚¬ìš© ë¦¬ì†ŒìŠ¤
- ì°¸ê³  ì½”ë“œ: databases/tau2-bench/src/tau2/domains/mock/
- ì°¸ê³  ì½”ë“œ: tool_simulations/tool-dataset-generation/utils/tool_simulation/prompt.py

## ë‚´ì¼ í•  ì¼
- clinical_diagnosis ë„ë©”ì¸ ì™„ì„±
- GYM ì¸í„°í˜ì´ìŠ¤ êµ¬í˜„
- ì²« ë²ˆì§¸ tool simulation í…ŒìŠ¤íŠ¸

```

ì´ì œ í•µì‹¬ êµ¬í˜„ì„ ì‹œì‘í•©ë‹ˆë‹¤. Ï„Â²-bench íŒ¨í„´ì„ ë”°ë¼ base í”„ë ˆì„ì›Œí¬ë¶€í„° ë§Œë“¤ê³ , clinical_diagnosis ë„ë©”ì¸ì„ êµ¬í˜„í•˜ê² ìŠµë‹ˆë‹¤.

```python
"""BIOAgents - Medical/Biomedical Agent GYM Framework."""

__version__ = "0.1.0"

```

```python
"""Base environment and database classes for BIOAgents domains."""

from bioagents.environment.db import DB
from bioagents.environment.environment import Environment
from bioagents.environment.toolkit import ToolKitBase, ToolType, is_tool

__all__ = ["DB", "Environment", "ToolKitBase", "ToolType", "is_tool"]

```

Ï„Â²-benchì˜ base í´ë˜ìŠ¤ êµ¬ì¡°ë¥¼ ì°¸ê³ í•˜ë˜, ì˜ë£Œ ë„ë©”ì¸ì— ë§ê²Œ í™•ì¥í•©ë‹ˆë‹¤.

ì´ì œ Ï„Â²-bench êµ¬ì¡°ë¥¼ ì™„ì „íˆ íŒŒì•…í–ˆìŠµë‹ˆë‹¤. BIOAgentsë§Œì˜ ë…ë¦½ì ì¸ base frameworkì„ ë§Œë“¤ê³  clinical_diagnosis ë„ë©”ì¸ì„ êµ¬í˜„í•˜ê² ìŠµë‹ˆë‹¤.

```python
"""Base database class for BIOAgents domains."""

import hashlib
import json
from pathlib import Path
from typing import Any, Optional

from pydantic import BaseModel


class DB(BaseModel):
    """Domain database base class.
    
    All domain databases (patient records, drug databases, etc.) inherit from this.
    Provides JSON serialization, loading, and hashing.
    """

    @classmethod
    def load(cls, path: str) -> "DB":
        """Load the database from a JSON file."""
        with open(path, "r", encoding="utf-8") as f:
            data = json.load(f)
        return cls.model_validate(data)

    def dump(self, path: str) -> None:
        """Dump the database to a JSON file."""
        Path(path).parent.mkdir(parents=True, exist_ok=True)
        with open(path, "w", encoding="utf-8") as f:
            json.dump(self.model_dump(), f, indent=2, ensure_ascii=False, default=str)

    def get_hash(self) -> str:
        """Get a deterministic hash of the database state."""
        data_str = json.dumps(self.model_dump(), sort_keys=True, default=str)
        return hashlib.sha256(data_str.encode()).hexdigest()[:16]

    def get_json_schema(self) -> dict[str, Any]:
        """Get the JSON schema of the database."""
        return self.model_json_schema()

    def update(self, update_data: dict[str, Any]) -> None:
        """Update database fields with new data."""
        for key, value in update_data.items():
            if hasattr(self, key):
                current = getattr(self, key)
                if isinstance(current, dict) and isinstance(value, dict):
                    current.update(value)
                else:
                    setattr(self, key, value)

```

```python
"""Tool framework for BIOAgents domains.

Provides the base toolkit class, tool decorator, and tool type definitions.
Follows Ï„Â²-bench patterns adapted for medical domains.
"""

import inspect
import json
from enum import Enum
from typing import Any, Callable, Dict, Optional, TypeVar, get_type_hints

from pydantic import BaseModel, Field

from bioagents.environment.db import DB

TOOL_ATTR = "__bioagent_tool__"
TOOL_TYPE_ATTR = "__bioagent_tool_type__"

T = TypeVar("T", bound=DB)


class ToolType(str, Enum):
    """Type of a tool."""
    READ = "read"        # Read-only (queries, lookups)
    WRITE = "write"      # Modifies state (orders, prescriptions)
    THINK = "think"      # Internal reasoning
    GENERIC = "generic"  # General purpose (transfer, calculate)


def is_tool(tool_type: ToolType = ToolType.READ):
    """Decorator to mark a method as an available tool.
    
    Args:
        tool_type: The type of tool (READ, WRITE, THINK, GENERIC)
    
    Usage:
        @is_tool(ToolType.READ)
        def search_pubmed(self, query: str) -> list[dict]:
            ...
    """
    def decorator(func):
        setattr(func, TOOL_ATTR, True)
        setattr(func, TOOL_TYPE_ATTR, tool_type)
        return func
    return decorator


class ToolDefinition(BaseModel):
    """OpenAI-compatible tool definition for function calling."""
    type: str = "function"
    function: dict = Field(description="Function specification")

    @classmethod
    def from_method(cls, name: str, method: Callable) -> "ToolDefinition":
        """Create a tool definition from a method's signature and docstring."""
        sig = inspect.signature(method)
        doc = inspect.getdoc(method) or ""
        
        # Parse docstring for description and args
        lines = doc.strip().split("\n")
        description = lines[0] if lines else name
        
        # Build parameters from signature
        properties = {}
        required = []
        
        for param_name, param in sig.parameters.items():
            if param_name == "self":
                continue
            
            # Get type annotation
            param_type = "string"
            annotation = param.annotation
            if annotation != inspect.Parameter.empty:
                type_map = {
                    str: "string",
                    int: "integer",
                    float: "number",
                    bool: "boolean",
                    list: "array",
                    dict: "object",
                }
                # Handle basic types
                origin = getattr(annotation, "__origin__", None)
                if origin is list:
                    param_type = "array"
                elif origin is dict:
                    param_type = "object"
                elif annotation in type_map:
                    param_type = type_map[annotation]
                else:
                    param_type = "string"
            
            # Parse description from docstring Args section
            param_desc = f"Parameter: {param_name}"
            in_args = False
            for line in lines:
                stripped = line.strip()
                if stripped.startswith("Args:"):
                    in_args = True
                    continue
                if in_args and stripped.startswith(f"{param_name}:"):
                    param_desc = stripped.split(":", 1)[1].strip()
                    break
                if in_args and stripped.startswith("Returns:"):
                    break
            
            properties[param_name] = {
                "type": param_type,
                "description": param_desc,
            }
            
            if param.default == inspect.Parameter.empty:
                required.append(param_name)
        
        return cls(
            type="function",
            function={
                "name": name,
                "description": description,
                "parameters": {
                    "type": "object",
                    "properties": properties,
                    "required": required,
                }
            }
        )


class _ToolKitMeta(type):
    """Metaclass that collects @is_tool decorated methods."""
    
    def __init__(cls, name, bases, attrs):
        func_tools = {}
        for attr_name, method in attrs.items():
            if isinstance(method, property):
                method = method.fget
            if hasattr(method, TOOL_ATTR):
                func_tools[attr_name] = method
        
        # Store discovered tools
        cls._declared_tools = func_tools
        super().__init__(name, bases, attrs)


class ToolKitBase(metaclass=_ToolKitMeta):
    """Base class for domain tool kits.
    
    Subclass this to define domain-specific tools:
    
        class ClinicalTools(ToolKitBase):
            db: ClinicalDB
            
            @is_tool(ToolType.READ)
            def get_patient_info(self, patient_id: str) -> dict:
                ...
    """
    
    def __init__(self, db: Optional[T] = None):
        self.db: Optional[T] = db
    
    @property
    def tools(self) -> Dict[str, Callable]:
        """Get all registered tool methods."""
        result = {}
        # Collect from all classes in MRO
        for klass in type(self).__mro__:
            if hasattr(klass, "_declared_tools"):
                for name in klass._declared_tools:
                    if name not in result:
                        result[name] = getattr(self, name)
        return result
    
    def use_tool(self, tool_name: str, **kwargs) -> Any:
        """Execute a tool by name with given arguments."""
        if tool_name not in self.tools:
            available = list(self.tools.keys())
            raise ValueError(
                f"Tool '{tool_name}' not found. Available tools: {available}"
            )
        return self.tools[tool_name](**kwargs)
    
    def has_tool(self, tool_name: str) -> bool:
        """Check if a tool exists."""
        return tool_name in self.tools
    
    def get_tool_type(self, tool_name: str) -> ToolType:
        """Get the type of a tool."""
        method = self.tools.get(tool_name)
        if method is None:
            raise ValueError(f"Tool '{tool_name}' not found.")
        return getattr(method, TOOL_TYPE_ATTR, ToolType.GENERIC)
    
    def get_tool_definitions(self) -> list[ToolDefinition]:
        """Get OpenAI-compatible tool definitions for all tools."""
        definitions = []
        for name, method in sorted(self.tools.items()):
            definitions.append(ToolDefinition.from_method(name, method))
        return definitions
    
    def get_tool_definitions_dict(self) -> list[dict]:
        """Get tool definitions as a list of dicts (for JSON serialization)."""
        return [td.model_dump() for td in self.get_tool_definitions()]
    
    def get_db_hash(self) -> Optional[str]:
        """Get hash of the underlying database."""
        if self.db is None:
            return None
        return self.db.get_hash()
    
    def get_statistics(self) -> dict[str, Any]:
        """Get statistics about the toolkit."""
        tools = self.tools
        return {
            "num_tools": len(tools),
            "num_read_tools": sum(1 for n in tools if self.get_tool_type(n) == ToolType.READ),
            "num_write_tools": sum(1 for n in tools if self.get_tool_type(n) == ToolType.WRITE),
            "num_think_tools": sum(1 for n in tools if self.get_tool_type(n) == ToolType.THINK),
            "num_generic_tools": sum(1 for n in tools if self.get_tool_type(n) == ToolType.GENERIC),
            "tool_names": sorted(tools.keys()),
        }

```

```python
"""Base Environment class for BIOAgents domains.

Manages the domain state, tool execution, and agent interaction protocol.
"""

import json
from copy import deepcopy
from datetime import datetime
from typing import Any, Literal, Optional

from pydantic import BaseModel, Field

from bioagents.environment.db import DB
from bioagents.environment.toolkit import ToolKitBase, ToolDefinition


class ToolCall(BaseModel):
    """Represents a tool call from an agent."""
    id: str = Field(description="Unique identifier for this tool call")
    name: str = Field(description="Name of the tool to call")
    arguments: dict = Field(default_factory=dict, description="Arguments for the tool")
    requestor: Literal["user", "assistant"] = Field(default="assistant")


class ToolMessage(BaseModel):
    """Response from a tool execution."""
    id: str = Field(description="Matches the tool call id")
    content: str = Field(description="Result of the tool execution")
    role: str = Field(default="tool")
    error: bool = Field(default=False, description="Whether the tool call resulted in an error")


class Message(BaseModel):
    """A message in the conversation."""
    role: Literal["system", "user", "assistant", "tool"] = Field(description="Message role")
    content: Optional[str] = Field(default=None, description="Message content")
    tool_calls: Optional[list[ToolCall]] = Field(default=None, description="Tool calls")
    tool_call_id: Optional[str] = Field(default=None, description="For tool responses")
    timestamp: Optional[str] = Field(default=None)
    turn_idx: Optional[int] = Field(default=None)


class EnvironmentState(BaseModel):
    """Current state of the environment."""
    messages: list[Message] = Field(default_factory=list)
    turn_count: int = Field(default=0)
    terminated: bool = Field(default=False)
    truncated: bool = Field(default=False)
    reward: float = Field(default=0.0)
    info: dict = Field(default_factory=dict)


class Environment:
    """Base environment for BIOAgents domains.
    
    Manages:
    - Domain policy enforcement
    - Tool execution and state management
    - Conversation history tracking
    - Reward computation interface
    
    Usage:
        db = ClinicalDB.load("data/domains/clinical_diagnosis/db.json")
        tools = ClinicalTools(db)
        env = Environment(
            domain_name="clinical_diagnosis",
            policy=open("policy.md").read(),
            tools=tools,
        )
    """

    def __init__(
        self,
        domain_name: str,
        policy: str,
        tools: Optional[ToolKitBase] = None,
        user_tools: Optional[ToolKitBase] = None,
        max_turns: int = 20,
    ):
        self.domain_name = domain_name
        self.policy = policy
        self.tools = tools
        self.user_tools = user_tools
        self.max_turns = max_turns
        self.state = EnvironmentState()
        self._initial_db_hash = tools.get_db_hash() if tools else None

    def reset(self) -> tuple[Optional[str], dict]:
        """Reset the environment to initial state.
        
        Returns:
            observation: Initial observation (system prompt + context)
            info: Dictionary with tools, policy, etc.
        """
        self.state = EnvironmentState()
        
        info = {
            "domain_name": self.domain_name,
            "policy": self.policy,
            "tools": self.tools.get_tool_definitions_dict() if self.tools else [],
            "max_turns": self.max_turns,
        }
        
        return None, info

    def step(self, action: str) -> tuple[str, float, bool, bool, dict]:
        """Execute an agent action (message or tool call).
        
        Args:
            action: Either a text message or a JSON tool call string
            
        Returns:
            observation: Result of the action
            reward: Step reward (0.0 during interaction, final reward at end)
            terminated: Whether the episode ended naturally
            truncated: Whether the episode was cut short (max turns)
            info: Additional information
        """
        self.state.turn_count += 1
        
        # Try to parse as tool call
        tool_response = self._try_tool_call(action)
        
        if tool_response is not None:
            # It was a tool call
            observation = tool_response.content
            self.state.messages.append(Message(
                role="assistant",
                content=None,
                tool_calls=[ToolCall(
                    id=tool_response.id,
                    name=self._last_tool_name,
                    arguments=self._last_tool_args,
                )],
                turn_idx=self.state.turn_count,
            ))
            self.state.messages.append(Message(
                role="tool",
                content=tool_response.content,
                tool_call_id=tool_response.id,
                turn_idx=self.state.turn_count,
            ))
        else:
            # It was a regular message
            observation = action
            self.state.messages.append(Message(
                role="assistant",
                content=action,
                turn_idx=self.state.turn_count,
            ))
        
        # Check termination conditions
        terminated = self.state.terminated
        truncated = self.state.turn_count >= self.max_turns
        
        reward = 0.0
        info = {
            "turn_count": self.state.turn_count,
            "tool_response": tool_response.model_dump() if tool_response else None,
        }
        
        return observation, reward, terminated, truncated, info

    def add_user_message(self, content: str) -> None:
        """Add a user message to the conversation."""
        self.state.messages.append(Message(
            role="user",
            content=content,
            turn_idx=self.state.turn_count,
        ))

    def get_observation(self) -> str:
        """Get the current conversation as a formatted string."""
        parts = []
        for msg in self.state.messages:
            if msg.content:
                parts.append(f"{msg.role}: {msg.content}")
            elif msg.tool_calls:
                for tc in msg.tool_calls:
                    parts.append(f"assistant: {tc.name}({json.dumps(tc.arguments)})")
        return "\n".join(parts)

    def get_tool_definitions(self) -> list[dict]:
        """Get tool definitions for the agent."""
        if self.tools is None:
            return []
        return self.tools.get_tool_definitions_dict()

    def execute_tool(self, tool_name: str, **kwargs) -> ToolMessage:
        """Execute a tool and return the response."""
        call_id = f"call_{self.state.turn_count}_{tool_name}"
        
        try:
            result = self.tools.use_tool(tool_name, **kwargs)
            content = self._to_json_str(result)
            return ToolMessage(id=call_id, content=content, error=False)
        except Exception as e:
            return ToolMessage(id=call_id, content=f"Error: {e}", error=True)

    def _try_tool_call(self, action: str) -> Optional[ToolMessage]:
        """Try to parse and execute a tool call from the action string."""
        self._last_tool_name = None
        self._last_tool_args = {}
        
        # Try JSON format: {"name": "tool_name", "arguments": {...}}
        try:
            parsed = json.loads(action)
            if isinstance(parsed, dict) and "name" in parsed:
                tool_name = parsed["name"]
                arguments = parsed.get("arguments", {})
                self._last_tool_name = tool_name
                self._last_tool_args = arguments
                return self.execute_tool(tool_name, **arguments)
        except (json.JSONDecodeError, TypeError):
            pass
        
        # Try functional format: tool_name(arg1='val1', arg2='val2')
        import re
        match = re.match(r'(\w+)\((.*)\)$', action.strip(), re.DOTALL)
        if match and self.tools and self.tools.has_tool(match.group(1)):
            tool_name = match.group(1)
            args_str = match.group(2).strip()
            try:
                # Parse keyword arguments
                if args_str:
                    # Safely evaluate the arguments
                    arguments = {}
                    # Simple parsing for key=value pairs
                    for arg_match in re.finditer(r"(\w+)\s*=\s*('[^']*'|\"[^\"]*\"|[^,]+)", args_str):
                        key = arg_match.group(1)
                        val = arg_match.group(2).strip().strip("'\"")
                        arguments[key] = val
                else:
                    arguments = {}
                self._last_tool_name = tool_name
                self._last_tool_args = arguments
                return self.execute_tool(tool_name, **arguments)
            except Exception:
                pass
        
        return None

    @staticmethod
    def _to_json_str(resp: Any) -> str:
        """Convert a response to a JSON string."""
        if isinstance(resp, str):
            return resp
        if isinstance(resp, BaseModel):
            return json.dumps(resp.model_dump(), default=str, ensure_ascii=False)
        if isinstance(resp, (list, dict)):
            return json.dumps(resp, default=str, ensure_ascii=False)
        return str(resp)

    def get_state_snapshot(self) -> dict:
        """Get a snapshot of the current environment state for logging."""
        return {
            "domain_name": self.domain_name,
            "turn_count": self.state.turn_count,
            "messages": [m.model_dump() for m in self.state.messages],
            "terminated": self.state.terminated,
            "truncated": self.state.truncated,
            "db_hash": self.tools.get_db_hash() if self.tools else None,
            "timestamp": datetime.now().isoformat(),
        }

```

ì´ì œ í•µì‹¬ì¸ **clinical_diagnosis ë„ë©”ì¸**ì„ êµ¬í˜„í•©ë‹ˆë‹¤. í™˜ì DB, ì˜ë£Œ ë„êµ¬, í™˜ê²½, ì •ì±…, taskë¥¼ ëª¨ë‘ ë§Œë“¤ê² ìŠµë‹ˆë‹¤.

```python
"""BIOAgents Medical Domains."""

```

```python
"""Clinical Diagnosis Domain for BIOAgents.

This domain simulates a clinical diagnostic environment where an agent
acts as a physician assistant, using tools to:
- Look up patient information
- Order and review lab tests
- Check vital signs
- Search medical literature
- Make differential diagnoses
- Prescribe treatments
"""

from bioagents.domains.clinical_diagnosis.data_model import ClinicalDB, get_db
from bioagents.domains.clinical_diagnosis.tools import ClinicalTools
from bioagents.domains.clinical_diagnosis.environment import get_environment, get_tasks

__all__ = ["ClinicalDB", "ClinicalTools", "get_db", "get_environment", "get_tasks"]

```

```python
"""Data models for the Clinical Diagnosis domain.

Defines the patient database schema including:
- Patient demographics and medical history
- Lab test results and vital signs
- Medications and allergies
- Clinical notes and diagnoses
"""

from typing import Dict, List, Literal, Optional
from datetime import datetime

from pydantic import BaseModel, Field

from bioagents.environment.db import DB

# --- Sub-models ---

class Allergy(BaseModel):
    allergen: str = Field(description="Name of the allergen")
    reaction: str = Field(description="Type of reaction (e.g., anaphylaxis, rash)")
    severity: Literal["mild", "moderate", "severe"] = Field(description="Severity of the allergic reaction")


class Medication(BaseModel):
    drug_name: str = Field(description="Name of the medication")
    dosage: str = Field(description="Dosage (e.g., '500mg')")
    frequency: str = Field(description="Frequency (e.g., 'twice daily')")
    route: str = Field(default="oral", description="Route of administration")
    start_date: str = Field(description="Start date (YYYY-MM-DD)")
    end_date: Optional[str] = Field(default=None, description="End date if discontinued")
    prescriber: str = Field(default="", description="Prescribing physician")


class VitalSigns(BaseModel):
    timestamp: str = Field(description="Time of measurement")
    blood_pressure_systolic: int = Field(description="Systolic BP (mmHg)")
    blood_pressure_diastolic: int = Field(description="Diastolic BP (mmHg)")
    heart_rate: int = Field(description="Heart rate (bpm)")
    temperature: float = Field(description="Body temperature (Â°C)")
    respiratory_rate: int = Field(description="Respiratory rate (breaths/min)")
    spo2: int = Field(description="Oxygen saturation (%)")
    weight: Optional[float] = Field(default=None, description="Weight (kg)")


class LabResult(BaseModel):
    test_id: str = Field(description="Unique test identifier")
    test_name: str = Field(description="Name of the lab test")
    category: str = Field(description="Category (e.g., 'hematology', 'chemistry', 'microbiology')")
    value: str = Field(description="Test result value")
    unit: str = Field(description="Unit of measurement")
    reference_range: str = Field(description="Normal reference range")
    flag: Optional[Literal["normal", "low", "high", "critical"]] = Field(default="normal")
    order_date: str = Field(description="Date ordered")
    result_date: str = Field(description="Date results available")
    status: Literal["pending", "completed", "cancelled"] = Field(default="completed")


class ClinicalNote(BaseModel):
    note_id: str = Field(description="Unique note identifier")
    note_type: Literal["admission", "progress", "discharge", "consultation", "procedure"] = Field(
        description="Type of clinical note"
    )
    author: str = Field(description="Author of the note")
    date: str = Field(description="Date of the note")
    content: str = Field(description="Note content")
    diagnosis_codes: Optional[List[str]] = Field(default=None, description="ICD-10 codes")


class Condition(BaseModel):
    condition_name: str = Field(description="Name of the condition")
    icd10_code: Optional[str] = Field(default=None, description="ICD-10 code")
    status: Literal["active", "resolved", "chronic"] = Field(description="Current status")
    onset_date: Optional[str] = Field(default=None, description="Date of onset")
    notes: Optional[str] = Field(default=None, description="Additional notes")


class Patient(BaseModel):
    patient_id: str = Field(description="Unique patient identifier")
    name: str = Field(description="Patient full name")
    age: int = Field(description="Patient age")
    sex: Literal["male", "female", "other"] = Field(description="Patient sex")
    date_of_birth: str = Field(description="Date of birth (YYYY-MM-DD)")
    blood_type: Optional[str] = Field(default=None, description="Blood type (e.g., 'A+')")
    allergies: List[Allergy] = Field(default_factory=list, description="Known allergies")
    medications: List[Medication] = Field(default_factory=list, description="Current medications")
    conditions: List[Condition] = Field(default_factory=list, description="Medical conditions")
    vital_signs: List[VitalSigns] = Field(default_factory=list, description="Vital signs history")
    lab_results: List[LabResult] = Field(default_factory=list, description="Lab test results")
    clinical_notes: List[ClinicalNote] = Field(default_factory=list, description="Clinical notes")
    chief_complaint: Optional[str] = Field(default=None, description="Current chief complaint")
    family_history: Optional[str] = Field(default=None, description="Family medical history")
    social_history: Optional[str] = Field(default=None, description="Social history")


# --- Drug Interaction Database ---

class DrugInteraction(BaseModel):
    drug_a: str = Field(description="First drug")
    drug_b: str = Field(description="Second drug")
    severity: Literal["minor", "moderate", "major", "contraindicated"] = Field(
        description="Interaction severity"
    )
    description: str = Field(description="Description of the interaction")
    recommendation: str = Field(description="Clinical recommendation")


# --- Clinical Guidelines ---

class ClinicalGuideline(BaseModel):
    guideline_id: str = Field(description="Unique guideline identifier")
    condition: str = Field(description="Medical condition")
    title: str = Field(description="Guideline title")
    source: str = Field(description="Source organization (e.g., 'AHA', 'WHO')")
    summary: str = Field(description="Brief summary of the guideline")
    recommendations: List[str] = Field(description="Key recommendations")
    last_updated: str = Field(description="Last update date")


# --- Lab Test Order ---

class LabOrder(BaseModel):
    order_id: str = Field(description="Unique order identifier")
    patient_id: str = Field(description="Patient the order is for")
    test_name: str = Field(description="Name of the test ordered")
    ordered_by: str = Field(default="agent", description="Who ordered the test")
    order_date: str = Field(description="Date of order")
    status: Literal["ordered", "in_progress", "completed", "cancelled"] = Field(default="ordered")
    priority: Literal["routine", "urgent", "stat"] = Field(default="routine")


# --- Main Database ---

class ClinicalDB(DB):
    """Clinical Diagnosis domain database.
    
    Contains patient records, drug interactions, clinical guidelines,
    and lab orders for the simulation.
    """
    patients: Dict[str, Patient] = Field(
        default_factory=dict, 
        description="Patient records indexed by patient_id"
    )
    drug_interactions: List[DrugInteraction] = Field(
        default_factory=list,
        description="Known drug interactions"
    )
    clinical_guidelines: Dict[str, ClinicalGuideline] = Field(
        default_factory=dict,
        description="Clinical guidelines indexed by guideline_id"
    )
    lab_orders: Dict[str, LabOrder] = Field(
        default_factory=dict,
        description="Active lab orders indexed by order_id"
    )
    diagnosis_log: List[dict] = Field(
        default_factory=list,
        description="Log of diagnoses made by the agent"
    )


# --- Data path ---

import os

_DOMAIN_DATA_DIR = os.path.join(
    os.path.dirname(os.path.abspath(__file__)),
    "..", "..", "..", "data", "domains", "clinical_diagnosis"
)
DB_PATH = os.path.join(_DOMAIN_DATA_DIR, "db.json")
POLICY_PATH = os.path.join(_DOMAIN_DATA_DIR, "policy.md")
TASKS_PATH = os.path.join(_DOMAIN_DATA_DIR, "tasks.json")


def get_db() -> ClinicalDB:
    """Load the clinical diagnosis database."""
    return ClinicalDB.load(DB_PATH)

```

```python
"""Medical tools for the Clinical Diagnosis domain.

Provides tools for:
- Patient record lookup
- Vital signs retrieval
- Lab test ordering and results
- Drug interaction checking
- Clinical guideline search
- Differential diagnosis generation
- Medical literature search (simulated)
- Specialist referral
"""

from typing import List, Optional

from bioagents.environment.toolkit import ToolKitBase, ToolType, is_tool
from bioagents.domains.clinical_diagnosis.data_model import (
    ClinicalDB,
    Patient,
    VitalSigns,
    LabResult,
    LabOrder,
    Medication,
    ClinicalNote,
    DrugInteraction,
    ClinicalGuideline,
)


class ClinicalTools(ToolKitBase):
    """Tools available to the clinical diagnosis agent."""

    db: ClinicalDB

    def __init__(self, db: ClinicalDB) -> None:
        super().__init__(db)

    # ==========================================
    # Category 1: Patient Information
    # ==========================================

    @is_tool(ToolType.READ)
    def get_patient_info(self, patient_id: str) -> dict:
        """Get basic patient demographics, allergies, conditions, and current medications.

        Args:
            patient_id: The unique patient identifier (e.g., 'P001')

        Returns:
            Patient demographics and medical summary

        Raises:
            ValueError: If the patient is not found
        """
        if patient_id not in self.db.patients:
            raise ValueError(f"Patient '{patient_id}' not found in the system.")
        
        patient = self.db.patients[patient_id]
        return {
            "patient_id": patient.patient_id,
            "name": patient.name,
            "age": patient.age,
            "sex": patient.sex,
            "date_of_birth": patient.date_of_birth,
            "blood_type": patient.blood_type,
            "chief_complaint": patient.chief_complaint,
            "allergies": [a.model_dump() for a in patient.allergies],
            "active_conditions": [
                c.model_dump() for c in patient.conditions if c.status in ("active", "chronic")
            ],
            "current_medications": [
                m.model_dump() for m in patient.medications if m.end_date is None
            ],
            "family_history": patient.family_history,
            "social_history": patient.social_history,
        }

    @is_tool(ToolType.READ)
    def get_patient_history(self, patient_id: str) -> dict:
        """Get the complete medical history of a patient including all conditions, past medications, and family/social history.

        Args:
            patient_id: The unique patient identifier

        Returns:
            Complete medical history

        Raises:
            ValueError: If the patient is not found
        """
        if patient_id not in self.db.patients:
            raise ValueError(f"Patient '{patient_id}' not found.")
        
        patient = self.db.patients[patient_id]
        return {
            "patient_id": patient.patient_id,
            "all_conditions": [c.model_dump() for c in patient.conditions],
            "all_medications": [m.model_dump() for m in patient.medications],
            "family_history": patient.family_history,
            "social_history": patient.social_history,
        }

    # ==========================================
    # Category 2: Vital Signs
    # ==========================================

    @is_tool(ToolType.READ)
    def get_vital_signs(self, patient_id: str) -> dict:
        """Get the most recent vital signs for a patient.

        Args:
            patient_id: The unique patient identifier

        Returns:
            Most recent vital signs (BP, HR, temperature, RR, SpO2)

        Raises:
            ValueError: If the patient is not found or no vitals recorded
        """
        if patient_id not in self.db.patients:
            raise ValueError(f"Patient '{patient_id}' not found.")
        
        patient = self.db.patients[patient_id]
        if not patient.vital_signs:
            raise ValueError(f"No vital signs recorded for patient '{patient_id}'.")
        
        latest = patient.vital_signs[-1]
        return latest.model_dump()

    @is_tool(ToolType.READ)
    def get_vital_signs_trend(self, patient_id: str, num_readings: int = 5) -> list:
        """Get the trend of vital signs over recent readings for a patient.

        Args:
            patient_id: The unique patient identifier
            num_readings: Number of recent readings to retrieve (default 5)

        Returns:
            List of recent vital sign readings, ordered oldest to newest
        """
        if patient_id not in self.db.patients:
            raise ValueError(f"Patient '{patient_id}' not found.")
        
        patient = self.db.patients[patient_id]
        readings = patient.vital_signs[-int(num_readings):]
        return [v.model_dump() for v in readings]

    # ==========================================
    # Category 3: Lab Tests
    # ==========================================

    @is_tool(ToolType.READ)
    def get_lab_results(self, patient_id: str, category: str = "") -> list:
        """Get lab test results for a patient, optionally filtered by category.

        Args:
            patient_id: The unique patient identifier
            category: Optional category filter (e.g., 'hematology', 'chemistry', 'microbiology'). Empty string returns all.

        Returns:
            List of lab test results

        Raises:
            ValueError: If the patient is not found
        """
        if patient_id not in self.db.patients:
            raise ValueError(f"Patient '{patient_id}' not found.")
        
        patient = self.db.patients[patient_id]
        results = patient.lab_results
        
        if category:
            results = [r for r in results if r.category.lower() == category.lower()]
        
        return [r.model_dump() for r in results]

    @is_tool(ToolType.WRITE)
    def order_lab_test(self, patient_id: str, test_name: str, priority: str = "routine") -> dict:
        """Order a new lab test for a patient. The test will be simulated and results returned.

        Args:
            patient_id: The unique patient identifier
            test_name: Name of the test to order (e.g., 'Complete Blood Count', 'Basic Metabolic Panel', 'Urinalysis')
            priority: Priority level ('routine', 'urgent', 'stat')

        Returns:
            Order confirmation with order_id and status

        Raises:
            ValueError: If the patient is not found
        """
        if patient_id not in self.db.patients:
            raise ValueError(f"Patient '{patient_id}' not found.")
        
        if priority not in ("routine", "urgent", "stat"):
            raise ValueError(f"Invalid priority '{priority}'. Must be 'routine', 'urgent', or 'stat'.")
        
        order_id = f"ORD-{len(self.db.lab_orders) + 1:04d}"
        
        from datetime import datetime
        order = LabOrder(
            order_id=order_id,
            patient_id=patient_id,
            test_name=test_name,
            order_date=datetime.now().strftime("%Y-%m-%d"),
            status="ordered",
            priority=priority,
        )
        self.db.lab_orders[order_id] = order
        
        return {
            "order_id": order_id,
            "test_name": test_name,
            "patient_id": patient_id,
            "status": "ordered",
            "priority": priority,
            "message": f"Lab test '{test_name}' ordered for patient {patient_id}. Results will be available soon.",
        }

    # ==========================================
    # Category 4: Medications & Drug Interactions
    # ==========================================

    @is_tool(ToolType.READ)
    def get_medications(self, patient_id: str) -> list:
        """Get the current medication list for a patient.

        Args:
            patient_id: The unique patient identifier

        Returns:
            List of current medications with dosage, frequency, and route
        """
        if patient_id not in self.db.patients:
            raise ValueError(f"Patient '{patient_id}' not found.")
        
        patient = self.db.patients[patient_id]
        current_meds = [m for m in patient.medications if m.end_date is None]
        return [m.model_dump() for m in current_meds]

    @is_tool(ToolType.READ)
    def check_drug_interaction(self, drug_a: str, drug_b: str) -> dict:
        """Check for known interactions between two drugs.

        Args:
            drug_a: Name of the first drug
            drug_b: Name of the second drug

        Returns:
            Interaction information including severity and recommendation
        """
        drug_a_lower = drug_a.lower()
        drug_b_lower = drug_b.lower()
        
        for interaction in self.db.drug_interactions:
            ia = interaction.drug_a.lower()
            ib = interaction.drug_b.lower()
            if (ia == drug_a_lower and ib == drug_b_lower) or \
               (ia == drug_b_lower and ib == drug_a_lower):
                return interaction.model_dump()
        
        return {
            "drug_a": drug_a,
            "drug_b": drug_b,
            "severity": "none",
            "description": "No known interaction found between these medications.",
            "recommendation": "No special precautions needed based on available data.",
        }

    @is_tool(ToolType.WRITE)
    def prescribe_medication(
        self, patient_id: str, drug_name: str, dosage: str, frequency: str, route: str = "oral"
    ) -> dict:
        """Prescribe a new medication for a patient. Checks for allergies and drug interactions.

        Args:
            patient_id: The unique patient identifier
            drug_name: Name of the medication
            dosage: Dosage (e.g., '500mg')
            frequency: Frequency (e.g., 'twice daily', 'every 8 hours')
            route: Route of administration (default 'oral')

        Returns:
            Prescription confirmation or warnings

        Raises:
            ValueError: If the patient is not found or allergy detected
        """
        if patient_id not in self.db.patients:
            raise ValueError(f"Patient '{patient_id}' not found.")
        
        patient = self.db.patients[patient_id]
        
        # Check allergies
        for allergy in patient.allergies:
            if allergy.allergen.lower() in drug_name.lower():
                raise ValueError(
                    f"ALLERGY ALERT: Patient is allergic to '{allergy.allergen}' "
                    f"(reaction: {allergy.reaction}, severity: {allergy.severity}). "
                    f"Cannot prescribe '{drug_name}'."
                )
        
        # Check drug interactions with current medications
        warnings = []
        current_meds = [m for m in patient.medications if m.end_date is None]
        for med in current_meds:
            interaction = self.check_drug_interaction(drug_name, med.drug_name)
            if interaction.get("severity") not in ("none", None):
                warnings.append(interaction)
        
        from datetime import datetime
        new_med = Medication(
            drug_name=drug_name,
            dosage=dosage,
            frequency=frequency,
            route=route,
            start_date=datetime.now().strftime("%Y-%m-%d"),
            prescriber="agent",
        )
        patient.medications.append(new_med)
        
        result = {
            "status": "prescribed",
            "drug_name": drug_name,
            "dosage": dosage,
            "frequency": frequency,
            "route": route,
            "patient_id": patient_id,
        }
        if warnings:
            result["interaction_warnings"] = warnings
        
        return result

    # ==========================================
    # Category 5: Clinical Notes
    # ==========================================

    @is_tool(ToolType.READ)
    def get_clinical_notes(self, patient_id: str, note_type: str = "") -> list:
        """Get clinical notes for a patient, optionally filtered by type.

        Args:
            patient_id: The unique patient identifier
            note_type: Optional filter ('admission', 'progress', 'discharge', 'consultation', 'procedure'). Empty returns all.

        Returns:
            List of clinical notes
        """
        if patient_id not in self.db.patients:
            raise ValueError(f"Patient '{patient_id}' not found.")
        
        patient = self.db.patients[patient_id]
        notes = patient.clinical_notes
        
        if note_type:
            notes = [n for n in notes if n.note_type == note_type]
        
        return [n.model_dump() for n in notes]

    @is_tool(ToolType.WRITE)
    def add_clinical_note(
        self, patient_id: str, note_type: str, content: str, diagnosis_codes: str = ""
    ) -> dict:
        """Add a new clinical note to the patient record.

        Args:
            patient_id: The unique patient identifier
            note_type: Type of note ('admission', 'progress', 'discharge', 'consultation', 'procedure')
            content: The note content
            diagnosis_codes: Comma-separated ICD-10 codes (optional)

        Returns:
            Confirmation of the note being added
        """
        if patient_id not in self.db.patients:
            raise ValueError(f"Patient '{patient_id}' not found.")
        
        from datetime import datetime
        patient = self.db.patients[patient_id]
        note_id = f"NOTE-{patient_id}-{len(patient.clinical_notes) + 1:03d}"
        
        codes = [c.strip() for c in diagnosis_codes.split(",") if c.strip()] if diagnosis_codes else None
        
        note = ClinicalNote(
            note_id=note_id,
            note_type=note_type,
            author="agent",
            date=datetime.now().strftime("%Y-%m-%d"),
            content=content,
            diagnosis_codes=codes,
        )
        patient.clinical_notes.append(note)
        
        return {"note_id": note_id, "status": "added", "note_type": note_type}

    # ==========================================
    # Category 6: Diagnosis & Guidelines
    # ==========================================

    @is_tool(ToolType.READ)
    def get_differential_diagnosis(self, symptoms: str) -> list:
        """Generate a differential diagnosis based on provided symptoms. Uses the internal knowledge base.

        Args:
            symptoms: Comma-separated list of symptoms (e.g., 'fever, cough, dyspnea, chest pain')

        Returns:
            List of possible diagnoses ranked by likelihood
        """
        # This will be enhanced with actual medical knowledge later
        # For now, returns a structured placeholder that tool simulation will fill
        symptom_list = [s.strip().lower() for s in symptoms.split(",")]
        
        # Simple rule-based differential (will be replaced by LLM simulation)
        differentials = []
        
        symptom_condition_map = {
            frozenset(["fever", "cough", "dyspnea"]): [
                {"condition": "Community-acquired pneumonia", "probability": "high", "icd10": "J18.9"},
                {"condition": "Acute bronchitis", "probability": "moderate", "icd10": "J20.9"},
                {"condition": "COVID-19", "probability": "moderate", "icd10": "U07.1"},
            ],
            frozenset(["chest pain", "dyspnea"]): [
                {"condition": "Acute coronary syndrome", "probability": "high", "icd10": "I21.9"},
                {"condition": "Pulmonary embolism", "probability": "moderate", "icd10": "I26.99"},
                {"condition": "Pneumothorax", "probability": "low", "icd10": "J93.9"},
            ],
            frozenset(["headache", "fever", "neck stiffness"]): [
                {"condition": "Bacterial meningitis", "probability": "high", "icd10": "G00.9"},
                {"condition": "Viral meningitis", "probability": "moderate", "icd10": "A87.9"},
                {"condition": "Subarachnoid hemorrhage", "probability": "low", "icd10": "I60.9"},
            ],
            frozenset(["abdominal pain", "nausea", "vomiting"]): [
                {"condition": "Acute appendicitis", "probability": "moderate", "icd10": "K35.80"},
                {"condition": "Acute cholecystitis", "probability": "moderate", "icd10": "K81.0"},
                {"condition": "Gastroenteritis", "probability": "moderate", "icd10": "K52.9"},
            ],
        }
        
        symptom_set = frozenset(symptom_list)
        best_match = None
        best_overlap = 0
        
        for key_symptoms, conditions in symptom_condition_map.items():
            overlap = len(symptom_set & key_symptoms)
            if overlap > best_overlap:
                best_overlap = overlap
                best_match = conditions
        
        if best_match:
            differentials = best_match
        else:
            differentials = [
                {
                    "condition": "Further evaluation needed",
                    "probability": "undetermined",
                    "icd10": "R69",
                    "note": f"Symptoms presented: {', '.join(symptom_list)}. Consider additional workup.",
                }
            ]
        
        return differentials

    @is_tool(ToolType.READ)
    def search_clinical_guidelines(self, condition: str) -> list:
        """Search for clinical guidelines related to a specific condition.

        Args:
            condition: The medical condition to search guidelines for

        Returns:
            List of relevant clinical guidelines
        """
        condition_lower = condition.lower()
        results = []
        
        for gid, guideline in self.db.clinical_guidelines.items():
            if condition_lower in guideline.condition.lower() or \
               condition_lower in guideline.title.lower():
                results.append(guideline.model_dump())
        
        if not results:
            return [{
                "message": f"No specific guidelines found for '{condition}'. Consider searching with broader terms.",
                "suggestion": "Try searching for the general disease category or related conditions.",
            }]
        
        return results

    @is_tool(ToolType.WRITE)
    def record_diagnosis(
        self, patient_id: str, diagnosis: str, icd10_code: str = "", confidence: str = "moderate", reasoning: str = ""
    ) -> dict:
        """Record a diagnosis for a patient.

        Args:
            patient_id: The unique patient identifier
            diagnosis: The diagnosis name
            icd10_code: ICD-10 code for the diagnosis
            confidence: Confidence level ('low', 'moderate', 'high')
            reasoning: Clinical reasoning supporting the diagnosis

        Returns:
            Confirmation of the diagnosis being recorded
        """
        if patient_id not in self.db.patients:
            raise ValueError(f"Patient '{patient_id}' not found.")
        
        from datetime import datetime
        
        patient = self.db.patients[patient_id]
        patient.conditions.append(
            __import__("bioagents.domains.clinical_diagnosis.data_model", fromlist=["Condition"]).Condition(
                condition_name=diagnosis,
                icd10_code=icd10_code or None,
                status="active",
                onset_date=datetime.now().strftime("%Y-%m-%d"),
                notes=reasoning,
            )
        )
        
        log_entry = {
            "patient_id": patient_id,
            "diagnosis": diagnosis,
            "icd10_code": icd10_code,
            "confidence": confidence,
            "reasoning": reasoning,
            "timestamp": datetime.now().isoformat(),
        }
        self.db.diagnosis_log.append(log_entry)
        
        return {
            "status": "recorded",
            "diagnosis": diagnosis,
            "icd10_code": icd10_code,
            "confidence": confidence,
            "patient_id": patient_id,
        }

    # ==========================================
    # Category 7: Medical Literature Search (simulated)
    # ==========================================

    @is_tool(ToolType.READ)
    def search_medical_literature(self, query: str) -> list:
        """Search medical literature (PubMed-style) for relevant articles and evidence.

        Args:
            query: Search query (e.g., 'pneumonia treatment guidelines 2025')

        Returns:
            List of relevant articles with title, abstract snippet, and PMID
        """
        # This will be connected to actual search/simulation later
        # For now provides structured format for tool simulation
        return [
            {
                "title": f"Search results for: {query}",
                "message": "Literature search simulation - will be connected to PubMed/medical knowledge base.",
                "note": "Use search_clinical_guidelines for guideline-specific queries.",
            }
        ]

    # ==========================================
    # Category 8: Communication & Workflow
    # ==========================================

    @is_tool(ToolType.GENERIC)
    def transfer_to_specialist(self, summary: str, specialty: str) -> str:
        """Transfer the case to a specialist with a clinical summary.

        Args:
            summary: Summary of the patient's condition and reason for referral
            specialty: Medical specialty to refer to (e.g., 'cardiology', 'neurology', 'surgery')

        Returns:
            Confirmation of the referral
        """
        return f"Referral to {specialty} submitted successfully. Summary: {summary}"

    @is_tool(ToolType.GENERIC)
    def think(self, thought: str) -> str:
        """Internal reasoning tool. Use this to think through complex clinical decisions before acting.

        Args:
            thought: Your clinical reasoning process

        Returns:
            Empty string (thinking is logged but produces no output)
        """
        return ""

    # ==========================================
    # Assertion helpers (for evaluation, not tools)
    # ==========================================

    def assert_diagnosis_recorded(self, patient_id: str, expected_diagnosis: str) -> bool:
        """Check if a specific diagnosis was recorded for a patient."""
        if patient_id not in self.db.patients:
            return False
        patient = self.db.patients[patient_id]
        return any(
            c.condition_name.lower() == expected_diagnosis.lower()
            for c in patient.conditions
            if c.status == "active"
        )

    def assert_lab_ordered(self, patient_id: str, expected_test: str) -> bool:
        """Check if a specific lab test was ordered for a patient."""
        return any(
            order.patient_id == patient_id and order.test_name.lower() == expected_test.lower()
            for order in self.db.lab_orders.values()
        )

    def assert_medication_prescribed(self, patient_id: str, expected_drug: str) -> bool:
        """Check if a specific medication was prescribed for a patient."""
        if patient_id not in self.db.patients:
            return False
        patient = self.db.patients[patient_id]
        return any(
            m.drug_name.lower() == expected_drug.lower() and m.end_date is None
            for m in patient.medications
        )

```

```python
"""Environment setup for the Clinical Diagnosis domain."""

import json
from pathlib import Path
from typing import Optional

from bioagents.domains.clinical_diagnosis.data_model import (
    ClinicalDB,
    DB_PATH,
    POLICY_PATH,
    TASKS_PATH,
)
from bioagents.domains.clinical_diagnosis.tools import ClinicalTools
from bioagents.environment.environment import Environment


def get_environment(
    db: Optional[ClinicalDB] = None,
    max_turns: int = 20,
) -> Environment:
    """Create a Clinical Diagnosis environment.
    
    Args:
        db: Optional pre-loaded database. If None, loads from default path.
        max_turns: Maximum number of interaction turns.
    
    Returns:
        Configured Environment instance.
    """
    if db is None:
        db = ClinicalDB.load(DB_PATH)
    
    tools = ClinicalTools(db)
    
    with open(POLICY_PATH, "r", encoding="utf-8") as f:
        policy = f.read()
    
    env = Environment(
        domain_name="clinical_diagnosis",
        policy=policy,
        tools=tools,
        max_turns=max_turns,
    )
    
    return env


def get_tasks(task_split: Optional[str] = None) -> list[dict]:
    """Load tasks for the Clinical Diagnosis domain.
    
    Args:
        task_split: Optional split name ('train', 'test', 'base'). 
                    None returns all tasks.
    
    Returns:
        List of task dictionaries.
    """
    with open(TASKS_PATH, "r", encoding="utf-8") as f:
        tasks = json.load(f)
    
    if task_split is None:
        return tasks
    
    # Check for split file
    split_file = Path(TASKS_PATH).parent / "split_tasks.json"
    if split_file.exists():
        with open(split_file, "r", encoding="utf-8") as f:
            splits = json.load(f)
        if task_split not in splits:
            raise ValueError(f"Invalid split '{task_split}'. Available: {list(splits.keys())}")
        valid_ids = set(splits[task_split])
        return [t for t in tasks if t["id"] in valid_ids]
    
    return tasks

```

ì´ì œ policy, ì‹œë®¬ë ˆì´ì…˜ DB, ê·¸ë¦¬ê³  task ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ë§Œë“¤ê² ìŠµë‹ˆë‹¤.

```markdown
# Clinical Diagnosis Agent Policy

You are a clinical diagnosis assistant working in a hospital setting. Your role is to help with patient assessment, diagnosis, and treatment planning using the available medical tools.

## Core Principles

1. **Patient Safety First**: Always check for allergies before prescribing medications. Always check for drug interactions when a patient is on multiple medications.
2. **Evidence-Based Medicine**: Base your diagnostic reasoning on clinical evidence, lab results, vital signs, and established clinical guidelines.
3. **Systematic Approach**: Follow a structured clinical reasoning process:
   - Review patient history and chief complaint
   - Assess vital signs
   - Review relevant lab results (order new ones if needed)
   - Generate differential diagnoses
   - Narrow down with additional information
   - Record your diagnosis with clinical reasoning

## Tool Usage Guidelines

1. **Always start** by reviewing the patient's basic information using `get_patient_info`.
2. **Check vital signs** early in the assessment using `get_vital_signs`.
3. **Review existing lab results** before ordering new tests using `get_lab_results`.
4. **Only order labs** that are clinically indicated and not already available.
5. **Use `think`** to reason through complex clinical decisions before acting.
6. **Check drug interactions** before prescribing any new medication.
7. **Record your diagnosis** with clear clinical reasoning using `record_diagnosis`.
8. **Search clinical guidelines** when managing conditions to ensure best practices.

## Communication Guidelines

1. Communicate findings clearly and concisely.
2. Use medical terminology appropriately but explain when needed.
3. Always state your clinical reasoning when making a diagnosis.
4. If uncertain, express the degree of uncertainty and recommend further evaluation.
5. If a case is beyond your scope, use `transfer_to_specialist` with a thorough summary.

## Restrictions

1. Do NOT diagnose without reviewing relevant patient data.
2. Do NOT prescribe medications that the patient is allergic to.
3. Do NOT ignore critical vital signs or lab values.
4. Do NOT make definitive diagnoses without sufficient evidence â€” express uncertainty when warranted.
5. If the case requires emergency intervention beyond your tools, transfer to the appropriate specialist immediately.

```

```json
{
  "patients": {
    "P001": {
      "patient_id": "P001",
      "name": "James Wilson",
      "age": 58,
      "sex": "male",
      "date_of_birth": "1967-08-14",
      "blood_type": "A+",
      "chief_complaint": "Persistent cough with fever for 5 days, worsening shortness of breath",
      "allergies": [
        {"allergen": "Penicillin", "reaction": "rash", "severity": "moderate"},
        {"allergen": "Sulfonamides", "reaction": "anaphylaxis", "severity": "severe"}
      ],
      "medications": [
        {"drug_name": "Metformin", "dosage": "1000mg", "frequency": "twice daily", "route": "oral", "start_date": "2023-01-15", "end_date": null, "prescriber": "Dr. Chen"},
        {"drug_name": "Lisinopril", "dosage": "10mg", "frequency": "once daily", "route": "oral", "start_date": "2022-06-20", "end_date": null, "prescriber": "Dr. Chen"},
        {"drug_name": "Atorvastatin", "dosage": "20mg", "frequency": "once daily", "route": "oral", "start_date": "2023-03-10", "end_date": null, "prescriber": "Dr. Chen"}
      ],
      "conditions": [
        {"condition_name": "Type 2 Diabetes Mellitus", "icd10_code": "E11.9", "status": "chronic", "onset_date": "2020-03-15", "notes": "HbA1c 7.2% at last check"},
        {"condition_name": "Hypertension", "icd10_code": "I10", "status": "chronic", "onset_date": "2019-06-20", "notes": "Well controlled on Lisinopril"},
        {"condition_name": "Hyperlipidemia", "icd10_code": "E78.5", "status": "chronic", "onset_date": "2023-01-10", "notes": null}
      ],
      "vital_signs": [
        {"timestamp": "2026-02-12T08:00:00", "blood_pressure_systolic": 148, "blood_pressure_diastolic": 92, "heart_rate": 98, "temperature": 38.7, "respiratory_rate": 22, "spo2": 93, "weight": 88.5},
        {"timestamp": "2026-02-12T14:00:00", "blood_pressure_systolic": 142, "blood_pressure_diastolic": 88, "heart_rate": 102, "temperature": 39.1, "respiratory_rate": 24, "spo2": 91, "weight": null}
      ],
      "lab_results": [
        {"test_id": "LAB-P001-001", "test_name": "White Blood Cell Count", "category": "hematology", "value": "15.2", "unit": "x10^9/L", "reference_range": "4.5-11.0", "flag": "high", "order_date": "2026-02-12", "result_date": "2026-02-12", "status": "completed"},
        {"test_id": "LAB-P001-002", "test_name": "C-Reactive Protein", "category": "chemistry", "value": "85", "unit": "mg/L", "reference_range": "0-5", "flag": "critical", "order_date": "2026-02-12", "result_date": "2026-02-12", "status": "completed"},
        {"test_id": "LAB-P001-003", "test_name": "Procalcitonin", "category": "chemistry", "value": "2.1", "unit": "ng/mL", "reference_range": "0-0.5", "flag": "high", "order_date": "2026-02-12", "result_date": "2026-02-12", "status": "completed"},
        {"test_id": "LAB-P001-004", "test_name": "Blood Glucose (Fasting)", "category": "chemistry", "value": "165", "unit": "mg/dL", "reference_range": "70-100", "flag": "high", "order_date": "2026-02-12", "result_date": "2026-02-12", "status": "completed"},
        {"test_id": "LAB-P001-005", "test_name": "HbA1c", "category": "chemistry", "value": "7.8", "unit": "%", "reference_range": "4.0-5.6", "flag": "high", "order_date": "2026-02-12", "result_date": "2026-02-12", "status": "completed"},
        {"test_id": "LAB-P001-006", "test_name": "Blood Culture", "category": "microbiology", "value": "Pending", "unit": "", "reference_range": "No growth", "flag": null, "order_date": "2026-02-12", "result_date": "2026-02-12", "status": "pending"}
      ],
      "clinical_notes": [
        {"note_id": "NOTE-P001-001", "note_type": "admission", "author": "Dr. Park", "date": "2026-02-12", "content": "58M presents with 5-day history of productive cough with yellow-green sputum, fever (self-measured 38.5C at home), and progressive dyspnea. No hemoptysis. Reports decreased appetite and malaise. PMH significant for T2DM, HTN, hyperlipidemia. Chest auscultation reveals crackles in right lower lobe. Decreased breath sounds on right base. CXR pending.", "diagnosis_codes": null}
      ],
      "family_history": "Father: MI at age 62. Mother: T2DM, alive. Brother: hypertension.",
      "social_history": "Former smoker (20 pack-years, quit 5 years ago). Occasional alcohol use. Retired office worker. Lives with wife."
    },
    "P002": {
      "patient_id": "P002",
      "name": "Sarah Kim",
      "age": 32,
      "sex": "female",
      "date_of_birth": "1993-11-22",
      "blood_type": "O-",
      "chief_complaint": "Severe headache with neck stiffness and photophobia for 2 days",
      "allergies": [],
      "medications": [
        {"drug_name": "Oral Contraceptive", "dosage": "standard", "frequency": "once daily", "route": "oral", "start_date": "2024-01-10", "end_date": null, "prescriber": "Dr. Lee"}
      ],
      "conditions": [
        {"condition_name": "Migraine without aura", "icd10_code": "G43.009", "status": "chronic", "onset_date": "2018-05-01", "notes": "Usually responds to triptans"}
      ],
      "vital_signs": [
        {"timestamp": "2026-02-12T10:30:00", "blood_pressure_systolic": 132, "blood_pressure_diastolic": 84, "heart_rate": 110, "temperature": 39.4, "respiratory_rate": 20, "spo2": 98, "weight": 58.0}
      ],
      "lab_results": [
        {"test_id": "LAB-P002-001", "test_name": "White Blood Cell Count", "category": "hematology", "value": "18.5", "unit": "x10^9/L", "reference_range": "4.5-11.0", "flag": "high", "order_date": "2026-02-12", "result_date": "2026-02-12", "status": "completed"},
        {"test_id": "LAB-P002-002", "test_name": "C-Reactive Protein", "category": "chemistry", "value": "120", "unit": "mg/L", "reference_range": "0-5", "flag": "critical", "order_date": "2026-02-12", "result_date": "2026-02-12", "status": "completed"}
      ],
      "clinical_notes": [
        {"note_id": "NOTE-P002-001", "note_type": "admission", "author": "Dr. Martinez", "date": "2026-02-12", "content": "32F presents with severe headache (8/10) that started 2 days ago with progressive neck stiffness and photophobia. Reports fever at home. Denies recent travel or sick contacts. No history of immunosuppression. Kernig and Brudzinski signs positive on exam. This presentation is different from her usual migraines - more acute onset, higher fever, and nuchal rigidity.", "diagnosis_codes": null}
      ],
      "family_history": "Unremarkable.",
      "social_history": "Non-smoker. Social drinker. Software engineer. No recent travel."
    },
    "P003": {
      "patient_id": "P003",
      "name": "Robert Chen",
      "age": 45,
      "sex": "male",
      "date_of_birth": "1980-04-03",
      "blood_type": "B+",
      "chief_complaint": "Acute right lower quadrant abdominal pain with nausea and vomiting",
      "allergies": [
        {"allergen": "Ibuprofen", "reaction": "GI bleeding", "severity": "severe"}
      ],
      "medications": [],
      "conditions": [],
      "vital_signs": [
        {"timestamp": "2026-02-12T16:00:00", "blood_pressure_systolic": 138, "blood_pressure_diastolic": 82, "heart_rate": 96, "temperature": 38.2, "respiratory_rate": 18, "spo2": 99, "weight": 75.0}
      ],
      "lab_results": [
        {"test_id": "LAB-P003-001", "test_name": "White Blood Cell Count", "category": "hematology", "value": "14.8", "unit": "x10^9/L", "reference_range": "4.5-11.0", "flag": "high", "order_date": "2026-02-12", "result_date": "2026-02-12", "status": "completed"},
        {"test_id": "LAB-P003-002", "test_name": "Neutrophils", "category": "hematology", "value": "82", "unit": "%", "reference_range": "40-70", "flag": "high", "order_date": "2026-02-12", "result_date": "2026-02-12", "status": "completed"}
      ],
      "clinical_notes": [
        {"note_id": "NOTE-P003-001", "note_type": "admission", "author": "Dr. Williams", "date": "2026-02-12", "content": "45M presents with acute onset RLQ abdominal pain started 12 hours ago, initially periumbilical then migrated to RLQ. Associated nausea and 2 episodes of vomiting. Low-grade fever. McBurney's point tenderness positive. Rebound tenderness and guarding present. Psoas sign positive. Rovsing sign positive.", "diagnosis_codes": null}
      ],
      "family_history": "Father: colon cancer at age 65. Mother: alive and well.",
      "social_history": "Non-smoker. No alcohol. Restaurant manager."
    }
  },
  "drug_interactions": [
    {"drug_a": "Metformin", "drug_b": "Levofloxacin", "severity": "moderate", "description": "Fluoroquinolones may alter blood glucose levels when used with metformin. May cause hypoglycemia or hyperglycemia.", "recommendation": "Monitor blood glucose closely. Adjust metformin dosage if necessary."},
    {"drug_a": "Lisinopril", "drug_b": "Potassium supplements", "severity": "major", "description": "ACE inhibitors increase potassium retention. Concurrent potassium supplementation may lead to hyperkalemia.", "recommendation": "Avoid concurrent use or monitor potassium levels very closely."},
    {"drug_a": "Warfarin", "drug_b": "Azithromycin", "severity": "moderate", "description": "Macrolide antibiotics may increase the anticoagulant effect of warfarin, increasing bleeding risk.", "recommendation": "Monitor INR closely. May need warfarin dose adjustment."},
    {"drug_a": "Atorvastatin", "drug_b": "Clarithromycin", "severity": "major", "description": "Clarithromycin inhibits CYP3A4, significantly increasing statin levels and risk of rhabdomyolysis.", "recommendation": "Avoid combination. Use azithromycin as alternative macrolide."},
    {"drug_a": "Oral Contraceptive", "drug_b": "Rifampin", "severity": "major", "description": "Rifampin is a potent CYP3A4 inducer that significantly reduces oral contraceptive efficacy.", "recommendation": "Use alternative contraception method during and 28 days after rifampin therapy."},
    {"drug_a": "Metformin", "drug_b": "IV Contrast", "severity": "major", "description": "Risk of lactic acidosis. Metformin should be held before and 48h after iodinated contrast.", "recommendation": "Hold metformin 48 hours before and after contrast administration. Check renal function."}
  ],
  "clinical_guidelines": {
    "GL001": {
      "guideline_id": "GL001",
      "condition": "Community-acquired pneumonia",
      "title": "ATS/IDSA Guidelines for Management of Community-Acquired Pneumonia",
      "source": "American Thoracic Society / Infectious Diseases Society of America",
      "summary": "Evidence-based recommendations for the diagnosis and treatment of community-acquired pneumonia in adults.",
      "recommendations": [
        "For outpatient treatment without comorbidities: amoxicillin or doxycycline",
        "For outpatient with comorbidities: respiratory fluoroquinolone OR beta-lactam + macrolide",
        "For inpatient non-ICU: respiratory fluoroquinolone OR beta-lactam + macrolide",
        "For inpatient ICU: beta-lactam + macrolide OR beta-lactam + respiratory fluoroquinolone",
        "Obtain blood cultures and sputum culture before initiating antibiotics in hospitalized patients",
        "Start antibiotics within 4 hours of admission",
        "Consider local antibiogram for empiric therapy"
      ],
      "last_updated": "2024-01-15"
    },
    "GL002": {
      "guideline_id": "GL002",
      "condition": "Bacterial meningitis",
      "title": "IDSA Guidelines for Bacterial Meningitis Management",
      "source": "Infectious Diseases Society of America",
      "summary": "Clinical practice guidelines for the management of bacterial meningitis.",
      "recommendations": [
        "Perform lumbar puncture urgently if meningitis is suspected (unless contraindicated)",
        "Start empiric antibiotics immediately - do not delay for imaging",
        "Empiric therapy for adults: vancomycin + third-generation cephalosporin (ceftriaxone or cefotaxime)",
        "Add dexamethasone 0.15 mg/kg IV q6h for 4 days, started before or with first antibiotic dose",
        "Blood cultures before antibiotics",
        "CT scan before LP only if: immunocompromised, history of CNS disease, new-onset seizure, papilledema, altered consciousness, focal neurologic deficit",
        "Adjust antibiotics based on CSF culture and sensitivity results"
      ],
      "last_updated": "2024-06-01"
    },
    "GL003": {
      "guideline_id": "GL003",
      "condition": "Acute appendicitis",
      "title": "SAGES Guidelines for Diagnosis and Treatment of Acute Appendicitis",
      "source": "Society of American Gastrointestinal and Endoscopic Surgeons",
      "summary": "Guidelines for the clinical diagnosis and surgical management of acute appendicitis.",
      "recommendations": [
        "CT abdomen/pelvis is the imaging modality of choice (sensitivity >95%)",
        "Ultrasound as first-line imaging in pediatric patients and pregnant women",
        "Alvarado score â‰¥7 strongly suggests appendicitis",
        "Laparoscopic appendectomy is the standard of care for uncomplicated appendicitis",
        "Antibiotic-first approach may be considered for uncomplicated appendicitis in select patients",
        "Preoperative antibiotics: single dose of broad-spectrum antibiotic",
        "NPO status and IV fluid resuscitation before surgery",
        "Pain management: avoid NSAIDs if GI bleeding risk; use acetaminophen or opioids"
      ],
      "last_updated": "2025-03-20"
    }
  },
  "lab_orders": {},
  "diagnosis_log": []
}

```

```jsonc
[
  {
    "id": "dx_pneumonia_001",
    "description": {
      "purpose": "Diagnose and manage community-acquired pneumonia in a diabetic patient with drug allergies",
      "difficulty": "moderate",
      "key_challenges": ["penicillin allergy constrains antibiotic choice", "diabetic comorbidity affects management", "must identify critical lab values"]
    },
    "user_scenario": {
      "persona": "An attending physician seeking a clinical workup and diagnosis for a newly admitted patient",
      "instructions": "You are an attending physician who has a new patient, James Wilson (P001), admitted with respiratory symptoms. Ask the agent to perform a complete clinical assessment and provide a diagnosis with treatment plan. You should ask follow-up questions about the antibiotic choice given his allergies."
    },
    "ticket": "Patient P001 (James Wilson, 58M) admitted with 5-day productive cough, fever, and worsening dyspnea. Needs complete clinical assessment, diagnosis, and treatment plan. Note: penicillin and sulfonamide allergies.",
    "evaluation_criteria": {
      "actions": [
        {
          "action_id": "review_patient",
          "name": "get_patient_info",
          "arguments": {"patient_id": "P001"},
          "info": "Review patient demographics, allergies, and current medications"
        },
        {
          "action_id": "check_vitals",
          "name": "get_vital_signs",
          "arguments": {"patient_id": "P001"},
          "info": "Check current vital signs - should note fever, tachycardia, low SpO2"
        },
        {
          "action_id": "review_labs",
          "name": "get_lab_results",
          "arguments": {"patient_id": "P001"},
          "info": "Review lab results - should identify elevated WBC, CRP, procalcitonin"
        },
        {
          "action_id": "differential",
          "name": "get_differential_diagnosis",
          "arguments": {"symptoms": "fever, cough, dyspnea"},
          "info": "Generate differential diagnosis"
        },
        {
          "action_id": "guidelines",
          "name": "search_clinical_guidelines",
          "arguments": {"condition": "Community-acquired pneumonia"},
          "info": "Search for CAP treatment guidelines"
        },
        {
          "action_id": "record_dx",
          "name": "record_diagnosis",
          "arguments": {
            "patient_id": "P001",
            "diagnosis": "Community-acquired pneumonia",
            "icd10_code": "J18.9",
            "confidence": "high"
          },
          "compare_args": ["patient_id", "diagnosis"],
          "info": "Record the diagnosis"
        }
      ],
      "nl_assertions": [
        "The agent identified the patient's penicillin and sulfonamide allergies",
        "The agent noted critical findings: elevated WBC, CRP, procalcitonin, low SpO2, fever",
        "The agent diagnosed community-acquired pneumonia",
        "The agent recommended an antibiotic that avoids penicillin and sulfonamide classes",
        "The agent considered the patient's diabetes in the management plan"
      ],
      "reward_basis": ["ACTION", "NL_ASSERTION"]
    }
  },
  {
    "id": "dx_meningitis_001",
    "description": {
      "purpose": "Diagnose suspected bacterial meningitis requiring urgent workup",
      "difficulty": "hard",
      "key_challenges": ["time-sensitive condition", "must order LP", "empiric antibiotics needed immediately"]
    },
    "user_scenario": {
      "persona": "An emergency medicine physician consulting for an urgent case",
      "instructions": "You have a 32-year-old woman (P002) presenting with severe headache, neck stiffness, and photophobia. You need the agent to urgently assess this patient and provide management recommendations. Emphasize the urgency."
    },
    "ticket": "URGENT: Patient P002 (Sarah Kim, 32F) with severe headache, neck stiffness, photophobia, and fever. Positive Kernig and Brudzinski signs. Needs immediate assessment and management plan.",
    "evaluation_criteria": {
      "actions": [
        {
          "action_id": "review_patient",
          "name": "get_patient_info",
          "arguments": {"patient_id": "P002"},
          "info": "Review patient information"
        },
        {
          "action_id": "check_vitals",
          "name": "get_vital_signs",
          "arguments": {"patient_id": "P002"},
          "info": "Check vital signs - note fever and tachycardia"
        },
        {
          "action_id": "review_labs",
          "name": "get_lab_results",
          "arguments": {"patient_id": "P002"},
          "info": "Review available labs"
        },
        {
          "action_id": "order_lp",
          "name": "order_lab_test",
          "arguments": {"patient_id": "P002", "test_name": "Lumbar Puncture / CSF Analysis", "priority": "stat"},
          "compare_args": ["patient_id", "priority"],
          "info": "Order lumbar puncture - critical for meningitis diagnosis"
        },
        {
          "action_id": "order_blood_culture",
          "name": "order_lab_test",
          "arguments": {"patient_id": "P002", "test_name": "Blood Culture", "priority": "stat"},
          "compare_args": ["patient_id"],
          "info": "Order blood cultures before antibiotics"
        },
        {
          "action_id": "guidelines",
          "name": "search_clinical_guidelines",
          "arguments": {"condition": "Bacterial meningitis"},
          "info": "Search meningitis guidelines"
        },
        {
          "action_id": "record_dx",
          "name": "record_diagnosis",
          "arguments": {
            "patient_id": "P002",
            "diagnosis": "Suspected bacterial meningitis",
            "confidence": "high"
          },
          "compare_args": ["patient_id"],
          "info": "Record suspected diagnosis"
        }
      ],
      "nl_assertions": [
        "The agent recognized the urgency of the situation",
        "The agent ordered a lumbar puncture as a priority/stat test",
        "The agent recommended empiric antibiotics (vancomycin + ceftriaxone) before LP results",
        "The agent recommended adjunctive dexamethasone",
        "The agent considered differentiating from the patient's known migraine history"
      ],
      "reward_basis": ["ACTION", "NL_ASSERTION"]
    }
  },
  {
    "id": "dx_appendicitis_001",
    "description": {
      "purpose": "Diagnose acute appendicitis and manage appropriately, considering drug allergy",
      "difficulty": "moderate",
      "key_challenges": ["classic presentation", "patient has NSAID allergy", "surgical referral needed"]
    },
    "user_scenario": {
      "persona": "A surgical resident asking for pre-operative assessment",
      "instructions": "You are a surgical resident with patient Robert Chen (P003) who has acute abdominal pain. You need the agent to complete the assessment, confirm the diagnosis, and prepare the patient for potential surgery. Ask about pain management given his ibuprofen allergy."
    },
    "ticket": "Patient P003 (Robert Chen, 45M) with acute RLQ pain, nausea, vomiting. McBurney's point tenderness. Needs assessment, diagnosis confirmation, and pre-op preparation. Note: ibuprofen allergy (GI bleeding).",
    "evaluation_criteria": {
      "actions": [
        {
          "action_id": "review_patient",
          "name": "get_patient_info",
          "arguments": {"patient_id": "P003"},
          "info": "Review patient information and allergy"
        },
        {
          "action_id": "check_vitals",
          "name": "get_vital_signs",
          "arguments": {"patient_id": "P003"},
          "info": "Check vital signs"
        },
        {
          "action_id": "review_labs",
          "name": "get_lab_results",
          "arguments": {"patient_id": "P003"},
          "info": "Review labs - elevated WBC with neutrophilia"
        },
        {
          "action_id": "differential",
          "name": "get_differential_diagnosis",
          "arguments": {"symptoms": "abdominal pain, nausea, vomiting"},
          "info": "Generate differential diagnosis for abdominal pain"
        },
        {
          "action_id": "guidelines",
          "name": "search_clinical_guidelines",
          "arguments": {"condition": "Acute appendicitis"},
          "info": "Search appendicitis guidelines"
        },
        {
          "action_id": "record_dx",
          "name": "record_diagnosis",
          "arguments": {
            "patient_id": "P003",
            "diagnosis": "Acute appendicitis",
            "icd10_code": "K35.80",
            "confidence": "high"
          },
          "compare_args": ["patient_id", "diagnosis"],
          "info": "Record diagnosis"
        },
        {
          "action_id": "refer_surgery",
          "name": "transfer_to_specialist",
          "arguments": {
            "summary": "45M with classic acute appendicitis presentation requiring laparoscopic appendectomy",
            "specialty": "surgery"
          },
          "compare_args": ["specialty"],
          "info": "Refer to surgery for appendectomy"
        }
      ],
      "nl_assertions": [
        "The agent identified classic signs of acute appendicitis (McBurney's point, rebound, psoas sign)",
        "The agent recommended CT abdomen for confirmation",
        "The agent noted the ibuprofen allergy and avoided NSAIDs for pain management",
        "The agent recommended acetaminophen for pain control",
        "The agent referred the patient to surgery for appendectomy"
      ],
      "reward_basis": ["ACTION", "NL_ASSERTION"]
    }
  },
  {
    "id": "dx_drug_interaction_001",
    "description": {
      "purpose": "Identify and manage a potential drug interaction when prescribing antibiotics",
      "difficulty": "moderate",
      "key_challenges": ["must check interaction between new antibiotic and existing statin", "must suggest safe alternative"]
    },
    "user_scenario": {
      "persona": "A pharmacist flagging a potential interaction",
      "instructions": "You are a pharmacist who noticed the attending considered prescribing clarithromycin for patient P001's pneumonia. Ask the agent to check the interaction with the patient's current atorvastatin and recommend an alternative."
    },
    "ticket": "Pharmacist query: Please check drug interaction between clarithromycin and atorvastatin for patient P001 and recommend a safe antibiotic alternative.",
    "evaluation_criteria": {
      "actions": [
        {
          "action_id": "get_meds",
          "name": "get_medications",
          "arguments": {"patient_id": "P001"},
          "info": "Check current medication list"
        },
        {
          "action_id": "check_interaction",
          "name": "check_drug_interaction",
          "arguments": {"drug_a": "Atorvastatin", "drug_b": "Clarithromycin"},
          "info": "Check interaction - should flag as major (CYP3A4 inhibition, rhabdomyolysis risk)"
        },
        {
          "action_id": "check_allergies",
          "name": "get_patient_info",
          "arguments": {"patient_id": "P001"},
          "info": "Verify allergies before suggesting alternative"
        }
      ],
      "nl_assertions": [
        "The agent identified the major interaction between clarithromycin and atorvastatin",
        "The agent mentioned CYP3A4 inhibition and rhabdomyolysis risk",
        "The agent recommended azithromycin as a safer macrolide alternative",
        "The agent confirmed azithromycin doesn't conflict with patient's penicillin allergy",
        "The agent recommended monitoring during antibiotic therapy"
      ],
      "reward_basis": ["ACTION", "NL_ASSERTION"]
    }
  },
  {
    "id": "dx_critical_vitals_001",
    "description": {
      "purpose": "Respond appropriately to deteriorating vital signs",
      "difficulty": "hard",
      "key_challenges": ["recognize critical SpO2 drop", "identify possible sepsis", "take urgent action"]
    },
    "user_scenario": {
      "persona": "A nurse alerting about deteriorating patient",
      "instructions": "You are a nurse who noticed that patient P001's oxygen saturation has been trending down (93% â†’ 91%). The patient also has a rising heart rate and temperature. Alert the agent and ask what to do."
    },
    "ticket": "ALERT: Patient P001 vital signs deteriorating. SpO2 dropped from 93% to 91%. Heart rate rising. Temperature 39.1Â°C. Please assess and advise.",
    "evaluation_criteria": {
      "actions": [
        {
          "action_id": "check_vitals_trend",
          "name": "get_vital_signs_trend",
          "arguments": {"patient_id": "P001"},
          "info": "Review vital signs trend"
        },
        {
          "action_id": "review_labs",
          "name": "get_lab_results",
          "arguments": {"patient_id": "P001"},
          "info": "Review latest labs for signs of worsening infection/sepsis"
        },
        {
          "action_id": "review_notes",
          "name": "get_clinical_notes",
          "arguments": {"patient_id": "P001"},
          "info": "Review admission notes for context"
        },
        {
          "action_id": "add_progress_note",
          "name": "add_clinical_note",
          "arguments": {
            "patient_id": "P001",
            "note_type": "progress",
            "content": "Clinical deterioration noted with declining SpO2 and rising temperature"
          },
          "compare_args": ["patient_id", "note_type"],
          "info": "Document the clinical change"
        }
      ],
      "nl_assertions": [
        "The agent recognized the critical nature of declining oxygen saturation",
        "The agent recommended supplemental oxygen",
        "The agent considered possible sepsis or pneumonia progression",
        "The agent recommended repeat blood cultures and/or chest imaging",
        "The agent recommended escalation of care if condition continues to deteriorate"
      ],
      "reward_basis": ["ACTION", "NL_ASSERTION"]
    }
  }
]

```

ì´ì œ Gym ì¸í„°í˜ì´ìŠ¤ë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤.

```python
"""Gymnasium-compatible environment interface for BIOAgents."""

from bioagents.gym.agent_env import BioAgentGymEnv, BIOAGENT_ENV_ID, register_bioagent_gym

__all__ = ["BioAgentGymEnv", "BIOAGENT_ENV_ID", "register_bioagent_gym"]

```

```python
"""Gymnasium-compatible environment for BIOAgents.

Provides a standard Gymnasium interface for training RL agents
in medical/biomedical tool-use tasks.
"""

import json
from copy import deepcopy
from typing import Any, Optional

import gymnasium as gym
from gymnasium import spaces

BIOAGENT_ENV_ID = "BioAgent-v0"

# Domain registry
_DOMAIN_REGISTRY = {}


def _register_domain(domain_name: str, get_environment_fn, get_tasks_fn):
    """Register a domain for use in the gym."""
    _DOMAIN_REGISTRY[domain_name] = {
        "get_environment": get_environment_fn,
        "get_tasks": get_tasks_fn,
    }


def _load_default_domains():
    """Load all default domains."""
    try:
        from bioagents.domains.clinical_diagnosis.environment import get_environment, get_tasks
        _register_domain("clinical_diagnosis", get_environment, get_tasks)
    except Exception:
        pass


class BioAgentGymEnv(gym.Env):
    """Gymnasium-compatible environment for biomedical agent training.
    
    Observation space: Text (conversation history + tool results)
    Action space: Text (agent messages or tool calls in JSON)
    
    Usage:
        register_bioagent_gym()
        env = gym.make("BioAgent-v0", domain="clinical_diagnosis", task_id="dx_pneumonia_001")
        obs, info = env.reset()
        obs, reward, terminated, truncated, info = env.step(action)
    """
    
    metadata = {"render_modes": ["human", "ansi"]}

    def __init__(
        self,
        domain: str = "clinical_diagnosis",
        task_id: Optional[str] = None,
        task_split: Optional[str] = None,
        max_turns: int = 20,
        render_mode: Optional[str] = None,
        **kwargs,
    ):
        super().__init__()
        
        self.domain_name = domain
        self.task_id = task_id
        self.task_split = task_split
        self.max_turns = max_turns
        self.render_mode = render_mode
        
        # Load domain
        _load_default_domains()
        if domain not in _DOMAIN_REGISTRY:
            raise ValueError(
                f"Domain '{domain}' not registered. Available: {list(_DOMAIN_REGISTRY.keys())}"
            )
        
        domain_info = _DOMAIN_REGISTRY[domain]
        self._get_environment_fn = domain_info["get_environment"]
        self._get_tasks_fn = domain_info["get_tasks"]
        
        # Load tasks
        self._tasks = self._get_tasks_fn(task_split)
        self._task_map = {t["id"]: t for t in self._tasks}
        
        # Gymnasium spaces (text-based)
        self.observation_space = spaces.Text(
            min_length=0, max_length=100000, charset=None
        )
        self.action_space = spaces.Text(
            min_length=1, max_length=10000, charset=None
        )
        
        # State
        self._env = None
        self._current_task = None
        self._turn_count = 0
        self._conversation_history = []
        self._tool_call_log = []
    
    def reset(
        self, *, seed: Optional[int] = None, options: Optional[dict] = None
    ) -> tuple[str, dict]:
        """Reset the environment with a task.
        
        Args:
            seed: Random seed
            options: Optional dict with 'task_id' to select a specific task
            
        Returns:
            observation: Initial observation (system prompt + patient ticket)
            info: Dict with task info, tools, policy
        """
        super().reset(seed=seed)
        
        # Select task
        task_id = self.task_id
        if options and "task_id" in options:
            task_id = options["task_id"]
        
        if task_id:
            if task_id not in self._task_map:
                raise ValueError(f"Task '{task_id}' not found. Available: {list(self._task_map.keys())}")
            self._current_task = self._task_map[task_id]
        else:
            # Random task selection
            idx = self.np_random.integers(0, len(self._tasks))
            self._current_task = self._tasks[idx]
        
        # Create fresh environment
        self._env = self._get_environment_fn(max_turns=self.max_turns)
        self._turn_count = 0
        self._conversation_history = []
        self._tool_call_log = []
        
        # Build initial observation
        initial_obs = self._build_initial_observation()
        
        info = {
            "task_id": self._current_task["id"],
            "task_description": self._current_task.get("description", {}),
            "domain": self.domain_name,
            "policy": self._env.policy,
            "tools": self._env.get_tool_definitions(),
            "max_turns": self.max_turns,
        }
        
        return initial_obs, info
    
    def step(self, action: str) -> tuple[str, float, bool, bool, dict]:
        """Execute an agent action.
        
        The action can be:
        1. A JSON tool call: {"name": "tool_name", "arguments": {...}}
        2. A text message to the user
        
        Returns:
            observation: Result of the action
            reward: Computed reward (0.0 during interaction, final at end)
            terminated: Whether the episode ended
            truncated: Whether max turns exceeded
            info: Additional information
        """
        self._turn_count += 1
        
        observation, reward, terminated, truncated, info = self._env.step(action)
        
        # Track tool calls
        if info.get("tool_response"):
            self._tool_call_log.append({
                "turn": self._turn_count,
                "tool_name": self._env._last_tool_name,
                "arguments": self._env._last_tool_args,
                "response": info["tool_response"],
            })
        
        # Record in conversation history
        self._conversation_history.append({
            "turn": self._turn_count,
            "agent_action": action,
            "observation": observation,
        })
        
        # Check truncation
        if self._turn_count >= self.max_turns:
            truncated = True
        
        # Compute reward at episode end
        if terminated or truncated:
            reward = self._compute_reward()
        
        info.update({
            "turn_count": self._turn_count,
            "tool_calls": self._tool_call_log,
            "task_id": self._current_task["id"] if self._current_task else None,
        })
        
        if self.render_mode == "human":
            self.render()
        
        return observation, reward, terminated, truncated, info
    
    def _build_initial_observation(self) -> str:
        """Build the initial observation from the task."""
        task = self._current_task
        parts = [
            f"=== BIOAgents Clinical Task: {task['id']} ===",
            "",
            f"Domain: {self.domain_name}",
            "",
            "--- Patient Ticket ---",
            task.get("ticket", "No ticket provided."),
            "",
            "--- Available Tools ---",
        ]
        
        tool_defs = self._env.get_tool_definitions()
        for i, td in enumerate(tool_defs, 1):
            func = td.get("function", {})
            parts.append(f"{i}. {func.get('name', '?')}: {func.get('description', '')[:100]}")
        
        parts.extend([
            "",
            "--- Instructions ---",
            "Use the available tools to assess the patient and provide your clinical recommendation.",
            "To call a tool, respond with JSON: {\"name\": \"tool_name\", \"arguments\": {...}}",
            "When done, provide your final assessment as a text message.",
        ])
        
        return "\n".join(parts)
    
    def _compute_reward(self) -> float:
        """Compute the reward for the completed episode.
        
        Evaluates:
        1. ACTION score: Did the agent call the expected tools?
        2. NL_ASSERTION score: Did the agent's reasoning meet clinical criteria?
        """
        if self._current_task is None:
            return 0.0
        
        eval_criteria = self._current_task.get("evaluation_criteria", {})
        expected_actions = eval_criteria.get("actions", [])
        reward_basis = eval_criteria.get("reward_basis", ["ACTION"])
        
        total_score = 0.0
        num_components = 0
        
        # --- ACTION score ---
        if "ACTION" in reward_basis and expected_actions:
            action_score = self._score_actions(expected_actions)
            total_score += action_score
            num_components += 1
        
        # --- NL_ASSERTION score (placeholder for now) ---
        if "NL_ASSERTION" in reward_basis:
            # This will be computed by LLM judge later
            # For now, give partial credit based on action completion
            assertion_score = total_score / max(num_components, 1)  # mirror action score
            total_score += assertion_score
            num_components += 1
        
        return total_score / max(num_components, 1)
    
    def _score_actions(self, expected_actions: list[dict]) -> float:
        """Score the agent's tool usage against expected actions."""
        if not expected_actions:
            return 1.0
        
        actual_tool_names = [tc["tool_name"] for tc in self._tool_call_log]
        
        matched = 0
        for exp in expected_actions:
            exp_name = exp.get("name", "")
            compare_args = exp.get("compare_args", [])
            exp_args = exp.get("arguments", {})
            
            for tc in self._tool_call_log:
                if tc["tool_name"] == exp_name:
                    # Check argument matching if specified
                    if compare_args:
                        all_match = all(
                            str(tc["arguments"].get(k, "")).lower() == str(exp_args.get(k, "")).lower()
                            for k in compare_args
                            if k in exp_args
                        )
                        if all_match:
                            matched += 1
                            break
                    else:
                        matched += 1
                        break
        
        return matched / len(expected_actions)
    
    def render(self):
        """Render the environment state."""
        output = []
        output.append(f"\n{'='*60}")
        output.append(f"Domain: {self.domain_name} | Turn: {self._turn_count}/{self.max_turns}")
        output.append(f"Task: {self._current_task['id'] if self._current_task else 'None'}")
        output.append(f"Tool calls made: {len(self._tool_call_log)}")
        output.append(f"{'='*60}")
        
        for entry in self._conversation_history[-3:]:  # Show last 3 turns
            output.append(f"\n[Turn {entry['turn']}]")
            action_preview = entry['agent_action'][:200]
            output.append(f"  Action: {action_preview}")
            obs_preview = entry['observation'][:200]
            output.append(f"  Result: {obs_preview}")
        
        text = "\n".join(output)
        if self.render_mode == "human":
            print(text)
        return text
    
    def get_trajectory(self) -> dict:
        """Get the complete interaction trajectory for logging."""
        return {
            "domain": self.domain_name,
            "task_id": self._current_task["id"] if self._current_task else None,
            "total_turns": self._turn_count,
            "tool_call_log": self._tool_call_log,
            "conversation_history": self._conversation_history,
            "final_reward": self._compute_reward() if self._current_task else 0.0,
        }


def register_bioagent_gym():
    """Register the BIOAgents environment with Gymnasium."""
    try:
        gym.register(
            id=BIOAGENT_ENV_ID,
            entry_point="bioagents.gym.agent_env:BioAgentGymEnv",
        )
    except gym.error.Error:
        # Already registered
        pass

```

ì´ì œ ë‚¨ì€ `__init__.py` íŒŒì¼ë“¤ê³¼ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ë§Œë“¤ê² ìŠµë‹ˆë‹¤.

```python
"""BIOAgents tool utilities."""

```

```python
"""BIOAgents evaluation module."""

```

```python
"""BIOAgents training module."""

```

```python
"""BIOAgents utility functions."""

```

```python
"""Medical QA Domain - Coming soon."""

```

```python
"""Visual Diagnosis Domain - Coming soon."""

```

```python
"""EHR Management Domain - Coming soon."""

```

```python
"""Drug Interaction Domain - Coming soon."""

```

ì´ì œ í†µí•© í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‘ì„±í•˜ê³  ì‹¤í–‰í•©ë‹ˆë‹¤.

```python
"""Test script for the Clinical Diagnosis domain.

Tests:
1. DB loading and schema validation
2. Tool execution (all tools)
3. Environment setup
4. Gym interface
5. Task loading and basic reward computation
"""

import json
import sys
import os
sys.path.insert(0, os.path.join(os.path.dirname(__file__), ".."))

from bioagents.environment.db import DB
from bioagents.environment.toolkit import ToolKitBase, ToolType, is_tool, ToolDefinition
from bioagents.domains.clinical_diagnosis.data_model import ClinicalDB, DB_PATH, POLICY_PATH, TASKS_PATH
from bioagents.domains.clinical_diagnosis.tools import ClinicalTools
from bioagents.domains.clinical_diagnosis.environment import get_environment, get_tasks


def test_db_loading():
    """Test database loading and validation."""
    print("\n=== Test 1: DB Loading ===")
    db = ClinicalDB.load(DB_PATH)
    
    assert len(db.patients) == 3, f"Expected 3 patients, got {len(db.patients)}"
    assert "P001" in db.patients
    assert "P002" in db.patients
    assert "P003" in db.patients
    
    p1 = db.patients["P001"]
    assert p1.name == "James Wilson"
    assert p1.age == 58
    assert len(p1.allergies) == 2
    assert len(p1.medications) == 3
    assert len(p1.vital_signs) == 2
    assert len(p1.lab_results) == 6
    
    # Test hash
    h1 = db.get_hash()
    h2 = db.get_hash()
    assert h1 == h2, "Hash should be deterministic"
    
    # Test dump and reload
    test_path = "/tmp/test_clinical_db.json"
    db.dump(test_path)
    db_reloaded = ClinicalDB.load(test_path)
    assert db.get_hash() == db_reloaded.get_hash(), "Reloaded DB should have same hash"
    os.remove(test_path)
    
    print(f"  âœ“ Loaded {len(db.patients)} patients")
    print(f"  âœ“ {len(db.drug_interactions)} drug interactions")
    print(f"  âœ“ {len(db.clinical_guidelines)} clinical guidelines")
    print(f"  âœ“ DB hash: {h1}")
    print("  âœ“ DB loading test PASSED")


def test_tools():
    """Test all clinical tools."""
    print("\n=== Test 2: Tool Execution ===")
    db = ClinicalDB.load(DB_PATH)
    tools = ClinicalTools(db)
    
    # 2a. get_patient_info
    print("  Testing get_patient_info...")
    info = tools.get_patient_info(patient_id="P001")
    assert info["name"] == "James Wilson"
    assert len(info["allergies"]) == 2
    assert info["chief_complaint"] is not None
    print(f"    âœ“ Patient P001: {info['name']}, age {info['age']}, {len(info['allergies'])} allergies")
    
    # 2b. get_vital_signs
    print("  Testing get_vital_signs...")
    vitals = tools.get_vital_signs(patient_id="P001")
    assert vitals["temperature"] == 39.1
    assert vitals["spo2"] == 91
    print(f"    âœ“ Vitals: T={vitals['temperature']}Â°C, SpO2={vitals['spo2']}%, HR={vitals['heart_rate']}")
    
    # 2c. get_vital_signs_trend
    print("  Testing get_vital_signs_trend...")
    trend = tools.get_vital_signs_trend(patient_id="P001", num_readings=5)
    assert len(trend) == 2  # P001 has 2 readings
    print(f"    âœ“ Got {len(trend)} vital sign readings")
    
    # 2d. get_lab_results
    print("  Testing get_lab_results...")
    labs = tools.get_lab_results(patient_id="P001")
    assert len(labs) == 6
    critical_labs = [l for l in labs if l["flag"] == "critical"]
    print(f"    âœ“ {len(labs)} lab results, {len(critical_labs)} critical")
    
    # 2e. get_lab_results with filter
    labs_chem = tools.get_lab_results(patient_id="P001", category="chemistry")
    assert all(l["category"] == "chemistry" for l in labs_chem)
    print(f"    âœ“ Filtered chemistry labs: {len(labs_chem)}")
    
    # 2f. order_lab_test
    print("  Testing order_lab_test...")
    order = tools.order_lab_test(patient_id="P001", test_name="Chest X-Ray", priority="urgent")
    assert order["status"] == "ordered"
    assert order["priority"] == "urgent"
    print(f"    âœ“ Order {order['order_id']} placed: {order['test_name']}")
    
    # 2g. get_medications
    print("  Testing get_medications...")
    meds = tools.get_medications(patient_id="P001")
    assert len(meds) == 3
    print(f"    âœ“ {len(meds)} current medications: {[m['drug_name'] for m in meds]}")
    
    # 2h. check_drug_interaction
    print("  Testing check_drug_interaction...")
    interaction = tools.check_drug_interaction(drug_a="Atorvastatin", drug_b="Clarithromycin")
    assert interaction["severity"] == "major"
    print(f"    âœ“ Atorvastatin + Clarithromycin: {interaction['severity']} interaction")
    
    no_interaction = tools.check_drug_interaction(drug_a="Metformin", drug_b="Atorvastatin")
    assert no_interaction["severity"] == "none"
    print(f"    âœ“ Metformin + Atorvastatin: {no_interaction['severity']}")
    
    # 2i. prescribe_medication
    print("  Testing prescribe_medication...")
    rx = tools.prescribe_medication(
        patient_id="P003", drug_name="Acetaminophen", dosage="1000mg", frequency="every 6 hours"
    )
    assert rx["status"] == "prescribed"
    print(f"    âœ“ Prescribed {rx['drug_name']} {rx['dosage']} for P003")
    
    # Test allergy detection
    try:
        tools.prescribe_medication(
            patient_id="P001", drug_name="Penicillin V", dosage="500mg", frequency="four times daily"
        )
        assert False, "Should have raised ValueError for allergy"
    except ValueError as e:
        print(f"    âœ“ Allergy detected: {str(e)[:80]}")
    
    # 2j. get_clinical_notes
    print("  Testing get_clinical_notes...")
    notes = tools.get_clinical_notes(patient_id="P001")
    assert len(notes) >= 1
    print(f"    âœ“ {len(notes)} clinical notes for P001")
    
    # 2k. add_clinical_note
    print("  Testing add_clinical_note...")
    note = tools.add_clinical_note(
        patient_id="P001", note_type="progress",
        content="Patient assessed. CAP diagnosed.", diagnosis_codes="J18.9"
    )
    assert note["status"] == "added"
    print(f"    âœ“ Added note {note['note_id']}")
    
    # 2l. get_differential_diagnosis
    print("  Testing get_differential_diagnosis...")
    ddx = tools.get_differential_diagnosis(symptoms="fever, cough, dyspnea")
    assert len(ddx) > 0
    print(f"    âœ“ Differential diagnosis: {[d['condition'] for d in ddx]}")
    
    # 2m. search_clinical_guidelines
    print("  Testing search_clinical_guidelines...")
    guidelines = tools.search_clinical_guidelines(condition="pneumonia")
    assert len(guidelines) > 0
    print(f"    âœ“ Found {len(guidelines)} guideline(s)")
    
    # 2n. record_diagnosis
    print("  Testing record_diagnosis...")
    dx = tools.record_diagnosis(
        patient_id="P001", diagnosis="Community-acquired pneumonia",
        icd10_code="J18.9", confidence="high",
        reasoning="Fever, productive cough, elevated WBC, CRP, procalcitonin, CXR findings"
    )
    assert dx["status"] == "recorded"
    print(f"    âœ“ Recorded diagnosis: {dx['diagnosis']} ({dx['confidence']} confidence)")
    
    # 2o. transfer_to_specialist
    print("  Testing transfer_to_specialist...")
    ref = tools.transfer_to_specialist(
        summary="Suspected bacterial meningitis", specialty="neurology"
    )
    assert "neurology" in ref
    print(f"    âœ“ Referral: {ref[:80]}")
    
    # 2p. think
    print("  Testing think...")
    thought = tools.think(thought="Patient has classic CAP presentation with comorbidities")
    assert thought == ""
    print(f"    âœ“ Think tool works (no output)")
    
    # 2q. Assertion helpers
    assert tools.assert_diagnosis_recorded("P001", "Community-acquired pneumonia")
    assert tools.assert_lab_ordered("P001", "Chest X-Ray")
    assert tools.assert_medication_prescribed("P003", "Acetaminophen")
    print(f"    âœ“ Assertion helpers verified")
    
    # Tool statistics
    stats = tools.get_statistics()
    print(f"\n  Tool Statistics: {stats['num_tools']} total")
    print(f"    READ: {stats['num_read_tools']}, WRITE: {stats['num_write_tools']}")
    print(f"    THINK: {stats['num_think_tools']}, GENERIC: {stats['num_generic_tools']}")
    print(f"    Tools: {stats['tool_names']}")
    
    # Tool definitions (OpenAI format)
    defs = tools.get_tool_definitions_dict()
    print(f"  âœ“ Generated {len(defs)} OpenAI-compatible tool definitions")
    
    print("  âœ“ All tool tests PASSED")


def test_environment():
    """Test environment setup and interaction."""
    print("\n=== Test 3: Environment ===")
    
    env = get_environment()
    assert env.domain_name == "clinical_diagnosis"
    assert env.tools is not None
    assert len(env.policy) > 0
    
    obs, info = env.reset()
    assert "policy" in info
    assert "tools" in info
    print(f"  âœ“ Environment created: {env.domain_name}")
    print(f"  âœ“ Policy length: {len(env.policy)} chars")
    print(f"  âœ“ Tools available: {len(info['tools'])}")
    
    # Test tool execution via environment
    result = env.execute_tool("get_patient_info", patient_id="P001")
    assert not result.error
    print(f"  âœ“ Tool execution through environment works")
    
    # Test step with tool call
    action = json.dumps({"name": "get_vital_signs", "arguments": {"patient_id": "P001"}})
    obs, reward, terminated, truncated, info = env.step(action)
    assert "39.1" in obs  # Should contain temperature
    print(f"  âœ“ Environment step works. Turn: {info['turn_count']}")
    
    # Test state snapshot
    snapshot = env.get_state_snapshot()
    assert snapshot["turn_count"] == 1
    print(f"  âœ“ State snapshot: turn={snapshot['turn_count']}, db_hash={snapshot['db_hash']}")
    
    print("  âœ“ Environment tests PASSED")


def test_tasks():
    """Test task loading."""
    print("\n=== Test 4: Tasks ===")
    
    tasks = get_tasks()
    assert len(tasks) == 5
    print(f"  âœ“ Loaded {len(tasks)} tasks")
    
    for task in tasks:
        tid = task["id"]
        desc = task.get("description", {})
        eval_criteria = task.get("evaluation_criteria", {})
        n_actions = len(eval_criteria.get("actions", []))
        n_assertions = len(eval_criteria.get("nl_assertions", []))
        print(f"    - {tid}: difficulty={desc.get('difficulty', '?')}, "
              f"actions={n_actions}, assertions={n_assertions}")
    
    print("  âœ“ Task loading tests PASSED")


def test_gym_interface():
    """Test Gymnasium-compatible interface."""
    print("\n=== Test 5: GYM Interface ===")
    
    from bioagents.gym.agent_env import BioAgentGymEnv
    
    env = BioAgentGymEnv(
        domain="clinical_diagnosis",
        task_id="dx_pneumonia_001",
        max_turns=10,
    )
    
    obs, info = env.reset()
    assert "dx_pneumonia_001" in obs
    assert "clinical_diagnosis" in info["domain"]
    assert len(info["tools"]) > 0
    print(f"  âœ“ GYM env created. Task: {info['task_id']}")
    print(f"  âœ“ Tools: {len(info['tools'])}")
    print(f"  âœ“ Initial observation ({len(obs)} chars):")
    print(f"    {obs[:200]}...")
    
    # Simulate a simple clinical workflow
    actions = [
        json.dumps({"name": "get_patient_info", "arguments": {"patient_id": "P001"}}),
        json.dumps({"name": "get_vital_signs", "arguments": {"patient_id": "P001"}}),
        json.dumps({"name": "get_lab_results", "arguments": {"patient_id": "P001"}}),
        json.dumps({"name": "get_differential_diagnosis", "arguments": {"symptoms": "fever, cough, dyspnea"}}),
        json.dumps({"name": "search_clinical_guidelines", "arguments": {"condition": "Community-acquired pneumonia"}}),
        json.dumps({"name": "record_diagnosis", "arguments": {
            "patient_id": "P001", 
            "diagnosis": "Community-acquired pneumonia",
            "icd10_code": "J18.9",
            "confidence": "high",
            "reasoning": "Fever, productive cough, elevated WBC/CRP/PCT, crackles on exam"
        }}),
    ]
    
    for i, action in enumerate(actions):
        obs, reward, terminated, truncated, info = env.step(action)
        tool_name = json.loads(action)["name"]
        print(f"  Step {i+1}: {tool_name} â†’ reward={reward:.2f}, done={terminated or truncated}")
    
    # Get trajectory
    trajectory = env.get_trajectory()
    print(f"\n  âœ“ Trajectory: {trajectory['total_turns']} turns, "
          f"{len(trajectory['tool_call_log'])} tool calls")
    print(f"  âœ“ Final reward: {trajectory['final_reward']:.3f}")
    
    # Verify expected actions were covered
    expected_tools = ["get_patient_info", "get_vital_signs", "get_lab_results",
                      "get_differential_diagnosis", "search_clinical_guidelines", "record_diagnosis"]
    actual_tools = [tc["tool_name"] for tc in trajectory["tool_call_log"]]
    
    for et in expected_tools:
        found = et in actual_tools
        status = "âœ“" if found else "âœ—"
        print(f"    {status} {et}")
    
    print(f"\n  âœ“ GYM interface tests PASSED")


def main():
    """Run all tests."""
    print("=" * 60)
    print("BIOAgents Clinical Diagnosis Domain - Test Suite")
    print("=" * 60)
    
    test_db_loading()
    test_tools()
    test_environment()
    test_tasks()
    test_gym_interface()
    
    print("\n" + "=" * 60)
    print("ALL TESTS PASSED âœ“")
    print("=" * 60)


if __name__ == "__main__":
    main()

```

í…ŒìŠ¤íŠ¸ë¥¼ ì‹¤í–‰í•˜ê² ìŠµë‹ˆë‹¤.

Gymnasium Text spaceì—ì„œ charsetì´ Noneì¼ ìˆ˜ ì—†ëŠ” ë¬¸ì œë¥¼ ìˆ˜ì •í•©ë‹ˆë‹¤.

```python
"""Gymnasium-compatible environment for BIOAgents.

Provides a standard Gymnasium interface for training RL agents
in medical/biomedical tool-use tasks.
"""

import json
from copy import deepcopy
from typing import Any, Optional

import gymnasium as gym
from gymnasium import spaces

BIOAGENT_ENV_ID = "BioAgent-v0"

# Domain registry
_DOMAIN_REGISTRY = {}


def _register_domain(domain_name: str, get_environment_fn, get_tasks_fn):
    """Register a domain for use in the gym."""
    _DOMAIN_REGISTRY[domain_name] = {
        "get_environment": get_environment_fn,
        "get_tasks": get_tasks_fn,
    }


def _load_default_domains():
    """Load all default domains."""
    try:
        from bioagents.domains.clinical_diagnosis.environment import get_environment, get_tasks
        _register_domain("clinical_diagnosis", get_environment, get_tasks)
    except Exception:
        pass


class BioAgentGymEnv(gym.Env):
    """Gymnasium-compatible environment for biomedical agent training.
    
    Observation space: Text (conversation history + tool results)
    Action space: Text (agent messages or tool calls in JSON)
    
    Usage:
        register_bioagent_gym()
        env = gym.make("BioAgent-v0", domain="clinical_diagnosis", task_id="dx_pneumonia_001")
        obs, info = env.reset()
        obs, reward, terminated, truncated, info = env.step(action)
    """
    
    metadata = {"render_modes": ["human", "ansi"]}

    def __init__(
        self,
        domain: str = "clinical_diagnosis",
        task_id: Optional[str] = None,
        task_split: Optional[str] = None,
        max_turns: int = 20,
        render_mode: Optional[str] = None,
        **kwargs,
    ):
        super().__init__()
        
        self.domain_name = domain
        self.task_id = task_id
        self.task_split = task_split
        self.max_turns = max_turns
        self.render_mode = render_mode
        
        # Load domain
        _load_default_domains()
        if domain not in _DOMAIN_REGISTRY:
            raise ValueError(
                f"Domain '{domain}' not registered. Available: {list(_DOMAIN_REGISTRY.keys())}"
            )
        
        domain_info = _DOMAIN_REGISTRY[domain]
        self._get_environment_fn = domain_info["get_environment"]
        self._get_tasks_fn = domain_info["get_tasks"]
        
        # Load tasks
        self._tasks = self._get_tasks_fn(task_split)
        self._task_map = {t["id"]: t for t in self._tasks}
        
        # Gymnasium spaces (text-based)
        # Use default alphanumeric + common punctuation charset
        _charset = "".join(chr(i) for i in range(32, 127))  # printable ASCII
        self.observation_space = spaces.Text(
            min_length=0, max_length=100000, charset=_charset
        )
        self.action_space = spaces.Text(
            min_length=1, max_length=10000, charset=_charset
        )
        
        # State
        self._env = None
        self._current_task = None
        self._turn_count = 0
        self._conversation_history = []
        self._tool_call_log = []
    
    def reset(
        self, *, seed: Optional[int] = None, options: Optional[dict] = None
    ) -> tuple[str, dict]:
        """Reset the environment with a task.
        
        Args:
            seed: Random seed
            options: Optional dict with 'task_id' to select a specific task
            
        Returns:
            observation: Initial observation (system prompt + patient ticket)
            info: Dict with task info, tools, policy
        """
        super().reset(seed=seed)
        
        # Select task
        task_id = self.task_id
        if options and "task_id" in options:
            task_id = options["task_id"]
        
        if task_id:
            if task_id not in self._task_map:
                raise ValueError(f"Task '{task_id}' not found. Available: {list(self._task_map.keys())}")
            self._current_task = self._task_map[task_id]
        else:
            # Random task selection
            idx = self.np_random.integers(0, len(self._tasks))
            self._current_task = self._tasks[idx]
        
        # Create fresh environment
        self._env = self._get_environment_fn(max_turns=self.max_turns)
        self._turn_count = 0
        self._conversation_history = []
        self._tool_call_log = []
        
        # Build initial observation
        initial_obs = self._build_initial_observation()
        
        info = {
            "task_id": self._current_task["id"],
            "task_description": self._current_task.get("description", {}),
            "domain": self.domain_name,
            "policy": self._env.policy,
            "tools": self._env.get_tool_definitions(),
            "max_turns": self.max_turns,
        }
        
        return initial_obs, info
    
    def step(self, action: str) -> tuple[str, float, bool, bool, dict]:
        """Execute an agent action.
        
        The action can be:
        1. A JSON tool call: {"name": "tool_name", "arguments": {...}}
        2. A text message to the user
        
        Returns:
            observation: Result of the action
            reward: Computed reward (0.0 during interaction, final at end)
            terminated: Whether the episode ended
            truncated: Whether max turns exceeded
            info: Additional information
        """
        self._turn_count += 1
        
        observation, reward, terminated, truncated, info = self._env.step(action)
        
        # Track tool calls
        if info.get("tool_response"):
            self._tool_call_log.append({
                "turn": self._turn_count,
                "tool_name": self._env._last_tool_name,
                "arguments": self._env._last_tool_args,
                "response": info["tool_response"],
            })
        
        # Record in conversation history
        self._conversation_history.append({
            "turn": self._turn_count,
            "agent_action": action,
            "observation": observation,
        })
        
        # Check truncation
        if self._turn_count >= self.max_turns:
            truncated = True
        
        # Compute reward at episode end
        if terminated or truncated:
            reward = self._compute_reward()
        
        info.update({
            "turn_count": self._turn_count,
            "tool_calls": self._tool_call_log,
            "task_id": self._current_task["id"] if self._current_task else None,
        })
        
        if self.render_mode == "human":
            self.render()
        
        return observation, reward, terminated, truncated, info
    
    def _build_initial_observation(self) -> str:
        """Build the initial observation from the task."""
        task = self._current_task
        parts = [
            f"=== BIOAgents Clinical Task: {task['id']} ===",
            "",
            f"Domain: {self.domain_name}",
            "",
            "--- Patient Ticket ---",
            task.get("ticket", "No ticket provided."),
            "",
            "--- Available Tools ---",
        ]
        
        tool_defs = self._env.get_tool_definitions()
        for i, td in enumerate(tool_defs, 1):
            func = td.get("function", {})
            parts.append(f"{i}. {func.get('name', '?')}: {func.get('description', '')[:100]}")
        
        parts.extend([
            "",
            "--- Instructions ---",
            "Use the available tools to assess the patient and provide your clinical recommendation.",
            "To call a tool, respond with JSON: {\"name\": \"tool_name\", \"arguments\": {...}}",
            "When done, provide your final assessment as a text message.",
        ])
        
        return "\n".join(parts)
    
    def _compute_reward(self) -> float:
        """Compute the reward for the completed episode.
        
        Evaluates:
        1. ACTION score: Did the agent call the expected tools?
        2. NL_ASSERTION score: Did the agent's reasoning meet clinical criteria?
        """
        if self._current_task is None:
            return 0.0
        
        eval_criteria = self._current_task.get("evaluation_criteria", {})
        expected_actions = eval_criteria.get("actions", [])
        reward_basis = eval_criteria.get("reward_basis", ["ACTION"])
        
        total_score = 0.0
        num_components = 0
        
        # --- ACTION score ---
        if "ACTION" in reward_basis and expected_actions:
            action_score = self._score_actions(expected_actions)
            total_score += action_score
            num_components += 1
        
        # --- NL_ASSERTION score (placeholder for now) ---
        if "NL_ASSERTION" in reward_basis:
            # This will be computed by LLM judge later
            # For now, give partial credit based on action completion
            assertion_score = total_score / max(num_components, 1)  # mirror action score
            total_score += assertion_score
            num_components += 1
        
        return total_score / max(num_components, 1)
    
    def _score_actions(self, expected_actions: list[dict]) -> float:
        """Score the agent's tool usage against expected actions."""
        if not expected_actions:
            return 1.0
        
        actual_tool_names = [tc["tool_name"] for tc in self._tool_call_log]
        
        matched = 0
        for exp in expected_actions:
            exp_name = exp.get("name", "")
            compare_args = exp.get("compare_args", [])
            exp_args = exp.get("arguments", {})
            
            for tc in self._tool_call_log:
                if tc["tool_name"] == exp_name:
                    # Check argument matching if specified
                    if compare_args:
                        all_match = all(
                            str(tc["arguments"].get(k, "")).lower() == str(exp_args.get(k, "")).lower()
                            for k in compare_args
                            if k in exp_args
                        )
                        if all_match:
                            matched += 1
                            break
                    else:
                        matched += 1
                        break
        
        return matched / len(expected_actions)
    
    def render(self):
        """Render the environment state."""
        output = []
        output.append(f"\n{'='*60}")
        output.append(f"Domain: {self.domain_name} | Turn: {self._turn_count}/{self.max_turns}")
        output.append(f"Task: {self._current_task['id'] if self._current_task else 'None'}")
        output.append(f"Tool calls made: {len(self._tool_call_log)}")
        output.append(f"{'='*60}")
        
        for entry in self._conversation_history[-3:]:  # Show last 3 turns
            output.append(f"\n[Turn {entry['turn']}]")
            action_preview = entry['agent_action'][:200]
            output.append(f"  Action: {action_preview}")
            obs_preview = entry['observation'][:200]
            output.append(f"  Result: {obs_preview}")
        
        text = "\n".join(output)
        if self.render_mode == "human":
            print(text)
        return text
    
    def get_trajectory(self) -> dict:
        """Get the complete interaction trajectory for logging."""
        return {
            "domain": self.domain_name,
            "task_id": self._current_task["id"] if self._current_task else None,
            "total_turns": self._turn_count,
            "tool_call_log": self._tool_call_log,
            "conversation_history": self._conversation_history,
            "final_reward": self._compute_reward() if self._current_task else 0.0,
        }


def register_bioagent_gym():
    """Register the BIOAgents environment with Gymnasium."""
    try:
        gym.register(
            id=BIOAGENT_ENV_ID,
            entry_point="bioagents.gym.agent_env:BioAgentGymEnv",
        )
    except gym.error.Error:
        # Already registered
        pass

```

ëª¨ë“  í…ŒìŠ¤íŠ¸ê°€ í†µê³¼í–ˆìŠµë‹ˆë‹¤! ğŸ‰ ì´ì œ toolì˜ THINK íƒ€ì… ì¹´ìš´íŠ¸ê°€ 0ì¸ ë¬¸ì œ í™•ì¸í•˜ê³  todo ì—…ë°ì´íŠ¸ í›„, ì˜¤ëŠ˜ ë¡œê·¸ë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.

---

**User**

íŠ¹ì • environmentë¼ë˜ê°€ uvë¡œ ì‘ì„±í•˜ê³  í•„ìš”í•œ requirementë¥¼ ê¸°ë¡í•´ë‘ê³  ì§„í–‰í•´ì¤„ë˜?

env bioagent ë“±ìœ¼ë¡œ í•˜ë‚˜ í†µí•©í•´ì„œ ë§Œë“¤ì–´ì¤˜

---

**Cursor**

uvê°€ ì—†ìœ¼ë‹ˆ ë¨¼ì € ì„¤ì¹˜í•˜ê³ , uv ê¸°ë°˜ìœ¼ë¡œ `bioagent` í™˜ê²½ì„ ì„¸íŒ…í•˜ê² ìŠµë‹ˆë‹¤.

ì´ì œ `pyproject.toml`ì„ ì‘ì„±í•˜ê³  uvë¡œ `bioagent` í™˜ê²½ì„ ë§Œë“¤ê² ìŠµë‹ˆë‹¤.

```plaintext
[project]
name = "bioagents"
version = "0.1.0"
description = "BIOAgents â€“ Medical/Biomedical Agent GYM Framework"
readme = "README.md"
requires-python = ">=3.10"
license = {text = "MIT"}

dependencies = [
    # ---- Core ----
    "pydantic>=2.0,<3",
    "gymnasium>=1.0",
    "numpy>=1.24",

    # ---- LLM / Model serving ----
    "transformers>=4.40",
    "torch>=2.1",
    "accelerate>=0.30",
    "vllm>=0.4",

    # ---- Training (RL + SFT) ----
    "trl>=0.8",
    "deepspeed>=0.14",
    "peft>=0.10",

    # ---- Evaluation metrics ----
    "rouge-score>=0.1",
    "nltk>=3.8",
    "bert-score>=0.3",
    "scikit-learn>=1.3",

    # ---- Data / IO ----
    "datasets>=2.18",
    "pandas>=2.0",
    "pillow>=10.0",

    # ---- Logging & Experiment tracking ----
    "wandb>=0.16",
    "loguru>=0.7",
    "rich>=13.0",

    # ---- Misc ----
    "httpx>=0.27",
    "tqdm>=4.66",
    "pyyaml>=6.0",
]

[project.optional-dependencies]
dev = [
    "pytest>=8.0",
    "pytest-cov>=5.0",
    "ruff>=0.4",
    "ipython>=8.0",
    "jupyter>=1.0",
]
flash = [
    "flash-attn>=2.5",
]

[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[tool.hatch.build.targets.wheel]
packages = ["bioagents"]

[tool.ruff]
line-length = 120
target-version = "py310"

[tool.ruff.lint]
select = ["E", "F", "I", "W"]

[tool.pytest.ini_options]
testpaths = ["tests"]

```

venv í™œì„±í™” ì™„ë£Œ. ì´ì œ í•µì‹¬ ì˜ì¡´ì„±ì„ ë¨¼ì € ì„¤ì¹˜í•©ë‹ˆë‹¤ (GPU ê´€ë ¨ í° íŒ¨í‚¤ì§€ëŠ” ë‹¨ê³„ì ìœ¼ë¡œ).

í•µì‹¬ íŒ¨í‚¤ì§€ ëª¨ë‘ ì„¤ì¹˜ ì™„ë£Œ. ì´ì œ í”„ë¡œì íŠ¸ ìì²´ë¥¼ editable ëª¨ë“œë¡œ ì„¤ì¹˜í•˜ê³  í…ŒìŠ¤íŠ¸ë¥¼ ëŒë ¤ í™•ì¸í•©ë‹ˆë‹¤.

ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼! ì´ì œ ì‹¤ì œ ì„¤ì¹˜ëœ íŒ¨í‚¤ì§€ë¥¼ lock íŒŒì¼ë¡œ ê¸°ë¡í•´ë‘ê² ìŠµë‹ˆë‹¤.

ì¼ì¼ ë¡œê·¸ë„ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.

```markdown
# Daily Log: 2026-02-12 (Day 1)

## ì˜¤ëŠ˜ì˜ ëª©í‘œ
- [x] í”„ë¡œì íŠ¸ ì „ì²´ ë¦¬ì†ŒìŠ¤ ë¶„ì„ & PLANNING.md ì‘ì„±
- [x] ê²°ì •ì‚¬í•­ í™•ì • (ëª¨ë¸: Lingshu-7B ìš°ì„ , ë„ë©”ì¸: clinical_diagnosis ìš°ì„ , ë…¼ë¬¸: framework paper)
- [x] í”„ë¡œì íŠ¸ ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„±
- [ ] clinical_diagnosis ë„ë©”ì¸ core êµ¬í˜„ ì‹œì‘
  - [ ] data_model.py (DB ìŠ¤í‚¤ë§ˆ)
  - [ ] tools.py (Medical tools)
  - [ ] environment.py (í™˜ê²½)
  - [ ] policy.md (ì—ì´ì „íŠ¸ ì •ì±…)
  - [ ] db.json (ì‹œë®¬ë ˆì´ì…˜ DB)
  - [ ] tasks.json (í‰ê°€ task)
- [ ] base environment & tool framework êµ¬í˜„

## ì‘ì—… ê¸°ë¡

### 09:00 - í”„ë¡œì íŠ¸ ë¶„ì„ ì™„ë£Œ
- **ìˆ˜í–‰**: ì „ì²´ ë””ë ‰í† ë¦¬ íŠ¸ë¦¬ ë¶„ì„, ë³´ìœ  ë°ì´í„°ì…‹/ëª¨ë¸/í”„ë ˆì„ì›Œí¬ ì „ìˆ˜ ì¡°ì‚¬
- **ë°œê²¬ì‚¬í•­**:
  - Ï„Â²-bench ë„ë©”ì¸ íŒ¨í„´ì´ ë§¤ìš° ê¹”ë” (data_model â†’ tools â†’ environment êµ¬ì¡°)
  - AgentGym-RLì˜ verl ê¸°ë°˜ RL trainer í™œìš© ê°€ëŠ¥
  - tool-dataset-generationì˜ tool simulation ì—”ì§„ ì¬í™œìš© ê°€ëŠ¥
  - MRPOì˜ BERTScore+ROUGE reward ì²´ê³„ ì°¸ê³  ê°€ëŠ¥
  - databases/instructionì— 52k+ ì˜ë£Œ instruction ë°ì´í„° ë³´ìœ 
- **ì‚°ì¶œë¬¼**: `PLANNING.md` (ì „ì²´ ê¸°íš ë¬¸ì„œ)

### 09:30 - ê²°ì •ì‚¬í•­ í™•ì •
- ì£¼ ëª¨ë¸: Lingshu-7B â†’ Qwen2.5-VL-7B ìˆœì„œ
- ë„ë©”ì¸: clinical_diagnosis ë¨¼ì €
- ë…¼ë¬¸: framework paper
- ì¼ì¼ ë¡œê·¸ ì‹œìŠ¤í…œ: logs/daily/YYYY-MM-DD.md

### 10:00 - ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„±
```
bioagents/                    # ë©”ì¸ íŒ¨í‚¤ì§€
â”œâ”€â”€ domains/                  # ë„ë©”ì¸ë³„ ì½”ë“œ
â”‚   â”œâ”€â”€ clinical_diagnosis/   # ì„ìƒ ì§„ë‹¨ ë„ë©”ì¸
â”‚   â”œâ”€â”€ medical_qa/           # ì˜ë£Œ QA ë„ë©”ì¸
â”‚   â”œâ”€â”€ visual_diagnosis/     # ì‹œê° ì§„ë‹¨ ë„ë©”ì¸
â”‚   â”œâ”€â”€ ehr_management/       # EHR ê´€ë¦¬ ë„ë©”ì¸
â”‚   â””â”€â”€ drug_interaction/     # ì•½ë¬¼ ìƒí˜¸ì‘ìš© ë„ë©”ì¸
â”œâ”€â”€ gym/                      # Gymnasium ì¸í„°í˜ì´ìŠ¤
â”œâ”€â”€ environment/              # í™˜ê²½ ë² ì´ìŠ¤ í´ë˜ìŠ¤
â”œâ”€â”€ tools/                    # ë„êµ¬ ë² ì´ìŠ¤ í´ë˜ìŠ¤
â”œâ”€â”€ evaluation/               # í‰ê°€ ëª¨ë“ˆ
â”œâ”€â”€ training/                 # í•™ìŠµ ëª¨ë“ˆ
â””â”€â”€ utils/                    # ìœ í‹¸ë¦¬í‹°
data/domains/                 # ë„ë©”ì¸ë³„ ë°ì´í„° (db.json, tasks.json, policy.md)
configs/                      # í•™ìŠµ/í‰ê°€ ì„¤ì •
checkpoints/                  # ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸
logs/daily/                   # ì¼ì¼ ì‘ì—… ë¡œê·¸
```

### 10:30 - clinical_diagnosis ë„ë©”ì¸ êµ¬í˜„
- `data_model.py` â€“ Patient, LabResult, VitalSigns, Medication, ClinicalNote, DrugInteraction, ClinicalGuideline, ClinicalDB
- `tools.py` â€“ 17ê°œ ë„êµ¬ êµ¬í˜„ (READ 11, WRITE 4, THINK 0, GENERIC 2)
- `environment.py` â€“ get_environment(), get_tasks()
- `data/domains/clinical_diagnosis/db.json` â€“ í™˜ì 3ëª… (P001 íë ´, P002 ìˆ˜ë§‰ì—¼, P003 ì¶©ìˆ˜ì—¼), ì•½ë¬¼ìƒí˜¸ì‘ìš© 6ê±´, ê°€ì´ë“œë¼ì¸ 3ê±´
- `data/domains/clinical_diagnosis/policy.md` â€“ ì—ì´ì „íŠ¸ í–‰ë™ ì •ì±… ë¬¸ì„œ
- `data/domains/clinical_diagnosis/tasks.json` â€“ 5ê°œ í‰ê°€ task

### 11:00 - Base Framework êµ¬í˜„
- `bioagents/environment/db.py` â€“ DB ë² ì´ìŠ¤ í´ë˜ìŠ¤ (load/dump/hash)
- `bioagents/environment/toolkit.py` â€“ ToolKitBase, @is_tool decorator, ToolDefinition (OpenAI í˜¸í™˜)
- `bioagents/environment/environment.py` â€“ Environment (reset/step/execute_tool)

### 11:30 - GYM ì¸í„°í˜ì´ìŠ¤ êµ¬í˜„
- `bioagents/gym/agent_env.py` â€“ BioAgentGymEnv (Gymnasium í˜¸í™˜)
- observation/action: Text space
- ë„ë©”ì¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬, ìë™ reward ê³„ì‚° (ACTION + NL_ASSERTION)

### 12:00 - í…ŒìŠ¤íŠ¸ í†µê³¼ í™•ì¸
- `tests/test_clinical_diagnosis.py` â€“ 5ê°œ í…ŒìŠ¤íŠ¸ ëª¨ë“ˆ ì „ë¶€ PASS
  - DB loading, Tool execution (17 tools), Environment, Tasks, GYM interface
  - reward=1.000 (perfect action score on dx_pneumonia_001)

### 12:30 - í™˜ê²½ & ì˜ì¡´ì„± ê´€ë¦¬ êµ¬ì¶•
- **uv 0.10.2** ì„¤ì¹˜, `.venv` ê°€ìƒí™˜ê²½ ìƒì„± (Python 3.12.4)
- `pyproject.toml` ì‘ì„± â€“ í”„ë¡œì íŠ¸ ë©”íƒ€ë°ì´í„° + ì „ì²´ ì˜ì¡´ì„± ì •ì˜
- í•µì‹¬ íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì™„ë£Œ:
  - torch 2.9.1+cu128 (CUDA âœ“)
  - transformers 4.57.6
  - trl 0.28.0
  - gymnasium 1.2.3
  - vllm 0.15.1
  - deepspeed, peft, accelerate, wandb ë“±
- `requirements.lock` â€“ 203ê°œ íŒ¨í‚¤ì§€ ë²„ì „ ê³ ì •
- `pip install -e .` (editable mode) â†’ ìƒˆ í™˜ê²½ì—ì„œ ì „ í…ŒìŠ¤íŠ¸ ì¬í†µê³¼ í™•ì¸

---

## í™˜ê²½ ì •ë³´
```
Python 3.12.4 (.venv via uv)
torch 2.9.1+cu128 (CUDA enabled)
transformers 4.57.6
trl 0.28.0 (GRPO/PPO)
gymnasium 1.2.3
vllm 0.15.1
203 packages total (requirements.lock)
```

## ì‚¬ìš© ë¦¬ì†ŒìŠ¤
- ì°¸ê³  ì½”ë“œ: databases/tau2-bench/src/tau2/domains/mock/
- ì°¸ê³  ì½”ë“œ: tool_simulations/tool-dataset-generation/utils/tool_simulation/prompt.py

## ë‚´ì¼ í•  ì¼
- Lingshu-7B ê¸°ë°˜ clinical_diagnosis task ì‹¤í–‰ (inference)
- medical_qa ë„ë©”ì¸ êµ¬í˜„
- LLM-as-Judge NL_ASSERTION reward êµ¬í˜„

```

ì™„ë£Œ! í™˜ê²½ êµ¬ì„± ìš”ì•½ì…ë‹ˆë‹¤:

## í™˜ê²½ ì„¸íŒ… ì™„ë£Œ âœ“

| í•­ëª© | ë‚´ìš© |
|---|---|
| **ê°€ìƒí™˜ê²½** | `.venv` (uv 0.10.2, Python 3.12.4) |
| **í™œì„±í™”** | `source .venv/bin/activate` |
| **í”„ë¡œì íŠ¸ ì„¤ì¹˜** | `uv pip install -e .` (editable) |
| **ì˜ì¡´ì„± ì •ì˜** | `pyproject.toml` (core + dev + flash ê·¸ë£¹) |
| **ë²„ì „ ê³ ì •** | `requirements.lock` (203 íŒ¨í‚¤ì§€) |

### ì£¼ìš” íŒ¨í‚¤ì§€ ë²„ì „

```
torch          2.9.1+cu128  (CUDA âœ“)
transformers   4.57.6
trl            0.28.0       (GRPO/PPO)
gymnasium      1.2.3
vllm           0.15.1
deepspeed, peft, accelerate, wandb â€¦
```

### ì‚¬ìš©ë²•

```bash
cd /data/project/private/minstar/workspace/BIOAgents
source .venv/bin/activate

# í…ŒìŠ¤íŠ¸ ì‹¤í–‰
python tests/test_clinical_diagnosis.py

# ë˜ëŠ” pytest
pytest tests/ -v

# í™˜ê²½ ì¬í˜„ (ë‹¤ë¥¸ ë¨¸ì‹ )
uv venv .venv --python 3.12
source .venv/bin/activate
uv pip install -r requirements.lock
uv pip install -e .
```

`pyproject.toml`ì— ëª¨ë“  ì˜ì¡´ì„±ì´ ì„ ì–¸ë˜ì–´ ìˆê³ , `requirements.lock`ì— ì •í™•í•œ ë²„ì „ì´ ê³ ì •ë˜ì–´ ìˆì–´ì„œ ì–´ë””ì„œë“  ë™ì¼í•˜ê²Œ ì¬í˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì•ìœ¼ë¡œ ìƒˆ íŒ¨í‚¤ì§€ ì¶”ê°€ ì‹œ `pyproject.toml` â†’ `requirements.lock` ìˆœìœ¼ë¡œ ì—…ë°ì´íŠ¸í• ê²Œìš”.

```plaintext
torch          2.9.1+cu128  (CUDA âœ“)
transformers   4.57.6
trl            0.28.0       (GRPO/PPO)
gymnasium      1.2.3
vllm           0.15.1
deepspeed, peft, accelerate, wandb â€¦
```

```shellscript
cd /data/project/private/minstar/workspace/BIOAgents
source .venv/bin/activate

# í…ŒìŠ¤íŠ¸ ì‹¤í–‰
python tests/test_clinical_diagnosis.py

# ë˜ëŠ” pytest
pytest tests/ -v

# í™˜ê²½ ì¬í˜„ (ë‹¤ë¥¸ ë¨¸ì‹ )
uv venv .venv --python 3.12
source .venv/bin/activate
uv pip install -r requirements.lock
uv pip install -e .
```

---

**User**

ì´ì–´ì„œ ê³„ì† ì§„í–‰í•´ë´. í›„ë³´êµ° ëª¨ë¸ë¡œë„ ì‹¤í—˜í•´ë´ë„ì¢‹ì„ê²ƒ ê°™ì•„

---

**Cursor**

ë¨¼ì € í›„ë³´ ëª¨ë¸ ê²½ë¡œë¥¼ í™•ì¸í•˜ê³ , LLM Agent Runnerë¥¼ ë§Œë“¤ê² ìŠµë‹ˆë‹¤.

8x A100-80GB ì „ë¶€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤! ëª¨ë¸ ë‹¤ìš´ë¡œë“œë¥¼ ë°±ê·¸ë¼ìš´ë“œë¡œ ì‹œì‘í•˜ë©´ì„œ ë™ì‹œì— Agent Runnerë¥¼ êµ¬í˜„í•˜ê² ìŠµë‹ˆë‹¤.

ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì§„í–‰ ì¤‘. ì´ì œ í•µì‹¬ì¸ **LLM Agent Runner**ë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤.

```python
"""LLM Agent Runner for BIOAgents.

Connects a language model to the BIOAgents environment for multi-turn
tool-use evaluation. Supports:
- vLLM-based fast inference
- HuggingFace transformers-based inference
- Multi-turn tool calling with automatic parsing
- Full trajectory logging

Usage:
    runner = AgentRunner(
        model_name_or_path="Qwen/Qwen2.5-VL-7B-Instruct",
        backend="vllm",
    )
    results = runner.run_task(domain="clinical_diagnosis", task_id="dx_pneumonia_001")
"""

import json
import os
import re
import time
from copy import deepcopy
from dataclasses import dataclass, field
from datetime import datetime
from pathlib import Path
from typing import Any, Literal, Optional

from loguru import logger


@dataclass
class RunConfig:
    """Configuration for an agent run."""
    model_name_or_path: str
    backend: Literal["vllm", "transformers"] = "transformers"
    domain: str = "clinical_diagnosis"
    task_ids: Optional[list[str]] = None       # None = run all tasks
    task_split: Optional[str] = None
    max_turns: int = 15
    temperature: float = 0.1
    top_p: float = 0.95
    max_new_tokens: int = 1024
    tensor_parallel_size: int = 1
    gpu_memory_utilization: float = 0.85
    log_dir: str = "logs/runs"
    seed: int = 42


@dataclass
class TurnRecord:
    """Record of a single turn in the agent-environment interaction."""
    turn_idx: int
    prompt: str = ""
    raw_output: str = ""
    parsed_tool_call: Optional[dict] = None
    tool_response: Optional[str] = None
    is_final_answer: bool = False
    latency_seconds: float = 0.0


@dataclass 
class TaskResult:
    """Result of running a single task."""
    task_id: str
    domain: str
    model_name: str
    turns: list[TurnRecord] = field(default_factory=list)
    total_turns: int = 0
    action_score: float = 0.0
    final_reward: float = 0.0
    completed: bool = False
    error: Optional[str] = None
    trajectory: dict = field(default_factory=dict)
    start_time: str = ""
    end_time: str = ""
    total_latency: float = 0.0


def build_system_prompt(policy: str, tools: list[dict]) -> str:
    """Build the system prompt with policy and tool definitions."""
    tool_section = json.dumps(tools, indent=2, ensure_ascii=False)
    
    return f"""You are a medical AI assistant operating in a clinical environment. Follow the policy below and use the available tools to help with patient care.

## Policy
{policy}

## Available Tools
You have access to the following tools. To use a tool, respond with ONLY a JSON object in this exact format:
```json
{{"name": "tool_name", "arguments": {{"arg1": "value1", "arg2": "value2"}}}}
```

Do NOT include any other text when making a tool call. One tool call per response.

When you have gathered enough information and want to give your final assessment, respond with your clinical analysis as plain text (no JSON).

## Tool Definitions
{tool_section}"""


def parse_tool_call(text: str) -> Optional[dict]:
    """Parse a tool call from model output.
    
    Supports formats:
    1. Pure JSON: {"name": "...", "arguments": {...}}
    2. JSON in code block: ```json\n{...}\n```
    3. JSON embedded in text with markers
    """
    text = text.strip()
    
    # Try 1: Extract JSON from code blocks
    code_block_match = re.search(r'```(?:json)?\s*\n?({.*?})\s*\n?```', text, re.DOTALL)
    if code_block_match:
        try:
            parsed = json.loads(code_block_match.group(1))
            if "name" in parsed:
                return parsed
        except json.JSONDecodeError:
            pass
    
    # Try 2: Direct JSON parse
    try:
        parsed = json.loads(text)
        if isinstance(parsed, dict) and "name" in parsed:
            return parsed
    except json.JSONDecodeError:
        pass
    
    # Try 3: Find JSON-like pattern in text
    json_match = re.search(r'(\{[^{}]*"name"\s*:\s*"[^"]+?"[^{}]*"arguments"\s*:\s*\{[^{}]*\}[^{}]*\})', text, re.DOTALL)
    if json_match:
        try:
            parsed = json.loads(json_match.group(1))
            if "name" in parsed:
                return parsed
        except json.JSONDecodeError:
            pass
    
    # Try 4: More lenient nested JSON search
    matches = list(re.finditer(r'\{', text))
    for m in matches:
        start = m.start()
        depth = 0
        for i in range(start, len(text)):
            if text[i] == '{':
                depth += 1
            elif text[i] == '}':
                depth -= 1
                if depth == 0:
                    candidate = text[start:i+1]
                    try:
                        parsed = json.loads(candidate)
                        if isinstance(parsed, dict) and "name" in parsed:
                            return parsed
                    except json.JSONDecodeError:
                        pass
                    break
    
    return None


class AgentRunner:
    """Runs LLM agents in BIOAgents environments."""
    
    def __init__(self, config: RunConfig):
        self.config = config
        self.model = None
        self.tokenizer = None
        self._setup_logging()
    
    def _setup_logging(self):
        """Set up logging directory."""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        model_short = Path(self.config.model_name_or_path).name
        self.run_id = f"{model_short}_{self.config.domain}_{timestamp}"
        self.log_path = Path(self.config.log_dir) / self.run_id
        self.log_path.mkdir(parents=True, exist_ok=True)
        
        # Save config
        config_dict = {
            k: v for k, v in self.config.__dict__.items()
        }
        with open(self.log_path / "config.json", "w") as f:
            json.dump(config_dict, f, indent=2, default=str)
    
    def load_model(self):
        """Load the language model."""
        if self.config.backend == "vllm":
            self._load_vllm()
        else:
            self._load_transformers()
    
    def _load_vllm(self):
        """Load model with vLLM."""
        from vllm import LLM, SamplingParams
        
        logger.info(f"Loading {self.config.model_name_or_path} with vLLM (tp={self.config.tensor_parallel_size})")
        self.model = LLM(
            model=self.config.model_name_or_path,
            tensor_parallel_size=self.config.tensor_parallel_size,
            gpu_memory_utilization=self.config.gpu_memory_utilization,
            trust_remote_code=True,
            max_model_len=8192,
            seed=self.config.seed,
        )
        self.sampling_params = SamplingParams(
            temperature=self.config.temperature,
            top_p=self.config.top_p,
            max_tokens=self.config.max_new_tokens,
            stop=["```\n\n", "\n\nUser:", "\n\nHuman:"],
        )
        logger.info("vLLM model loaded successfully")
    
    def _load_transformers(self):
        """Load model with HuggingFace transformers."""
        import torch
        from transformers import AutoModelForCausalLM, AutoTokenizer, AutoProcessor
        
        logger.info(f"Loading {self.config.model_name_or_path} with transformers")
        
        model_path = self.config.model_name_or_path
        
        # Check if it's a VL (vision-language) model
        self._is_vl_model = any(kw in model_path.lower() for kw in ["vl", "vision", "lingshu"])
        
        if self._is_vl_model:
            try:
                self.processor = AutoProcessor.from_pretrained(
                    model_path, trust_remote_code=True
                )
                self.tokenizer = self.processor.tokenizer if hasattr(self.processor, 'tokenizer') else self.processor
            except Exception:
                self.tokenizer = AutoTokenizer.from_pretrained(
                    model_path, trust_remote_code=True
                )
                self.processor = None
        else:
            self.tokenizer = AutoTokenizer.from_pretrained(
                model_path, trust_remote_code=True
            )
            self.processor = None
        
        self.model = AutoModelForCausalLM.from_pretrained(
            model_path,
            torch_dtype=torch.bfloat16,
            device_map="auto",
            trust_remote_code=True,
            attn_implementation="flash_attention_2",
        )
        self.model.eval()
        logger.info(f"Transformers model loaded on {self.model.device}")
    
    def generate(self, messages: list[dict]) -> str:
        """Generate a response from the model."""
        if self.config.backend == "vllm":
            return self._generate_vllm(messages)
        else:
            return self._generate_transformers(messages)
    
    def _generate_vllm(self, messages: list[dict]) -> str:
        """Generate with vLLM using chat template."""
        from vllm import SamplingParams
        
        # Use the tokenizer from vLLM engine
        tokenizer = self.model.get_tokenizer()
        prompt = tokenizer.apply_chat_template(
            messages, tokenize=False, add_generation_prompt=True
        )
        
        outputs = self.model.generate([prompt], self.sampling_params)
        return outputs[0].outputs[0].text.strip()
    
    def _generate_transformers(self, messages: list[dict]) -> str:
        """Generate with HuggingFace transformers."""
        import torch
        
        # Apply chat template
        if hasattr(self.tokenizer, 'apply_chat_template'):
            text = self.tokenizer.apply_chat_template(
                messages, tokenize=False, add_generation_prompt=True
            )
        else:
            # Manual template for models without chat template
            text = ""
            for msg in messages:
                role = msg["role"]
                content = msg["content"]
                text += f"<|{role}|>\n{content}\n"
            text += "<|assistant|>\n"
        
        inputs = self.tokenizer(text, return_tensors="pt").to(self.model.device)
        
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=self.config.max_new_tokens,
                temperature=self.config.temperature if self.config.temperature > 0 else None,
                top_p=self.config.top_p if self.config.temperature > 0 else None,
                do_sample=self.config.temperature > 0,
                pad_token_id=self.tokenizer.eos_token_id,
            )
        
        # Decode only the generated tokens
        generated_ids = outputs[0][inputs["input_ids"].shape[-1]:]
        return self.tokenizer.decode(generated_ids, skip_special_tokens=True).strip()
    
    def run_task(self, task: dict, env) -> TaskResult:
        """Run a single task in the environment.
        
        Args:
            task: Task dictionary from tasks.json
            env: BioAgentGymEnv instance
            
        Returns:
            TaskResult with full trajectory and scores
        """
        task_id = task["id"]
        result = TaskResult(
            task_id=task_id,
            domain=self.config.domain,
            model_name=Path(self.config.model_name_or_path).name,
            start_time=datetime.now().isoformat(),
        )
        
        logger.info(f"Starting task: {task_id}")
        
        # Reset environment
        obs, info = env.reset(options={"task_id": task_id})
        
        # Build conversation
        system_prompt = build_system_prompt(info["policy"], info["tools"])
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": obs},
        ]
        
        try:
            for turn_idx in range(self.config.max_turns):
                turn = TurnRecord(turn_idx=turn_idx)
                
                # Generate
                t0 = time.time()
                raw_output = self.generate(messages)
                turn.latency_seconds = time.time() - t0
                turn.raw_output = raw_output
                
                logger.debug(f"Turn {turn_idx}: {raw_output[:200]}...")
                
                # Parse tool call
                tool_call = parse_tool_call(raw_output)
                
                if tool_call is not None:
                    turn.parsed_tool_call = tool_call
                    
                    # Execute tool via environment
                    action = json.dumps(tool_call)
                    observation, reward, terminated, truncated, step_info = env.step(action)
                    
                    turn.tool_response = observation
                    
                    # Add to messages
                    messages.append({"role": "assistant", "content": raw_output})
                    messages.append({
                        "role": "user",
                        "content": f"Tool result for {tool_call['name']}:\n{observation}"
                    })
                    
                    result.turns.append(turn)
                    
                    if terminated or truncated:
                        break
                else:
                    # Final answer (no tool call)
                    turn.is_final_answer = True
                    messages.append({"role": "assistant", "content": raw_output})
                    result.turns.append(turn)
                    
                    # Mark as terminated - agent gave final answer
                    env.state.terminated = True
                    break
            
            # Get final trajectory and reward
            trajectory = env.get_trajectory()
            result.trajectory = trajectory
            result.total_turns = len(result.turns)
            result.final_reward = trajectory["final_reward"]
            result.action_score = self._compute_action_score(task, env._tool_call_log)
            result.completed = True
            
        except Exception as e:
            logger.error(f"Error in task {task_id}: {e}")
            result.error = str(e)
            import traceback
            result.error += "\n" + traceback.format_exc()
        
        result.end_time = datetime.now().isoformat()
        result.total_latency = sum(t.latency_seconds for t in result.turns)
        
        logger.info(
            f"Task {task_id}: turns={result.total_turns}, "
            f"action_score={result.action_score:.3f}, "
            f"reward={result.final_reward:.3f}, "
            f"latency={result.total_latency:.1f}s"
        )
        
        return result
    
    def _compute_action_score(self, task: dict, tool_call_log: list) -> float:
        """Compute action score based on expected vs actual tool calls."""
        eval_criteria = task.get("evaluation_criteria", {})
        expected_actions = eval_criteria.get("actions", [])
        
        if not expected_actions:
            return 1.0
        
        matched = 0
        for exp in expected_actions:
            exp_name = exp.get("name", "")
            compare_args = exp.get("compare_args", [])
            exp_args = exp.get("arguments", {})
            
            for tc in tool_call_log:
                if tc["tool_name"] == exp_name:
                    if compare_args:
                        all_match = all(
                            str(tc["arguments"].get(k, "")).lower() == str(exp_args.get(k, "")).lower()
                            for k in compare_args
                            if k in exp_args
                        )
                        if all_match:
                            matched += 1
                            break
                    else:
                        matched += 1
                        break
        
        return matched / len(expected_actions)
    
    def run_all_tasks(self) -> list[TaskResult]:
        """Run all tasks (or specified task_ids) and return results."""
        from bioagents.gym.agent_env import BioAgentGymEnv
        
        # Create environment
        gym_env = BioAgentGymEnv(
            domain=self.config.domain,
            task_split=self.config.task_split,
            max_turns=self.config.max_turns,
        )
        
        # Get tasks
        if self.config.task_ids:
            task_ids = self.config.task_ids
        else:
            task_ids = [t["id"] for t in gym_env._tasks]
        
        logger.info(f"Running {len(task_ids)} tasks with {Path(self.config.model_name_or_path).name}")
        
        results = []
        for task_id in task_ids:
            task = gym_env._task_map[task_id]
            result = self.run_task(task, gym_env)
            results.append(result)
            self._save_task_result(result)
        
        # Save summary
        self._save_summary(results)
        
        return results
    
    def _save_task_result(self, result: TaskResult):
        """Save individual task result."""
        path = self.log_path / f"task_{result.task_id}.json"
        data = {
            "task_id": result.task_id,
            "domain": result.domain,
            "model_name": result.model_name,
            "total_turns": result.total_turns,
            "action_score": result.action_score,
            "final_reward": result.final_reward,
            "completed": result.completed,
            "error": result.error,
            "total_latency": result.total_latency,
            "start_time": result.start_time,
            "end_time": result.end_time,
            "turns": [
                {
                    "turn_idx": t.turn_idx,
                    "raw_output": t.raw_output,
                    "parsed_tool_call": t.parsed_tool_call,
                    "tool_response": t.tool_response[:500] if t.tool_response else None,
                    "is_final_answer": t.is_final_answer,
                    "latency_seconds": t.latency_seconds,
                }
                for t in result.turns
            ],
        }
        with open(path, "w", encoding="utf-8") as f:
            json.dump(data, f, indent=2, ensure_ascii=False, default=str)
    
    def _save_summary(self, results: list[TaskResult]):
        """Save a summary of all task results."""
        summary = {
            "run_id": self.run_id,
            "model": Path(self.config.model_name_or_path).name,
            "domain": self.config.domain,
            "backend": self.config.backend,
            "num_tasks": len(results),
            "num_completed": sum(1 for r in results if r.completed),
            "num_errors": sum(1 for r in results if r.error),
            "avg_action_score": sum(r.action_score for r in results) / max(len(results), 1),
            "avg_reward": sum(r.final_reward for r in results) / max(len(results), 1),
            "avg_turns": sum(r.total_turns for r in results) / max(len(results), 1),
            "total_latency": sum(r.total_latency for r in results),
            "per_task": [
                {
                    "task_id": r.task_id,
                    "action_score": r.action_score,
                    "final_reward": r.final_reward,
                    "turns": r.total_turns,
                    "latency": r.total_latency,
                    "completed": r.completed,
                    "error": r.error is not None,
                }
                for r in results
            ],
            "timestamp": datetime.now().isoformat(),
        }
        
        path = self.log_path / "summary.json"
        with open(path, "w", encoding="utf-8") as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)
        
        # Print summary table
        print("\n" + "=" * 80)
        print(f"  RUN SUMMARY: {self.run_id}")
        print("=" * 80)
        print(f"  Model: {summary['model']}")
        print(f"  Domain: {summary['domain']}")
        print(f"  Backend: {summary['backend']}")
        print(f"  Tasks: {summary['num_completed']}/{summary['num_tasks']} completed")
        print(f"  Avg Action Score: {summary['avg_action_score']:.3f}")
        print(f"  Avg Reward: {summary['avg_reward']:.3f}")
        print(f"  Avg Turns: {summary['avg_turns']:.1f}")
        print(f"  Total Latency: {summary['total_latency']:.1f}s")
        print("-" * 80)
        print(f"  {'Task ID':<30} {'Score':>8} {'Reward':>8} {'Turns':>6} {'Time':>8}")
        print("-" * 80)
        for t in summary["per_task"]:
            status = "âœ“" if t["completed"] else "âœ—"
            print(f"  {status} {t['task_id']:<28} {t['action_score']:>8.3f} {t['final_reward']:>8.3f} {t['turns']:>6} {t['latency']:>7.1f}s")
        print("=" * 80)
        
        logger.info(f"Results saved to {self.log_path}")

```

ì´ì œ ì‹¤í—˜ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ë§Œë“­ë‹ˆë‹¤.

```python
#!/usr/bin/env python3
"""Run BIOAgents clinical diagnosis experiments with candidate models.

Usage:
    # Single model
    python scripts/run_experiment.py --model Qwen2.5-VL-7B-Instruct --backend vllm

    # All candidate models
    python scripts/run_experiment.py --all

    # Specific task
    python scripts/run_experiment.py --model Qwen2.5-VL-7B-Instruct --task dx_pneumonia_001
"""

import argparse
import json
import os
import sys
from datetime import datetime
from pathlib import Path

# Add project root to path
sys.path.insert(0, str(Path(__file__).parent.parent))

from bioagents.evaluation.agent_runner import AgentRunner, RunConfig
from loguru import logger

# Model paths
MODEL_REGISTRY = {
    "Qwen2.5-VL-7B-Instruct": "checkpoints/models/Qwen2.5-VL-7B-Instruct",
    "Lingshu-7B": "checkpoints/models/Lingshu-7B",
    "Qwen3-8B-Base": "/data/project/private/minstar/models/Qwen3-8B-Base",
}

PROJECT_ROOT = Path(__file__).parent.parent


def resolve_model_path(name: str) -> str:
    """Resolve model name to absolute path."""
    if name in MODEL_REGISTRY:
        path = PROJECT_ROOT / MODEL_REGISTRY[name]
    else:
        path = Path(name)
    
    if not path.exists():
        # Try HuggingFace hub ID
        return name
    return str(path.resolve())


def run_single_experiment(
    model_name: str,
    backend: str = "vllm",
    task_ids: list[str] = None,
    tp_size: int = 1,
):
    """Run experiment with a single model."""
    model_path = resolve_model_path(model_name)
    
    config = RunConfig(
        model_name_or_path=model_path,
        backend=backend,
        domain="clinical_diagnosis",
        task_ids=task_ids,
        max_turns=15,
        temperature=0.1,
        top_p=0.95,
        max_new_tokens=1024,
        tensor_parallel_size=tp_size,
        log_dir=str(PROJECT_ROOT / "logs" / "runs"),
    )
    
    logger.info(f"Starting experiment: {model_name} ({backend})")
    logger.info(f"Model path: {model_path}")
    
    runner = AgentRunner(config)
    runner.load_model()
    results = runner.run_all_tasks()
    
    return results


def main():
    parser = argparse.ArgumentParser(description="BIOAgents Experiment Runner")
    parser.add_argument("--model", type=str, default="Qwen2.5-VL-7B-Instruct",
                        help="Model name (from registry) or path")
    parser.add_argument("--backend", type=str, default="vllm", choices=["vllm", "transformers"],
                        help="Inference backend")
    parser.add_argument("--task", type=str, default=None,
                        help="Specific task ID (default: all tasks)")
    parser.add_argument("--tp", type=int, default=1,
                        help="Tensor parallel size for vLLM")
    parser.add_argument("--all", action="store_true",
                        help="Run all candidate models")
    parser.add_argument("--list-models", action="store_true",
                        help="List available models")
    args = parser.parse_args()
    
    if args.list_models:
        print("\nAvailable models:")
        for name, path in MODEL_REGISTRY.items():
            full_path = PROJECT_ROOT / path
            exists = "âœ“" if full_path.exists() else "âœ— (not downloaded)"
            print(f"  {name:<30} {exists}  â†’ {path}")
        return
    
    task_ids = [args.task] if args.task else None
    
    if args.all:
        all_results = {}
        for model_name in MODEL_REGISTRY:
            model_path = PROJECT_ROOT / MODEL_REGISTRY[model_name]
            if not model_path.exists():
                logger.warning(f"Skipping {model_name}: not found at {model_path}")
                continue
            try:
                results = run_single_experiment(
                    model_name, args.backend, task_ids, args.tp
                )
                all_results[model_name] = results
            except Exception as e:
                logger.error(f"Failed for {model_name}: {e}")
                all_results[model_name] = str(e)
        
        # Print comparison
        print_comparison(all_results)
    else:
        run_single_experiment(args.model, args.backend, task_ids, args.tp)


def print_comparison(all_results: dict):
    """Print a comparison table of all models."""
    print("\n" + "=" * 90)
    print("  MODEL COMPARISON - Clinical Diagnosis Domain")
    print("=" * 90)
    print(f"  {'Model':<30} {'Tasks':>6} {'Avg Score':>10} {'Avg Reward':>11} {'Avg Turns':>10}")
    print("-" * 90)
    
    for model_name, results in all_results.items():
        if isinstance(results, str):
            print(f"  {model_name:<30} ERROR: {results[:40]}")
            continue
        
        n = len(results)
        avg_score = sum(r.action_score for r in results) / max(n, 1)
        avg_reward = sum(r.final_reward for r in results) / max(n, 1)
        avg_turns = sum(r.total_turns for r in results) / max(n, 1)
        print(f"  {model_name:<30} {n:>6} {avg_score:>10.3f} {avg_reward:>11.3f} {avg_turns:>10.1f}")
    
    print("=" * 90)


if __name__ == "__main__":
    main()

```

ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì§„í–‰ ìƒí™©ì„ í™•ì¸í•©ë‹ˆë‹¤.

ë‹¤ìš´ë¡œë“œê°€ ì•„ì§ ì§„í–‰ ì¤‘ì…ë‹ˆë‹¤. í”„ë¡œì„¸ìŠ¤ë¥¼ í™•ì¸í•˜ê³ , ê¸°ë‹¤ë¦¬ëŠ” ë™ì•ˆ Qwen3-8B-Base(ì´ë¯¸ ë¡œì»¬ì— ìˆìŒ)ë¡œ ë¨¼ì € ì‹¤í—˜ì„ ì‹œì‘í•˜ê² ìŠµë‹ˆë‹¤.

ë‹¤ìš´ë¡œë“œ ì§„í–‰ ì¤‘. Qwen3-8B-BaseëŠ” base modelì´ë¼ chat templateì´ ì—†ì„ ìˆ˜ ìˆìœ¼ë‹ˆ Instruct ëª¨ë¸ì„ í™•ì¸í•©ë‹ˆë‹¤.

ì•„ì§ 1/5ì™€ 1/4ë§Œ ë‹¤ìš´ë¡œë“œëœ ìƒíƒœì¸ë° ì „ì²´ í´ë” í¬ê¸°ê°€ ì´ë¯¸ 14G, 13Gì´ë‹ˆ ê±°ì˜ ì™„ë£Œì— ê°€ê¹ìŠµë‹ˆë‹¤. ì ê¹ ë” ê¸°ë‹¤ë¦¬ê² ìŠµë‹ˆë‹¤.

Qwen2.5-VLì€ 5ê°œ shard ì¤‘ 1ê°œ, LingshuëŠ” 4ê°œ ì¤‘ 1ê°œë§Œ ìˆìŠµë‹ˆë‹¤. ë‹¤ìš´ë¡œë“œê°€ ë§ˆì§€ë§‰ shardë¶€í„° ë°›ì•„ì§€ê³  ìˆì–´ì„œ ê³„ì† ì§„í–‰ ì¤‘. ë‹¤ìš´ë¡œë“œë¥¼ ê¸°ë‹¤ë¦¬ë©´ì„œ, ì´ë¯¸ ë¡œì»¬ì— ìˆëŠ” **Qwen3-8B-Base**ë¡œ ë¨¼ì € baseline ì‹¤í—˜ì„ ëŒë¦¬ê² ìŠµë‹ˆë‹¤. chat templateì´ ìˆìœ¼ë‹ˆ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.

```python
#!/usr/bin/env python3
"""Quick baseline experiment with Qwen3-8B-Base (already available locally).

Run while Qwen2.5-VL and Lingshu are downloading.
"""

import json
import os
import sys
import time
from datetime import datetime
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent.parent))

from bioagents.evaluation.agent_runner import AgentRunner, RunConfig, parse_tool_call, build_system_prompt
from bioagents.gym.agent_env import BioAgentGymEnv
from loguru import logger


def run_baseline():
    """Run Qwen3-8B-Base on all clinical diagnosis tasks using transformers."""
    
    model_path = "/data/project/private/minstar/models/Qwen3-8B-Base"
    
    config = RunConfig(
        model_name_or_path=model_path,
        backend="transformers",
        domain="clinical_diagnosis",
        task_ids=None,  # all tasks
        max_turns=15,
        temperature=0.1,
        top_p=0.95,
        max_new_tokens=1024,
        log_dir=str(Path(__file__).parent.parent / "logs" / "runs"),
    )
    
    print("=" * 70)
    print("  BIOAgents Baseline Experiment")
    print(f"  Model: Qwen3-8B-Base")
    print(f"  Domain: clinical_diagnosis")
    print(f"  Time: {datetime.now().isoformat()}")
    print("=" * 70)
    
    runner = AgentRunner(config)
    
    print("\n[1/2] Loading model...")
    runner.load_model()
    print("  âœ“ Model loaded")
    
    print("\n[2/2] Running tasks...")
    results = runner.run_all_tasks()
    
    # Detailed per-task output
    print("\n\n" + "=" * 70)
    print("  DETAILED RESULTS")
    print("=" * 70)
    
    for result in results:
        print(f"\n--- Task: {result.task_id} ---")
        print(f"  Action Score: {result.action_score:.3f}")
        print(f"  Final Reward: {result.final_reward:.3f}")
        print(f"  Turns: {result.total_turns}")
        print(f"  Latency: {result.total_latency:.1f}s")
        if result.error:
            print(f"  ERROR: {result.error[:200]}")
        
        print(f"  Turn-by-turn:")
        for turn in result.turns:
            if turn.parsed_tool_call:
                tc = turn.parsed_tool_call
                print(f"    [{turn.turn_idx}] TOOL: {tc['name']}({json.dumps(tc.get('arguments', {}))[:80]})")
                if turn.tool_response:
                    print(f"         â†’ {turn.tool_response[:100]}...")
            elif turn.is_final_answer:
                print(f"    [{turn.turn_idx}] ANSWER: {turn.raw_output[:150]}...")
            else:
                print(f"    [{turn.turn_idx}] TEXT: {turn.raw_output[:100]}...")
    
    return results


if __name__ == "__main__":
    results = run_baseline()

```

Qwen3 baseline ì‹¤í—˜ì´ ì‹œì‘ëìŠµë‹ˆë‹¤. ë‹¤ìš´ë¡œë“œ ìƒíƒœì™€ baseline ì‹¤í—˜ ì§„í–‰ì„ í™•ì¸í•˜ê² ìŠµë‹ˆë‹¤.

flash_attention_2ê°€ í•„ìš”í•©ë‹ˆë‹¤. ì„¤ì¹˜í•˜ê³  ë‹¤ì‹œ ì‹œë„í•©ë‹ˆë‹¤.

condaì— ì„¤ì¹˜ëì§€ë§Œ .venvì—ëŠ” ì—†ìŠµë‹ˆë‹¤. uvë¡œ ì„¤ì¹˜í•©ë‹ˆë‹¤.

GLIBC í˜¸í™˜ì„± ë¬¸ì œ. flash_attn ëŒ€ì‹  `sdpa` (PyTorch native) attentionì„ ì‚¬ìš©í•˜ë„ë¡ ìˆ˜ì •í•©ë‹ˆë‹¤.

```python
"""LLM Agent Runner for BIOAgents.

Connects a language model to the BIOAgents environment for multi-turn
tool-use evaluation. Supports:
- vLLM-based fast inference
- HuggingFace transformers-based inference
- Multi-turn tool calling with automatic parsing
- Full trajectory logging

Usage:
    runner = AgentRunner(
        model_name_or_path="Qwen/Qwen2.5-VL-7B-Instruct",
        backend="vllm",
    )
    results = runner.run_task(domain="clinical_diagnosis", task_id="dx_pneumonia_001")
"""

import json
import os
import re
import time
from copy import deepcopy
from dataclasses import dataclass, field
from datetime import datetime
from pathlib import Path
from typing import Any, Literal, Optional

from loguru import logger


@dataclass
class RunConfig:
    """Configuration for an agent run."""
    model_name_or_path: str
    backend: Literal["vllm", "transformers"] = "transformers"
    domain: str = "clinical_diagnosis"
    task_ids: Optional[list[str]] = None       # None = run all tasks
    task_split: Optional[str] = None
    max_turns: int = 15
    temperature: float = 0.1
    top_p: float = 0.95
    max_new_tokens: int = 1024
    tensor_parallel_size: int = 1
    gpu_memory_utilization: float = 0.85
    log_dir: str = "logs/runs"
    seed: int = 42


@dataclass
class TurnRecord:
    """Record of a single turn in the agent-environment interaction."""
    turn_idx: int
    prompt: str = ""
    raw_output: str = ""
    parsed_tool_call: Optional[dict] = None
    tool_response: Optional[str] = None
    is_final_answer: bool = False
    latency_seconds: float = 0.0


@dataclass 
class TaskResult:
    """Result of running a single task."""
    task_id: str
    domain: str
    model_name: str
    turns: list[TurnRecord] = field(default_factory=list)
    total_turns: int = 0
    action_score: float = 0.0
    final_reward: float = 0.0
    completed: bool = False
    error: Optional[str] = None
    trajectory: dict = field(default_factory=dict)
    start_time: str = ""
    end_time: str = ""
    total_latency: float = 0.0


def build_system_prompt(policy: str, tools: list[dict]) -> str:
    """Build the system prompt with policy and tool definitions."""
    tool_section = json.dumps(tools, indent=2, ensure_ascii=False)
    
    return f"""You are a medical AI assistant operating in a clinical environment. Follow the policy below and use the available tools to help with patient care.

## Policy
{policy}

## Available Tools
You have access to the following tools. To use a tool, respond with ONLY a JSON object in this exact format:
```json
{{"name": "tool_name", "arguments": {{"arg1": "value1", "arg2": "value2"}}}}
```

Do NOT include any other text when making a tool call. One tool call per response.

When you have gathered enough information and want to give your final assessment, respond with your clinical analysis as plain text (no JSON).

## Tool Definitions
{tool_section}"""


def parse_tool_call(text: str) -> Optional[dict]:
    """Parse a tool call from model output.
    
    Supports formats:
    1. Pure JSON: {"name": "...", "arguments": {...}}
    2. JSON in code block: ```json\n{...}\n```
    3. JSON embedded in text with markers
    """
    text = text.strip()
    
    # Try 1: Extract JSON from code blocks
    code_block_match = re.search(r'```(?:json)?\s*\n?({.*?})\s*\n?```', text, re.DOTALL)
    if code_block_match:
        try:
            parsed = json.loads(code_block_match.group(1))
            if "name" in parsed:
                return parsed
        except json.JSONDecodeError:
            pass
    
    # Try 2: Direct JSON parse
    try:
        parsed = json.loads(text)
        if isinstance(parsed, dict) and "name" in parsed:
            return parsed
    except json.JSONDecodeError:
        pass
    
    # Try 3: Find JSON-like pattern in text
    json_match = re.search(r'(\{[^{}]*"name"\s*:\s*"[^"]+?"[^{}]*"arguments"\s*:\s*\{[^{}]*\}[^{}]*\})', text, re.DOTALL)
    if json_match:
        try:
            parsed = json.loads(json_match.group(1))
            if "name" in parsed:
                return parsed
        except json.JSONDecodeError:
            pass
    
    # Try 4: More lenient nested JSON search
    matches = list(re.finditer(r'\{', text))
    for m in matches:
        start = m.start()
        depth = 0
        for i in range(start, len(text)):
            if text[i] == '{':
                depth += 1
            elif text[i] == '}':
                depth -= 1
                if depth == 0:
                    candidate = text[start:i+1]
                    try:
                        parsed = json.loads(candidate)
                        if isinstance(parsed, dict) and "name" in parsed:
                            return parsed
                    except json.JSONDecodeError:
                        pass
                    break
    
    return None


class AgentRunner:
    """Runs LLM agents in BIOAgents environments."""
    
    def __init__(self, config: RunConfig):
        self.config = config
        self.model = None
        self.tokenizer = None
        self._setup_logging()
    
    def _setup_logging(self):
        """Set up logging directory."""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        model_short = Path(self.config.model_name_or_path).name
        self.run_id = f"{model_short}_{self.config.domain}_{timestamp}"
        self.log_path = Path(self.config.log_dir) / self.run_id
        self.log_path.mkdir(parents=True, exist_ok=True)
        
        # Save config
        config_dict = {
            k: v for k, v in self.config.__dict__.items()
        }
        with open(self.log_path / "config.json", "w") as f:
            json.dump(config_dict, f, indent=2, default=str)
    
    def load_model(self):
        """Load the language model."""
        if self.config.backend == "vllm":
            self._load_vllm()
        else:
            self._load_transformers()
    
    def _load_vllm(self):
        """Load model with vLLM."""
        from vllm import LLM, SamplingParams
        
        logger.info(f"Loading {self.config.model_name_or_path} with vLLM (tp={self.config.tensor_parallel_size})")
        self.model = LLM(
            model=self.config.model_name_or_path,
            tensor_parallel_size=self.config.tensor_parallel_size,
            gpu_memory_utilization=self.config.gpu_memory_utilization,
            trust_remote_code=True,
            max_model_len=8192,
            seed=self.config.seed,
        )
        self.sampling_params = SamplingParams(
            temperature=self.config.temperature,
            top_p=self.config.top_p,
            max_tokens=self.config.max_new_tokens,
            stop=["```\n\n", "\n\nUser:", "\n\nHuman:"],
        )
        logger.info("vLLM model loaded successfully")
    
    def _load_transformers(self):
        """Load model with HuggingFace transformers."""
        import torch
        from transformers import AutoModelForCausalLM, AutoTokenizer, AutoProcessor
        
        logger.info(f"Loading {self.config.model_name_or_path} with transformers")
        
        model_path = self.config.model_name_or_path
        
        # Check if it's a VL (vision-language) model
        self._is_vl_model = any(kw in model_path.lower() for kw in ["vl", "vision", "lingshu"])
        
        if self._is_vl_model:
            try:
                self.processor = AutoProcessor.from_pretrained(
                    model_path, trust_remote_code=True
                )
                self.tokenizer = self.processor.tokenizer if hasattr(self.processor, 'tokenizer') else self.processor
            except Exception:
                self.tokenizer = AutoTokenizer.from_pretrained(
                    model_path, trust_remote_code=True
                )
                self.processor = None
        else:
            self.tokenizer = AutoTokenizer.from_pretrained(
                model_path, trust_remote_code=True
            )
            self.processor = None
        
        # Use sdpa (PyTorch native) attention; fall back to eager if needed
        try:
            self.model = AutoModelForCausalLM.from_pretrained(
                model_path,
                torch_dtype=torch.bfloat16,
                device_map="auto",
                trust_remote_code=True,
                attn_implementation="sdpa",
            )
        except Exception:
            self.model = AutoModelForCausalLM.from_pretrained(
                model_path,
                torch_dtype=torch.bfloat16,
                device_map="auto",
                trust_remote_code=True,
            )
        self.model.eval()
        logger.info(f"Transformers model loaded on {self.model.device}")
    
    def generate(self, messages: list[dict]) -> str:
        """Generate a response from the model."""
        if self.config.backend == "vllm":
            return self._generate_vllm(messages)
        else:
            return self._generate_transformers(messages)
    
    def _generate_vllm(self, messages: list[dict]) -> str:
        """Generate with vLLM using chat template."""
        from vllm import SamplingParams
        
        # Use the tokenizer from vLLM engine
        tokenizer = self.model.get_tokenizer()
        prompt = tokenizer.apply_chat_template(
            messages, tokenize=False, add_generation_prompt=True
        )
        
        outputs = self.model.generate([prompt], self.sampling_params)
        return outputs[0].outputs[0].text.strip()
    
    def _generate_transformers(self, messages: list[dict]) -> str:
        """Generate with HuggingFace transformers."""
        import torch
        
        # Apply chat template
        if hasattr(self.tokenizer, 'apply_chat_template'):
            text = self.tokenizer.apply_chat_template(
                messages, tokenize=False, add_generation_prompt=True
            )
        else:
            # Manual template for models without chat template
            text = ""
            for msg in messages:
                role = msg["role"]
                content = msg["content"]
                text += f"<|{role}|>\n{content}\n"
            text += "<|assistant|>\n"
        
        inputs = self.tokenizer(text, return_tensors="pt").to(self.model.device)
        
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=self.config.max_new_tokens,
                temperature=self.config.temperature if self.config.temperature > 0 else None,
                top_p=self.config.top_p if self.config.temperature > 0 else None,
                do_sample=self.config.temperature > 0,
                pad_token_id=self.tokenizer.eos_token_id,
            )
        
        # Decode only the generated tokens
        generated_ids = outputs[0][inputs["input_ids"].shape[-1]:]
        return self.tokenizer.decode(generated_ids, skip_special_tokens=True).strip()
    
    def run_task(self, task: dict, env) -> TaskResult:
        """Run a single task in the environment.
        
        Args:
            task: Task dictionary from tasks.json
            env: BioAgentGymEnv instance
            
        Returns:
            TaskResult with full trajectory and scores
        """
        task_id = task["id"]
        result = TaskResult(
            task_id=task_id,
            domain=self.config.domain,
            model_name=Path(self.config.model_name_or_path).name,
            start_time=datetime.now().isoformat(),
        )
        
        logger.info(f"Starting task: {task_id}")
        
        # Reset environment
        obs, info = env.reset(options={"task_id": task_id})
        
        # Build conversation
        system_prompt = build_system_prompt(info["policy"], info["tools"])
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": obs},
        ]
        
        try:
            for turn_idx in range(self.config.max_turns):
                turn = TurnRecord(turn_idx=turn_idx)
                
                # Generate
                t0 = time.time()
                raw_output = self.generate(messages)
                turn.latency_seconds = time.time() - t0
                turn.raw_output = raw_output
                
                logger.debug(f"Turn {turn_idx}: {raw_output[:200]}...")
                
                # Parse tool call
                tool_call = parse_tool_call(raw_output)
                
                if tool_call is not None:
                    turn.parsed_tool_call = tool_call
                    
                    # Execute tool via environment
                    action = json.dumps(tool_call)
                    observation, reward, terminated, truncated, step_info = env.step(action)
                    
                    turn.tool_response = observation
                    
                    # Add to messages
                    messages.append({"role": "assistant", "content": raw_output})
                    messages.append({
                        "role": "user",
                        "content": f"Tool result for {tool_call['name']}:\n{observation}"
                    })
                    
                    result.turns.append(turn)
                    
                    if terminated or truncated:
                        break
                else:
                    # Final answer (no tool call)
                    turn.is_final_answer = True
                    messages.append({"role": "assistant", "content": raw_output})
                    result.turns.append(turn)
                    
                    # Mark as terminated - agent gave final answer
                    env.state.terminated = True
                    break
            
            # Get final trajectory and reward
            trajectory = env.get_trajectory()
            result.trajectory = trajectory
            result.total_turns = len(result.turns)
            result.final_reward = trajectory["final_reward"]
            result.action_score = self._compute_action_score(task, env._tool_call_log)
            result.completed = True
            
        except Exception as e:
            logger.error(f"Error in task {task_id}: {e}")
            result.error = str(e)
            import traceback
            result.error += "\n" + traceback.format_exc()
        
        result.end_time = datetime.now().isoformat()
        result.total_latency = sum(t.latency_seconds for t in result.turns)
        
        logger.info(
            f"Task {task_id}: turns={result.total_turns}, "
            f"action_score={result.action_score:.3f}, "
            f"reward={result.final_reward:.3f}, "
            f"latency={result.total_latency:.1f}s"
        )
        
        return result
    
    def _compute_action_score(self, task: dict, tool_call_log: list) -> float:
        """Compute action score based on expected vs actual tool calls."""
        eval_criteria = task.get("evaluation_criteria", {})
        expected_actions = eval_criteria.get("actions", [])
        
        if not expected_actions:
            return 1.0
        
        matched = 0
        for exp in expected_actions:
            exp_name = exp.get("name", "")
            compare_args = exp.get("compare_args", [])
            exp_args = exp.get("arguments", {})
            
            for tc in tool_call_log:
                if tc["tool_name"] == exp_name:
                    if compare_args:
                        all_match = all(
                            str(tc["arguments"].get(k, "")).lower() == str(exp_args.get(k, "")).lower()
                            for k in compare_args
                            if k in exp_args
                        )
                        if all_match:
                            matched += 1
                            break
                    else:
                        matched += 1
                        break
        
        return matched / len(expected_actions)
    
    def run_all_tasks(self) -> list[TaskResult]:
        """Run all tasks (or specified task_ids) and return results."""
        from bioagents.gym.agent_env import BioAgentGymEnv
        
        # Create environment
        gym_env = BioAgentGymEnv(
            domain=self.config.domain,
            task_split=self.config.task_split,
            max_turns=self.config.max_turns,
        )
        
        # Get tasks
        if self.config.task_ids:
            task_ids = self.config.task_ids
        else:
            task_ids = [t["id"] for t in gym_env._tasks]
        
        logger.info(f"Running {len(task_ids)} tasks with {Path(self.config.model_name_or_path).name}")
        
        results = []
        for task_id in task_ids:
            task = gym_env._task_map[task_id]
            result = self.run_task(task, gym_env)
            results.append(result)
            self._save_task_result(result)
        
        # Save summary
        self._save_summary(results)
        
        return results
    
    def _save_task_result(self, result: TaskResult):
        """Save individual task result."""
        path = self.log_path / f"task_{result.task_id}.json"
        data = {
            "task_id": result.task_id,
            "domain": result.domain,
            "model_name": result.model_name,
            "total_turns": result.total_turns,
            "action_score": result.action_score,
            "final_reward": result.final_reward,
            "completed": result.completed,
            "error": result.error,
            "total_latency": result.total_latency,
            "start_time": result.start_time,
            "end_time": result.end_time,
            "turns": [
                {
                    "turn_idx": t.turn_idx,
                    "raw_output": t.raw_output,
                    "parsed_tool_call": t.parsed_tool_call,
                    "tool_response": t.tool_response[:500] if t.tool_response else None,
                    "is_final_answer": t.is_final_answer,
                    "latency_seconds": t.latency_seconds,
                }
                for t in result.turns
            ],
        }
        with open(path, "w", encoding="utf-8") as f:
            json.dump(data, f, indent=2, ensure_ascii=False, default=str)
    
    def _save_summary(self, results: list[TaskResult]):
        """Save a summary of all task results."""
        summary = {
            "run_id": self.run_id,
            "model": Path(self.config.model_name_or_path).name,
            "domain": self.config.domain,
            "backend": self.config.backend,
            "num_tasks": len(results),
            "num_completed": sum(1 for r in results if r.completed),
            "num_errors": sum(1 for r in results if r.error),
            "avg_action_score": sum(r.action_score for r in results) / max(len(results), 1),
            "avg_reward": sum(r.final_reward for r in results) / max(len(results), 1),
            "avg_turns": sum(r.total_turns for r in results) / max(len(results), 1),
            "total_latency": sum(r.total_latency for r in results),
            "per_task": [
                {
                    "task_id": r.task_id,
                    "action_score": r.action_score,
                    "final_reward": r.final_reward,
                    "turns": r.total_turns,
                    "latency": r.total_latency,
                    "completed": r.completed,
                    "error": r.error is not None,
                }
                for r in results
            ],
            "timestamp": datetime.now().isoformat(),
        }
        
        path = self.log_path / "summary.json"
        with open(path, "w", encoding="utf-8") as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)
        
        # Print summary table
        print("\n" + "=" * 80)
        print(f"  RUN SUMMARY: {self.run_id}")
        print("=" * 80)
        print(f"  Model: {summary['model']}")
        print(f"  Domain: {summary['domain']}")
        print(f"  Backend: {summary['backend']}")
        print(f"  Tasks: {summary['num_completed']}/{summary['num_tasks']} completed")
        print(f"  Avg Action Score: {summary['avg_action_score']:.3f}")
        print(f"  Avg Reward: {summary['avg_reward']:.3f}")
        print(f"  Avg Turns: {summary['avg_turns']:.1f}")
        print(f"  Total Latency: {summary['total_latency']:.1f}s")
        print("-" * 80)
        print(f"  {'Task ID':<30} {'Score':>8} {'Reward':>8} {'Turns':>6} {'Time':>8}")
        print("-" * 80)
        for t in summary["per_task"]:
            status = "âœ“" if t["completed"] else "âœ—"
            print(f"  {status} {t['task_id']:<28} {t['action_score']:>8.3f} {t['final_reward']:>8.3f} {t['turns']:>6} {t['latency']:>7.1f}s")
        print("=" * 80)
        
        logger.info(f"Results saved to {self.log_path}")

```

ë™ì‹œì— ë‹¤ìš´ë¡œë“œ ìƒíƒœë„ í™•ì¸í•©ë‹ˆë‹¤.

ëª¨ë“  ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤! ì´ì œ 3ê°œ ëª¨ë¸ì„ ëª¨ë‘ ì‹¤í—˜í•©ë‹ˆë‹¤. Qwen3-8B-Baseë¶€í„° ì¬ì‹¤í–‰í•˜ê³ , vLLMìœ¼ë¡œ Qwen2.5-VLê³¼ Lingshuë„ ë™ì‹œ ì‹¤í–‰í•˜ê² ìŠµë‹ˆë‹¤.

Qwen2.5-VL-7Bê³¼ Lingshu-7B ì‹¤í—˜ë„ ì¤€ë¹„í•©ë‹ˆë‹¤. Lingshu-7Bì˜ ì•„í‚¤í…ì²˜ë¥¼ ë¨¼ì € í™•ì¸í•©ë‹ˆë‹¤.

Lingshu-7Bë„ Qwen2.5-VL ê¸°ë°˜ì…ë‹ˆë‹¤! ë‘ ëª¨ë¸ ëª¨ë‘ VL(Vision-Language) ëª¨ë¸ì´ë¼ ë¡œë”© ë°©ì‹ì„ ì—…ë°ì´íŠ¸í•´ì•¼ í•©ë‹ˆë‹¤.

```python
"""LLM Agent Runner for BIOAgents.

Connects a language model to the BIOAgents environment for multi-turn
tool-use evaluation. Supports:
- vLLM-based fast inference
- HuggingFace transformers-based inference
- Multi-turn tool calling with automatic parsing
- Full trajectory logging

Usage:
    runner = AgentRunner(
        model_name_or_path="Qwen/Qwen2.5-VL-7B-Instruct",
        backend="vllm",
    )
    results = runner.run_task(domain="clinical_diagnosis", task_id="dx_pneumonia_001")
"""

import json
import os
import re
import time
from copy import deepcopy
from dataclasses import dataclass, field
from datetime import datetime
from pathlib import Path
from typing import Any, Literal, Optional

from loguru import logger


@dataclass
class RunConfig:
    """Configuration for an agent run."""
    model_name_or_path: str
    backend: Literal["vllm", "transformers"] = "transformers"
    domain: str = "clinical_diagnosis"
    task_ids: Optional[list[str]] = None       # None = run all tasks
    task_split: Optional[str] = None
    max_turns: int = 15
    temperature: float = 0.1
    top_p: float = 0.95
    max_new_tokens: int = 1024
    tensor_parallel_size: int = 1
    gpu_memory_utilization: float = 0.85
    log_dir: str = "logs/runs"
    seed: int = 42


@dataclass
class TurnRecord:
    """Record of a single turn in the agent-environment interaction."""
    turn_idx: int
    prompt: str = ""
    raw_output: str = ""
    parsed_tool_call: Optional[dict] = None
    tool_response: Optional[str] = None
    is_final_answer: bool = False
    latency_seconds: float = 0.0


@dataclass 
class TaskResult:
    """Result of running a single task."""
    task_id: str
    domain: str
    model_name: str
    turns: list[TurnRecord] = field(default_factory=list)
    total_turns: int = 0
    action_score: float = 0.0
    final_reward: float = 0.0
    completed: bool = False
    error: Optional[str] = None
    trajectory: dict = field(default_factory=dict)
    start_time: str = ""
    end_time: str = ""
    total_latency: float = 0.0


def build_system_prompt(policy: str, tools: list[dict]) -> str:
    """Build the system prompt with policy and tool definitions."""
    tool_section = json.dumps(tools, indent=2, ensure_ascii=False)
    
    return f"""You are a medical AI assistant operating in a clinical environment. Follow the policy below and use the available tools to help with patient care.

## Policy
{policy}

## Available Tools
You have access to the following tools. To use a tool, respond with ONLY a JSON object in this exact format:
```json
{{"name": "tool_name", "arguments": {{"arg1": "value1", "arg2": "value2"}}}}
```

Do NOT include any other text when making a tool call. One tool call per response.

When you have gathered enough information and want to give your final assessment, respond with your clinical analysis as plain text (no JSON).

## Tool Definitions
{tool_section}"""


def parse_tool_call(text: str) -> Optional[dict]:
    """Parse a tool call from model output.
    
    Supports formats:
    1. Pure JSON: {"name": "...", "arguments": {...}}
    2. JSON in code block: ```json\n{...}\n```
    3. JSON embedded in text with markers
    """
    text = text.strip()
    
    # Try 1: Extract JSON from code blocks
    code_block_match = re.search(r'```(?:json)?\s*\n?({.*?})\s*\n?```', text, re.DOTALL)
    if code_block_match:
        try:
            parsed = json.loads(code_block_match.group(1))
            if "name" in parsed:
                return parsed
        except json.JSONDecodeError:
            pass
    
    # Try 2: Direct JSON parse
    try:
        parsed = json.loads(text)
        if isinstance(parsed, dict) and "name" in parsed:
            return parsed
    except json.JSONDecodeError:
        pass
    
    # Try 3: Find JSON-like pattern in text
    json_match = re.search(r'(\{[^{}]*"name"\s*:\s*"[^"]+?"[^{}]*"arguments"\s*:\s*\{[^{}]*\}[^{}]*\})', text, re.DOTALL)
    if json_match:
        try:
            parsed = json.loads(json_match.group(1))
            if "name" in parsed:
                return parsed
        except json.JSONDecodeError:
            pass
    
    # Try 4: More lenient nested JSON search
    matches = list(re.finditer(r'\{', text))
    for m in matches:
        start = m.start()
        depth = 0
        for i in range(start, len(text)):
            if text[i] == '{':
                depth += 1
            elif text[i] == '}':
                depth -= 1
                if depth == 0:
                    candidate = text[start:i+1]
                    try:
                        parsed = json.loads(candidate)
                        if isinstance(parsed, dict) and "name" in parsed:
                            return parsed
                    except json.JSONDecodeError:
                        pass
                    break
    
    return None


class AgentRunner:
    """Runs LLM agents in BIOAgents environments."""
    
    def __init__(self, config: RunConfig):
        self.config = config
        self.model = None
        self.tokenizer = None
        self._setup_logging()
    
    def _setup_logging(self):
        """Set up logging directory."""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        model_short = Path(self.config.model_name_or_path).name
        self.run_id = f"{model_short}_{self.config.domain}_{timestamp}"
        self.log_path = Path(self.config.log_dir) / self.run_id
        self.log_path.mkdir(parents=True, exist_ok=True)
        
        # Save config
        config_dict = {
            k: v for k, v in self.config.__dict__.items()
        }
        with open(self.log_path / "config.json", "w") as f:
            json.dump(config_dict, f, indent=2, default=str)
    
    def load_model(self):
        """Load the language model."""
        if self.config.backend == "vllm":
            self._load_vllm()
        else:
            self._load_transformers()
    
    def _load_vllm(self):
        """Load model with vLLM."""
        from vllm import LLM, SamplingParams
        
        logger.info(f"Loading {self.config.model_name_or_path} with vLLM (tp={self.config.tensor_parallel_size})")
        self.model = LLM(
            model=self.config.model_name_or_path,
            tensor_parallel_size=self.config.tensor_parallel_size,
            gpu_memory_utilization=self.config.gpu_memory_utilization,
            trust_remote_code=True,
            max_model_len=8192,
            seed=self.config.seed,
        )
        self.sampling_params = SamplingParams(
            temperature=self.config.temperature,
            top_p=self.config.top_p,
            max_tokens=self.config.max_new_tokens,
            stop=["```\n\n", "\n\nUser:", "\n\nHuman:"],
        )
        logger.info("vLLM model loaded successfully")
    
    def _load_transformers(self):
        """Load model with HuggingFace transformers."""
        import torch
        from transformers import (
            AutoModelForCausalLM,
            AutoTokenizer,
            AutoProcessor,
            AutoConfig,
        )
        
        logger.info(f"Loading {self.config.model_name_or_path} with transformers")
        
        model_path = self.config.model_name_or_path
        
        # Detect model type from config
        config = AutoConfig.from_pretrained(model_path, trust_remote_code=True)
        model_type = getattr(config, "model_type", "")
        architectures = getattr(config, "architectures", [])
        
        self._is_vl_model = any(
            "vl" in a.lower() or "vision" in a.lower()
            for a in (architectures or [])
        ) or "vl" in model_type.lower()
        
        logger.info(f"Model type: {model_type}, VL: {self._is_vl_model}, Arch: {architectures}")
        
        # Load tokenizer / processor
        if self._is_vl_model:
            try:
                self.processor = AutoProcessor.from_pretrained(
                    model_path, trust_remote_code=True
                )
                self.tokenizer = (
                    self.processor.tokenizer
                    if hasattr(self.processor, "tokenizer")
                    else self.processor
                )
            except Exception:
                self.tokenizer = AutoTokenizer.from_pretrained(
                    model_path, trust_remote_code=True
                )
                self.processor = None
        else:
            self.tokenizer = AutoTokenizer.from_pretrained(
                model_path, trust_remote_code=True
            )
            self.processor = None
        
        # Load model â€“ choose the right Auto class
        load_kwargs = dict(
            torch_dtype=torch.bfloat16,
            device_map="auto",
            trust_remote_code=True,
        )
        
        # Try sdpa first, then fall back
        for attn_impl in ["sdpa", None]:
            try:
                kw = {**load_kwargs}
                if attn_impl:
                    kw["attn_implementation"] = attn_impl
                
                if self._is_vl_model:
                    # VL models need their specific class
                    from transformers import Qwen2_5_VLForConditionalGeneration
                    self.model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
                        model_path, **kw
                    )
                else:
                    self.model = AutoModelForCausalLM.from_pretrained(
                        model_path, **kw
                    )
                break
            except Exception as e:
                logger.warning(f"Failed with attn_impl={attn_impl}: {e}")
                if attn_impl is None:
                    raise
        
        self.model.eval()
        logger.info(f"Model loaded (VL={self._is_vl_model})")
    
    def generate(self, messages: list[dict]) -> str:
        """Generate a response from the model."""
        if self.config.backend == "vllm":
            return self._generate_vllm(messages)
        else:
            return self._generate_transformers(messages)
    
    def _generate_vllm(self, messages: list[dict]) -> str:
        """Generate with vLLM using chat template."""
        from vllm import SamplingParams
        
        # Use the tokenizer from vLLM engine
        tokenizer = self.model.get_tokenizer()
        prompt = tokenizer.apply_chat_template(
            messages, tokenize=False, add_generation_prompt=True
        )
        
        outputs = self.model.generate([prompt], self.sampling_params)
        return outputs[0].outputs[0].text.strip()
    
    def _generate_transformers(self, messages: list[dict]) -> str:
        """Generate with HuggingFace transformers."""
        import torch
        
        # Apply chat template
        if hasattr(self.tokenizer, 'apply_chat_template'):
            text = self.tokenizer.apply_chat_template(
                messages, tokenize=False, add_generation_prompt=True
            )
        else:
            # Manual template for models without chat template
            text = ""
            for msg in messages:
                role = msg["role"]
                content = msg["content"]
                text += f"<|{role}|>\n{content}\n"
            text += "<|assistant|>\n"
        
        inputs = self.tokenizer(text, return_tensors="pt").to(self.model.device)
        
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=self.config.max_new_tokens,
                temperature=self.config.temperature if self.config.temperature > 0 else None,
                top_p=self.config.top_p if self.config.temperature > 0 else None,
                do_sample=self.config.temperature > 0,
                pad_token_id=self.tokenizer.eos_token_id,
            )
        
        # Decode only the generated tokens
        generated_ids = outputs[0][inputs["input_ids"].shape[-1]:]
        return self.tokenizer.decode(generated_ids, skip_special_tokens=True).strip()
    
    def run_task(self, task: dict, env) -> TaskResult:
        """Run a single task in the environment.
        
        Args:
            task: Task dictionary from tasks.json
            env: BioAgentGymEnv instance
            
        Returns:
            TaskResult with full trajectory and scores
        """
        task_id = task["id"]
        result = TaskResult(
            task_id=task_id,
            domain=self.config.domain,
            model_name=Path(self.config.model_name_or_path).name,
            start_time=datetime.now().isoformat(),
        )
        
        logger.info(f"Starting task: {task_id}")
        
        # Reset environment
        obs, info = env.reset(options={"task_id": task_id})
        
        # Build conversation
        system_prompt = build_system_prompt(info["policy"], info["tools"])
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": obs},
        ]
        
        try:
            for turn_idx in range(self.config.max_turns):
                turn = TurnRecord(turn_idx=turn_idx)
                
                # Generate
                t0 = time.time()
                raw_output = self.generate(messages)
                turn.latency_seconds = time.time() - t0
                turn.raw_output = raw_output
                
                logger.debug(f"Turn {turn_idx}: {raw_output[:200]}...")
                
                # Parse tool call
                tool_call = parse_tool_call(raw_output)
                
                if tool_call is not None:
                    turn.parsed_tool_call = tool_call
                    
                    # Execute tool via environment
                    action = json.dumps(tool_call)
                    observation, reward, terminated, truncated, step_info = env.step(action)
                    
                    turn.tool_response = observation
                    
                    # Add to messages
                    messages.append({"role": "assistant", "content": raw_output})
                    messages.append({
                        "role": "user",
                        "content": f"Tool result for {tool_call['name']}:\n{observation}"
                    })
                    
                    result.turns.append(turn)
                    
                    if terminated or truncated:
                        break
                else:
                    # Final answer (no tool call)
                    turn.is_final_answer = True
                    messages.append({"role": "assistant", "content": raw_output})
                    result.turns.append(turn)
                    
                    # Mark as terminated - agent gave final answer
                    env.state.terminated = True
                    break
            
            # Get final trajectory and reward
            trajectory = env.get_trajectory()
            result.trajectory = trajectory
            result.total_turns = len(result.turns)
            result.final_reward = trajectory["final_reward"]
            result.action_score = self._compute_action_score(task, env._tool_call_log)
            result.completed = True
            
        except Exception as e:
            logger.error(f"Error in task {task_id}: {e}")
            result.error = str(e)
            import traceback
            result.error += "\n" + traceback.format_exc()
        
        result.end_time = datetime.now().isoformat()
        result.total_latency = sum(t.latency_seconds for t in result.turns)
        
        logger.info(
            f"Task {task_id}: turns={result.total_turns}, "
            f"action_score={result.action_score:.3f}, "
            f"reward={result.final_reward:.3f}, "
            f"latency={result.total_latency:.1f}s"
        )
        
        return result
    
    def _compute_action_score(self, task: dict, tool_call_log: list) -> float:
        """Compute action score based on expected vs actual tool calls."""
        eval_criteria = task.get("evaluation_criteria", {})
        expected_actions = eval_criteria.get("actions", [])
        
        if not expected_actions:
            return 1.0
        
        matched = 0
        for exp in expected_actions:
            exp_name = exp.get("name", "")
            compare_args = exp.get("compare_args", [])
            exp_args = exp.get("arguments", {})
            
            for tc in tool_call_log:
                if tc["tool_name"] == exp_name:
                    if compare_args:
                        all_match = all(
                            str(tc["arguments"].get(k, "")).lower() == str(exp_args.get(k, "")).lower()
                            for k in compare_args
                            if k in exp_args
                        )
                        if all_match:
                            matched += 1
                            break
                    else:
                        matched += 1
                        break
        
        return matched / len(expected_actions)
    
    def run_all_tasks(self) -> list[TaskResult]:
        """Run all tasks (or specified task_ids) and return results."""
        from bioagents.gym.agent_env import BioAgentGymEnv
        
        # Create environment
        gym_env = BioAgentGymEnv(
            domain=self.config.domain,
            task_split=self.config.task_split,
            max_turns=self.config.max_turns,
        )
        
        # Get tasks
        if self.config.task_ids:
            task_ids = self.config.task_ids
        else:
            task_ids = [t["id"] for t in gym_env._tasks]
        
        logger.info(f"Running {len(task_ids)} tasks with {Path(self.config.model_name_or_path).name}")
        
        results = []
        for task_id in task_ids:
            task = gym_env._task_map[task_id]
            result = self.run_task(task, gym_env)
            results.append(result)
            self._save_task_result(result)
        
        # Save summary
        self._save_summary(results)
        
        return results
    
    def _save_task_result(self, result: TaskResult):
        """Save individual task result."""
        path = self.log_path / f"task_{result.task_id}.json"
        data = {
            "task_id": result.task_id,
            "domain": result.domain,
            "model_name": result.model_name,
            "total_turns": result.total_turns,
            "action_score": result.action_score,
            "final_reward": result.final_reward,
            "completed": result.completed,
            "error": result.error,
            "total_latency": result.total_latency,
            "start_time": result.start_time,
            "end_time": result.end_time,
            "turns": [
                {
                    "turn_idx": t.turn_idx,
                    "raw_output": t.raw_output,
                    "parsed_tool_call": t.parsed_tool_call,
                    "tool_response": t.tool_response[:500] if t.tool_response else None,
                    "is_final_answer": t.is_final_answer,
                    "latency_seconds": t.latency_seconds,
                }
                for t in result.turns
            ],
        }
        with open(path, "w", encoding="utf-8") as f:
            json.dump(data, f, indent=2, ensure_ascii=False, default=str)
    
    def _save_summary(self, results: list[TaskResult]):
        """Save a summary of all task results."""
        summary = {
            "run_id": self.run_id,
            "model": Path(self.config.model_name_or_path).name,
            "domain": self.config.domain,
            "backend": self.config.backend,
            "num_tasks": len(results),
            "num_completed": sum(1 for r in results if r.completed),
            "num_errors": sum(1 for r in results if r.error),
            "avg_action_score": sum(r.action_score for r in results) / max(len(results), 1),
            "avg_reward": sum(r.final_reward for r in results) / max(len(results), 1),
            "avg_turns": sum(r.total_turns for r in results) / max(len(results), 1),
            "total_latency": sum(r.total_latency for r in results),
            "per_task": [
                {
                    "task_id": r.task_id,
                    "action_score": r.action_score,
                    "final_reward": r.final_reward,
                    "turns": r.total_turns,
                    "latency": r.total_latency,
                    "completed": r.completed,
                    "error": r.error is not None,
                }
                for r in results
            ],
            "timestamp": datetime.now().isoformat(),
        }
        
        path = self.log_path / "summary.json"
        with open(path, "w", encoding="utf-8") as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)
        
        # Print summary table
        print("\n" + "=" * 80)
        print(f"  RUN SUMMARY: {self.run_id}")
        print("=" * 80)
        print(f"  Model: {summary['model']}")
        print(f"  Domain: {summary['domain']}")
        print(f"  Backend: {summary['backend']}")
        print(f"  Tasks: {summary['num_completed']}/{summary['num_tasks']} completed")
        print(f"  Avg Action Score: {summary['avg_action_score']:.3f}")
        print(f"  Avg Reward: {summary['avg_reward']:.3f}")
        print(f"  Avg Turns: {summary['avg_turns']:.1f}")
        print(f"  Total Latency: {summary['total_latency']:.1f}s")
        print("-" * 80)
        print(f"  {'Task ID':<30} {'Score':>8} {'Reward':>8} {'Turns':>6} {'Time':>8}")
        print("-" * 80)
        for t in summary["per_task"]:
            status = "âœ“" if t["completed"] else "âœ—"
            print(f"  {status} {t['task_id']:<28} {t['action_score']:>8.3f} {t['final_reward']:>8.3f} {t['turns']:>6} {t['latency']:>7.1f}s")
        print("=" * 80)
        
        logger.info(f"Results saved to {self.log_path}")

```

VL ëª¨ë¸ì˜ í…ìŠ¤íŠ¸ ì „ìš© generateë„ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.

```python
"""LLM Agent Runner for BIOAgents.

Connects a language model to the BIOAgents environment for multi-turn
tool-use evaluation. Supports:
- vLLM-based fast inference
- HuggingFace transformers-based inference
- Multi-turn tool calling with automatic parsing
- Full trajectory logging

Usage:
    runner = AgentRunner(
        model_name_or_path="Qwen/Qwen2.5-VL-7B-Instruct",
        backend="vllm",
    )
    results = runner.run_task(domain="clinical_diagnosis", task_id="dx_pneumonia_001")
"""

import json
import os
import re
import time
from copy import deepcopy
from dataclasses import dataclass, field
from datetime import datetime
from pathlib import Path
from typing import Any, Literal, Optional

from loguru import logger


@dataclass
class RunConfig:
    """Configuration for an agent run."""
    model_name_or_path: str
    backend: Literal["vllm", "transformers"] = "transformers"
    domain: str = "clinical_diagnosis"
    task_ids: Optional[list[str]] = None       # None = run all tasks
    task_split: Optional[str] = None
    max_turns: int = 15
    temperature: float = 0.1
    top_p: float = 0.95
    max_new_tokens: int = 1024
    tensor_parallel_size: int = 1
    gpu_memory_utilization: float = 0.85
    log_dir: str = "logs/runs"
    seed: int = 42


@dataclass
class TurnRecord:
    """Record of a single turn in the agent-environment interaction."""
    turn_idx: int
    prompt: str = ""
    raw_output: str = ""
    parsed_tool_call: Optional[dict] = None
    tool_response: Optional[str] = None
    is_final_answer: bool = False
    latency_seconds: float = 0.0


@dataclass 
class TaskResult:
    """Result of running a single task."""
    task_id: str
    domain: str
    model_name: str
    turns: list[TurnRecord] = field(default_factory=list)
    total_turns: int = 0
    action_score: float = 0.0
    final_reward: float = 0.0
    completed: bool = False
    error: Optional[str] = None
    trajectory: dict = field(default_factory=dict)
    start_time: str = ""
    end_time: str = ""
    total_latency: float = 0.0


def build_system_prompt(policy: str, tools: list[dict]) -> str:
    """Build the system prompt with policy and tool definitions."""
    tool_section = json.dumps(tools, indent=2, ensure_ascii=False)
    
    return f"""You are a medical AI assistant operating in a clinical environment. Follow the policy below and use the available tools to help with patient care.

## Policy
{policy}

## Available Tools
You have access to the following tools. To use a tool, respond with ONLY a JSON object in this exact format:
```json
{{"name": "tool_name", "arguments": {{"arg1": "value1", "arg2": "value2"}}}}
```

Do NOT include any other text when making a tool call. One tool call per response.

When you have gathered enough information and want to give your final assessment, respond with your clinical analysis as plain text (no JSON).

## Tool Definitions
{tool_section}"""


def parse_tool_call(text: str) -> Optional[dict]:
    """Parse a tool call from model output.
    
    Supports formats:
    1. Pure JSON: {"name": "...", "arguments": {...}}
    2. JSON in code block: ```json\n{...}\n```
    3. JSON embedded in text with markers
    """
    text = text.strip()
    
    # Try 1: Extract JSON from code blocks
    code_block_match = re.search(r'```(?:json)?\s*\n?({.*?})\s*\n?```', text, re.DOTALL)
    if code_block_match:
        try:
            parsed = json.loads(code_block_match.group(1))
            if "name" in parsed:
                return parsed
        except json.JSONDecodeError:
            pass
    
    # Try 2: Direct JSON parse
    try:
        parsed = json.loads(text)
        if isinstance(parsed, dict) and "name" in parsed:
            return parsed
    except json.JSONDecodeError:
        pass
    
    # Try 3: Find JSON-like pattern in text
    json_match = re.search(r'(\{[^{}]*"name"\s*:\s*"[^"]+?"[^{}]*"arguments"\s*:\s*\{[^{}]*\}[^{}]*\})', text, re.DOTALL)
    if json_match:
        try:
            parsed = json.loads(json_match.group(1))
            if "name" in parsed:
                return parsed
        except json.JSONDecodeError:
            pass
    
    # Try 4: More lenient nested JSON search
    matches = list(re.finditer(r'\{', text))
    for m in matches:
        start = m.start()
        depth = 0
        for i in range(start, len(text)):
            if text[i] == '{':
                depth += 1
            elif text[i] == '}':
                depth -= 1
                if depth == 0:
                    candidate = text[start:i+1]
                    try:
                        parsed = json.loads(candidate)
                        if isinstance(parsed, dict) and "name" in parsed:
                            return parsed
                    except json.JSONDecodeError:
                        pass
                    break
    
    return None


class AgentRunner:
    """Runs LLM agents in BIOAgents environments."""
    
    def __init__(self, config: RunConfig):
        self.config = config
        self.model = None
        self.tokenizer = None
        self._setup_logging()
    
    def _setup_logging(self):
        """Set up logging directory."""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        model_short = Path(self.config.model_name_or_path).name
        self.run_id = f"{model_short}_{self.config.domain}_{timestamp}"
        self.log_path = Path(self.config.log_dir) / self.run_id
        self.log_path.mkdir(parents=True, exist_ok=True)
        
        # Save config
        config_dict = {
            k: v for k, v in self.config.__dict__.items()
        }
        with open(self.log_path / "config.json", "w") as f:
            json.dump(config_dict, f, indent=2, default=str)
    
    def load_model(self):
        """Load the language model."""
        if self.config.backend == "vllm":
            self._load_vllm()
        else:
            self._load_transformers()
    
    def _load_vllm(self):
        """Load model with vLLM."""
        from vllm import LLM, SamplingParams
        
        logger.info(f"Loading {self.config.model_name_or_path} with vLLM (tp={self.config.tensor_parallel_size})")
        self.model = LLM(
            model=self.config.model_name_or_path,
            tensor_parallel_size=self.config.tensor_parallel_size,
            gpu_memory_utilization=self.config.gpu_memory_utilization,
            trust_remote_code=True,
            max_model_len=8192,
            seed=self.config.seed,
        )
        self.sampling_params = SamplingParams(
            temperature=self.config.temperature,
            top_p=self.config.top_p,
            max_tokens=self.config.max_new_tokens,
            stop=["```\n\n", "\n\nUser:", "\n\nHuman:"],
        )
        logger.info("vLLM model loaded successfully")
    
    def _load_transformers(self):
        """Load model with HuggingFace transformers."""
        import torch
        from transformers import (
            AutoModelForCausalLM,
            AutoTokenizer,
            AutoProcessor,
            AutoConfig,
        )
        
        logger.info(f"Loading {self.config.model_name_or_path} with transformers")
        
        model_path = self.config.model_name_or_path
        
        # Detect model type from config
        config = AutoConfig.from_pretrained(model_path, trust_remote_code=True)
        model_type = getattr(config, "model_type", "")
        architectures = getattr(config, "architectures", [])
        
        self._is_vl_model = any(
            "vl" in a.lower() or "vision" in a.lower()
            for a in (architectures or [])
        ) or "vl" in model_type.lower()
        
        logger.info(f"Model type: {model_type}, VL: {self._is_vl_model}, Arch: {architectures}")
        
        # Load tokenizer / processor
        if self._is_vl_model:
            try:
                self.processor = AutoProcessor.from_pretrained(
                    model_path, trust_remote_code=True
                )
                self.tokenizer = (
                    self.processor.tokenizer
                    if hasattr(self.processor, "tokenizer")
                    else self.processor
                )
            except Exception:
                self.tokenizer = AutoTokenizer.from_pretrained(
                    model_path, trust_remote_code=True
                )
                self.processor = None
        else:
            self.tokenizer = AutoTokenizer.from_pretrained(
                model_path, trust_remote_code=True
            )
            self.processor = None
        
        # Load model â€“ choose the right Auto class
        load_kwargs = dict(
            torch_dtype=torch.bfloat16,
            device_map="auto",
            trust_remote_code=True,
        )
        
        # Try sdpa first, then fall back
        for attn_impl in ["sdpa", None]:
            try:
                kw = {**load_kwargs}
                if attn_impl:
                    kw["attn_implementation"] = attn_impl
                
                if self._is_vl_model:
                    # VL models need their specific class
                    from transformers import Qwen2_5_VLForConditionalGeneration
                    self.model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
                        model_path, **kw
                    )
                else:
                    self.model = AutoModelForCausalLM.from_pretrained(
                        model_path, **kw
                    )
                break
            except Exception as e:
                logger.warning(f"Failed with attn_impl={attn_impl}: {e}")
                if attn_impl is None:
                    raise
        
        self.model.eval()
        logger.info(f"Model loaded (VL={self._is_vl_model})")
    
    def generate(self, messages: list[dict]) -> str:
        """Generate a response from the model."""
        if self.config.backend == "vllm":
            return self._generate_vllm(messages)
        else:
            return self._generate_transformers(messages)
    
    def _generate_vllm(self, messages: list[dict]) -> str:
        """Generate with vLLM using chat template."""
        from vllm import SamplingParams
        
        # Use the tokenizer from vLLM engine
        tokenizer = self.model.get_tokenizer()
        prompt = tokenizer.apply_chat_template(
            messages, tokenize=False, add_generation_prompt=True
        )
        
        outputs = self.model.generate([prompt], self.sampling_params)
        return outputs[0].outputs[0].text.strip()
    
    def _generate_transformers(self, messages: list[dict]) -> str:
        """Generate with HuggingFace transformers."""
        import torch
        
        # For VL models, use processor if available
        if self._is_vl_model and self.processor is not None:
            # Text-only input for VL model
            text = self.processor.apply_chat_template(
                messages, tokenize=False, add_generation_prompt=True
            )
            inputs = self.processor(
                text=[text], padding=True, return_tensors="pt"
            ).to(self.model.device)
        else:
            # Apply chat template
            if hasattr(self.tokenizer, "apply_chat_template"):
                text = self.tokenizer.apply_chat_template(
                    messages, tokenize=False, add_generation_prompt=True
                )
            else:
                text = ""
                for msg in messages:
                    role = msg["role"]
                    content = msg["content"]
                    text += f"<|{role}|>\n{content}\n"
                text += "<|assistant|>\n"
            
            inputs = self.tokenizer(text, return_tensors="pt").to(self.model.device)
        
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=self.config.max_new_tokens,
                temperature=self.config.temperature if self.config.temperature > 0 else None,
                top_p=self.config.top_p if self.config.temperature > 0 else None,
                do_sample=self.config.temperature > 0,
                pad_token_id=self.tokenizer.eos_token_id,
            )
        
        # Decode only the generated tokens
        input_len = inputs["input_ids"].shape[-1]
        generated_ids = outputs[0][input_len:]
        return self.tokenizer.decode(generated_ids, skip_special_tokens=True).strip()
    
    def run_task(self, task: dict, env) -> TaskResult:
        """Run a single task in the environment.
        
        Args:
            task: Task dictionary from tasks.json
            env: BioAgentGymEnv instance
            
        Returns:
            TaskResult with full trajectory and scores
        """
        task_id = task["id"]
        result = TaskResult(
            task_id=task_id,
            domain=self.config.domain,
            model_name=Path(self.config.model_name_or_path).name,
            start_time=datetime.now().isoformat(),
        )
        
        logger.info(f"Starting task: {task_id}")
        
        # Reset environment
        obs, info = env.reset(options={"task_id": task_id})
        
        # Build conversation
        system_prompt = build_system_prompt(info["policy"], info["tools"])
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": obs},
        ]
        
        try:
            for turn_idx in range(self.config.max_turns):
                turn = TurnRecord(turn_idx=turn_idx)
                
                # Generate
                t0 = time.time()
                raw_output = self.generate(messages)
                turn.latency_seconds = time.time() - t0
                turn.raw_output = raw_output
                
                logger.debug(f"Turn {turn_idx}: {raw_output[:200]}...")
                
                # Parse tool call
                tool_call = parse_tool_call(raw_output)
                
                if tool_call is not None:
                    turn.parsed_tool_call = tool_call
                    
                    # Execute tool via environment
                    action = json.dumps(tool_call)
                    observation, reward, terminated, truncated, step_info = env.step(action)
                    
                    turn.tool_response = observation
                    
                    # Add to messages
                    messages.append({"role": "assistant", "content": raw_output})
                    messages.append({
                        "role": "user",
                        "content": f"Tool result for {tool_call['name']}:\n{observation}"
                    })
                    
                    result.turns.append(turn)
                    
                    if terminated or truncated:
                        break
                else:
                    # Final answer (no tool call)
                    turn.is_final_answer = True
                    messages.append({"role": "assistant", "content": raw_output})
                    result.turns.append(turn)
                    
                    # Mark as terminated - agent gave final answer
                    env.state.terminated = True
                    break
            
            # Get final trajectory and reward
            trajectory = env.get_trajectory()
            result.trajectory = trajectory
            result.total_turns = len(result.turns)
            result.final_reward = trajectory["final_reward"]
            result.action_score = self._compute_action_score(task, env._tool_call_log)
            result.completed = True
            
        except Exception as e:
            logger.error(f"Error in task {task_id}: {e}")
            result.error = str(e)
            import traceback
            result.error += "\n" + traceback.format_exc()
        
        result.end_time = datetime.now().isoformat()
        result.total_latency = sum(t.latency_seconds for t in result.turns)
        
        logger.info(
            f"Task {task_id}: turns={result.total_turns}, "
            f"action_score={result.action_score:.3f}, "
            f"reward={result.final_reward:.3f}, "
            f"latency={result.total_latency:.1f}s"
        )
        
        return result
    
    def _compute_action_score(self, task: dict, tool_call_log: list) -> float:
        """Compute action score based on expected vs actual tool calls."""
        eval_criteria = task.get("evaluation_criteria", {})
        expected_actions = eval_criteria.get("actions", [])
        
        if not expected_actions:
            return 1.0
        
        matched = 0
        for exp in expected_actions:
            exp_name = exp.get("name", "")
            compare_args = exp.get("compare_args", [])
            exp_args = exp.get("arguments", {})
            
            for tc in tool_call_log:
                if tc["tool_name"] == exp_name:
                    if compare_args:
                        all_match = all(
                            str(tc["arguments"].get(k, "")).lower() == str(exp_args.get(k, "")).lower()
                            for k in compare_args
                            if k in exp_args
                        )
                        if all_match:
                            matched += 1
                            break
                    else:
                        matched += 1
                        break
        
        return matched / len(expected_actions)
    
    def run_all_tasks(self) -> list[TaskResult]:
        """Run all tasks (or specified task_ids) and return results."""
        from bioagents.gym.agent_env import BioAgentGymEnv
        
        # Create environment
        gym_env = BioAgentGymEnv(
            domain=self.config.domain,
            task_split=self.config.task_split,
            max_turns=self.config.max_turns,
        )
        
        # Get tasks
        if self.config.task_ids:
            task_ids = self.config.task_ids
        else:
            task_ids = [t["id"] for t in gym_env._tasks]
        
        logger.info(f"Running {len(task_ids)} tasks with {Path(self.config.model_name_or_path).name}")
        
        results = []
        for task_id in task_ids:
            task = gym_env._task_map[task_id]
            result = self.run_task(task, gym_env)
            results.append(result)
            self._save_task_result(result)
        
        # Save summary
        self._save_summary(results)
        
        return results
    
    def _save_task_result(self, result: TaskResult):
        """Save individual task result."""
        path = self.log_path / f"task_{result.task_id}.json"
        data = {
            "task_id": result.task_id,
            "domain": result.domain,
            "model_name": result.model_name,
            "total_turns": result.total_turns,
            "action_score": result.action_score,
            "final_reward": result.final_reward,
            "completed": result.completed,
            "error": result.error,
            "total_latency": result.total_latency,
            "start_time": result.start_time,
            "end_time": result.end_time,
            "turns": [
                {
                    "turn_idx": t.turn_idx,
                    "raw_output": t.raw_output,
                    "parsed_tool_call": t.parsed_tool_call,
                    "tool_response": t.tool_response[:500] if t.tool_response else None,
                    "is_final_answer": t.is_final_answer,
                    "latency_seconds": t.latency_seconds,
                }
                for t in result.turns
            ],
        }
        with open(path, "w", encoding="utf-8") as f:
            json.dump(data, f, indent=2, ensure_ascii=False, default=str)
    
    def _save_summary(self, results: list[TaskResult]):
        """Save a summary of all task results."""
        summary = {
            "run_id": self.run_id,
            "model": Path(self.config.model_name_or_path).name,
            "domain": self.config.domain,
            "backend": self.config.backend,
            "num_tasks": len(results),
            "num_completed": sum(1 for r in results if r.completed),
            "num_errors": sum(1 for r in results if r.error),
            "avg_action_score": sum(r.action_score for r in results) / max(len(results), 1),
            "avg_reward": sum(r.final_reward for r in results) / max(len(results), 1),
            "avg_turns": sum(r.total_turns for r in results) / max(len(results), 1),
            "total_latency": sum(r.total_latency for r in results),
            "per_task": [
                {
                    "task_id": r.task_id,
                    "action_score": r.action_score,
                    "final_reward": r.final_reward,
                    "turns": r.total_turns,
                    "latency": r.total_latency,
                    "completed": r.completed,
                    "error": r.error is not None,
                }
                for r in results
            ],
            "timestamp": datetime.now().isoformat(),
        }
        
        path = self.log_path / "summary.json"
        with open(path, "w", encoding="utf-8") as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)
        
        # Print summary table
        print("\n" + "=" * 80)
        print(f"  RUN SUMMARY: {self.run_id}")
        print("=" * 80)
        print(f"  Model: {summary['model']}")
        print(f"  Domain: {summary['domain']}")
        print(f"  Backend: {summary['backend']}")
        print(f"  Tasks: {summary['num_completed']}/{summary['num_tasks']} completed")
        print(f"  Avg Action Score: {summary['avg_action_score']:.3f}")
        print(f"  Avg Reward: {summary['avg_reward']:.3f}")
        print(f"  Avg Turns: {summary['avg_turns']:.1f}")
        print(f"  Total Latency: {summary['total_latency']:.1f}s")
        print("-" * 80)
        print(f"  {'Task ID':<30} {'Score':>8} {'Reward':>8} {'Turns':>6} {'Time':>8}")
        print("-" * 80)
        for t in summary["per_task"]:
            status = "âœ“" if t["completed"] else "âœ—"
            print(f"  {status} {t['task_id']:<28} {t['action_score']:>8.3f} {t['final_reward']:>8.3f} {t['turns']:>6} {t['latency']:>7.1f}s")
        print("=" * 80)
        
        logger.info(f"Results saved to {self.log_path}")

```

ì´ì œ 3ê°œ ëª¨ë¸ì„ ëª¨ë‘ ëŒë¦¬ëŠ” í†µí•© ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‘ì„±í•©ë‹ˆë‹¤.

```python
#!/usr/bin/env python3
"""Run all candidate models on clinical_diagnosis tasks.

Models:
  1. Lingshu-7B (medical specialized VL model) â†’ GPU 0
  2. Qwen2.5-VL-7B-Instruct (general VL model) â†’ GPU 1
  3. Qwen3-8B-Base (text-only baseline) â†’ GPU 2

Each runs sequentially to avoid GPU OOM. Results are saved to logs/runs/.
"""

import json
import os
import sys
import time
from datetime import datetime
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent.parent))

from bioagents.evaluation.agent_runner import AgentRunner, RunConfig
from loguru import logger

PROJECT_ROOT = Path(__file__).parent.parent

MODELS = [
    {
        "name": "Lingshu-7B",
        "path": str(PROJECT_ROOT / "checkpoints/models/Lingshu-7B"),
        "backend": "transformers",
        "gpu": "0,1",
        "notes": "Medical specialized VL model (Qwen2.5-VL based)",
    },
    {
        "name": "Qwen2.5-VL-7B-Instruct",
        "path": str(PROJECT_ROOT / "checkpoints/models/Qwen2.5-VL-7B-Instruct"),
        "backend": "transformers",
        "gpu": "2,3",
        "notes": "General purpose VL model",
    },
    {
        "name": "Qwen3-8B-Base",
        "path": "/data/project/private/minstar/models/Qwen3-8B-Base",
        "backend": "transformers",
        "gpu": "4,5",
        "notes": "Text-only base model (no instruction tuning)",
    },
]

ALL_TASK_IDS = [
    "dx_pneumonia_001",
    "dx_meningitis_001",
    "dx_appendicitis_001",
    "dx_drug_interaction_001",
    "dx_critical_vitals_001",
]


def run_model(model_info: dict, task_ids: list[str] = None):
    """Run a single model on all tasks."""
    name = model_info["name"]
    path = model_info["path"]
    
    # Check model exists
    if not Path(path).exists():
        logger.error(f"Model {name} not found at {path}")
        return None
    
    # Set GPU
    os.environ["CUDA_VISIBLE_DEVICES"] = model_info["gpu"]
    
    config = RunConfig(
        model_name_or_path=path,
        backend=model_info["backend"],
        domain="clinical_diagnosis",
        task_ids=task_ids or ALL_TASK_IDS,
        max_turns=15,
        temperature=0.1,
        top_p=0.95,
        max_new_tokens=1024,
        log_dir=str(PROJECT_ROOT / "logs" / "runs"),
    )
    
    print(f"\n{'='*70}")
    print(f"  Model: {name}")
    print(f"  Path: {path}")
    print(f"  GPU: {model_info['gpu']}")
    print(f"  Notes: {model_info['notes']}")
    print(f"  Tasks: {len(config.task_ids)}")
    print(f"  Time: {datetime.now().isoformat()}")
    print(f"{'='*70}\n")
    
    runner = AgentRunner(config)
    
    logger.info(f"Loading model: {name}")
    t0 = time.time()
    runner.load_model()
    load_time = time.time() - t0
    logger.info(f"Model loaded in {load_time:.1f}s")
    
    results = runner.run_all_tasks()
    
    # Clean up GPU memory
    import torch
    del runner.model
    if hasattr(runner, 'processor') and runner.processor is not None:
        del runner.processor
    if hasattr(runner, 'tokenizer'):
        del runner.tokenizer
    torch.cuda.empty_cache()
    import gc
    gc.collect()
    
    return {
        "name": name,
        "results": results,
        "load_time": load_time,
        "run_id": runner.run_id,
    }


def print_final_comparison(all_runs: list[dict]):
    """Print comparison table of all models."""
    print("\n\n" + "=" * 100)
    print("  FINAL COMPARISON - BIOAgents Clinical Diagnosis Baseline")
    print(f"  Date: {datetime.now().strftime('%Y-%m-%d %H:%M')}")
    print("=" * 100)
    
    # Header
    header = f"{'Model':<28}"
    for tid in ALL_TASK_IDS:
        short = tid.replace("dx_", "").replace("_001", "")
        header += f" {short:>12}"
    header += f" {'AVG':>8} {'Turns':>6}"
    print(header)
    print("-" * 100)
    
    comparison_data = []
    
    for run_info in all_runs:
        if run_info is None:
            continue
        name = run_info["name"]
        results = run_info["results"]
        
        row = f"{name:<28}"
        scores = []
        total_turns = []
        
        # Map results by task_id
        result_map = {r.task_id: r for r in results}
        
        for tid in ALL_TASK_IDS:
            r = result_map.get(tid)
            if r and r.completed:
                row += f" {r.action_score:>12.3f}"
                scores.append(r.action_score)
                total_turns.append(r.total_turns)
            else:
                row += f" {'ERR':>12}"
        
        avg_score = sum(scores) / max(len(scores), 1)
        avg_turns = sum(total_turns) / max(len(total_turns), 1)
        row += f" {avg_score:>8.3f} {avg_turns:>6.1f}"
        print(row)
        
        comparison_data.append({
            "model": name,
            "avg_action_score": avg_score,
            "avg_turns": avg_turns,
            "per_task": {
                r.task_id: {
                    "action_score": r.action_score,
                    "final_reward": r.final_reward,
                    "turns": r.total_turns,
                    "completed": r.completed,
                }
                for r in results
            },
            "load_time": run_info["load_time"],
            "run_id": run_info["run_id"],
        })
    
    print("=" * 100)
    
    # Save comparison
    comp_path = PROJECT_ROOT / "logs" / "runs" / f"comparison_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(comp_path, "w") as f:
        json.dump(comparison_data, f, indent=2, default=str)
    print(f"\nComparison saved to: {comp_path}")


def main():
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument("--models", nargs="+", default=None,
                        help="Model names to run (default: all)")
    parser.add_argument("--tasks", nargs="+", default=None,
                        help="Task IDs to run (default: all)")
    args = parser.parse_args()
    
    models_to_run = MODELS
    if args.models:
        models_to_run = [m for m in MODELS if m["name"] in args.models]
    
    task_ids = args.tasks
    
    print("=" * 70)
    print("  BIOAgents - Multi-Model Clinical Diagnosis Experiment")
    print(f"  Models: {[m['name'] for m in models_to_run]}")
    print(f"  Tasks: {task_ids or 'ALL'}")
    print(f"  Start: {datetime.now().isoformat()}")
    print("=" * 70)
    
    all_runs = []
    for model_info in models_to_run:
        try:
            result = run_model(model_info, task_ids)
            all_runs.append(result)
        except Exception as e:
            logger.error(f"Failed for {model_info['name']}: {e}")
            import traceback
            traceback.print_exc()
            all_runs.append(None)
    
    print_final_comparison(all_runs)
    
    print(f"\nAll experiments completed at {datetime.now().isoformat()}")


if __name__ == "__main__":
    main()

```

Qwen3 baselineì´ ì–´ë–»ê²Œ ë˜ê³  ìˆëŠ”ì§€ í™•ì¸í•˜ê³ , Lingshu-7B ë¨¼ì € ì‹¤í–‰í•©ë‹ˆë‹¤.

ì•„ì§ ì¶œë ¥ì´ ì•ˆ ë‚˜ì˜¨ ìƒíƒœ. ëª¨ë¸ ë¡œë”© ì¤‘ì¼ ê²ë‹ˆë‹¤. Lingshu-7Bë¥¼ ë¨¼ì € ë‹¤ë¥¸ GPUì—ì„œ ì‹¤í–‰í•©ì‹œë‹¤.

3ê°œ ëª¨ë¸ì´ ê°ê° ë‹¤ë¥¸ GPUì—ì„œ ë™ì‹œ ì‹¤í–‰ ì¤‘! ì§„í–‰ ìƒíƒœë¥¼ ëª¨ë‹ˆí„°ë§í•©ë‹ˆë‹¤.

GPU 0ì—ì„œ Qwen3(36GB), GPU 2-3ì—ì„œ Lingshu ë¡œë”© ì¤‘. Qwen2.5-VLì€ ì•„ì§ ë¡œë”© ì‹œì‘ ì „. ë¡œê·¸ íŒŒì¼ì„ ì§ì ‘ í™•ì¸í•©ë‹ˆë‹¤.

Qwen3ëŠ” ë¡œë”©ë¼ì„œ ì‹¤í–‰ ì¤‘(GPU 0, 37GB), ë‚˜ë¨¸ì§€ëŠ” ëë‚¬ê±°ë‚˜ ì—ëŸ¬ê°€ ë‚¬ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë¡œê·¸ íŒŒì¼ì„ í™•ì¸í•©ë‹ˆë‹¤.

ë‘ ê°€ì§€ ë¬¸ì œë¥¼ ë°œê²¬í–ˆìŠµë‹ˆë‹¤:
1. `BioAgentGymEnv`ì— `.state` ì†ì„±ì´ ì—†ìŒ â†’ runnerì—ì„œ ì§ì ‘ ì ‘ê·¼í•˜ëŠ” ë¶€ë¶„ ìˆ˜ì •
2. Lingshuê°€ tool call í›„ ë°˜ë³µ í…ìŠ¤íŠ¸ ìƒì„± â†’ ë°˜ë³µ ê°ì§€ + ì¤‘ë‹¨ ë¡œì§ í•„ìš”

ë°”ë¡œ ìˆ˜ì •í•©ë‹ˆë‹¤.

```python
"""LLM Agent Runner for BIOAgents.

Connects a language model to the BIOAgents environment for multi-turn
tool-use evaluation. Supports:
- vLLM-based fast inference
- HuggingFace transformers-based inference
- Multi-turn tool calling with automatic parsing
- Full trajectory logging

Usage:
    runner = AgentRunner(
        model_name_or_path="Qwen/Qwen2.5-VL-7B-Instruct",
        backend="vllm",
    )
    results = runner.run_task(domain="clinical_diagnosis", task_id="dx_pneumonia_001")
"""

import json
import os
import re
import time
from copy import deepcopy
from dataclasses import dataclass, field
from datetime import datetime
from pathlib import Path
from typing import Any, Literal, Optional

from loguru import logger


@dataclass
class RunConfig:
    """Configuration for an agent run."""
    model_name_or_path: str
    backend: Literal["vllm", "transformers"] = "transformers"
    domain: str = "clinical_diagnosis"
    task_ids: Optional[list[str]] = None       # None = run all tasks
    task_split: Optional[str] = None
    max_turns: int = 15
    temperature: float = 0.1
    top_p: float = 0.95
    max_new_tokens: int = 1024
    tensor_parallel_size: int = 1
    gpu_memory_utilization: float = 0.85
    log_dir: str = "logs/runs"
    seed: int = 42


@dataclass
class TurnRecord:
    """Record of a single turn in the agent-environment interaction."""
    turn_idx: int
    prompt: str = ""
    raw_output: str = ""
    parsed_tool_call: Optional[dict] = None
    tool_response: Optional[str] = None
    is_final_answer: bool = False
    latency_seconds: float = 0.0


@dataclass 
class TaskResult:
    """Result of running a single task."""
    task_id: str
    domain: str
    model_name: str
    turns: list[TurnRecord] = field(default_factory=list)
    total_turns: int = 0
    action_score: float = 0.0
    final_reward: float = 0.0
    completed: bool = False
    error: Optional[str] = None
    trajectory: dict = field(default_factory=dict)
    start_time: str = ""
    end_time: str = ""
    total_latency: float = 0.0


def build_system_prompt(policy: str, tools: list[dict]) -> str:
    """Build the system prompt with policy and tool definitions."""
    tool_section = json.dumps(tools, indent=2, ensure_ascii=False)
    
    return f"""You are a medical AI assistant operating in a clinical environment. Follow the policy below and use the available tools to help with patient care.

## Policy
{policy}

## Available Tools
You have access to the following tools. To use a tool, respond with ONLY a JSON object in this exact format:
```json
{{"name": "tool_name", "arguments": {{"arg1": "value1", "arg2": "value2"}}}}
```

Do NOT include any other text when making a tool call. One tool call per response.

When you have gathered enough information and want to give your final assessment, respond with your clinical analysis as plain text (no JSON).

## Tool Definitions
{tool_section}"""


def parse_tool_call(text: str) -> Optional[dict]:
    """Parse a tool call from model output.
    
    Supports formats:
    1. Pure JSON: {"name": "...", "arguments": {...}}
    2. JSON in code block: ```json\n{...}\n```
    3. JSON embedded in text with markers
    """
    text = text.strip()
    
    # Try 1: Extract JSON from code blocks
    code_block_match = re.search(r'```(?:json)?\s*\n?({.*?})\s*\n?```', text, re.DOTALL)
    if code_block_match:
        try:
            parsed = json.loads(code_block_match.group(1))
            if "name" in parsed:
                return parsed
        except json.JSONDecodeError:
            pass
    
    # Try 2: Direct JSON parse
    try:
        parsed = json.loads(text)
        if isinstance(parsed, dict) and "name" in parsed:
            return parsed
    except json.JSONDecodeError:
        pass
    
    # Try 3: Find JSON-like pattern in text
    json_match = re.search(r'(\{[^{}]*"name"\s*:\s*"[^"]+?"[^{}]*"arguments"\s*:\s*\{[^{}]*\}[^{}]*\})', text, re.DOTALL)
    if json_match:
        try:
            parsed = json.loads(json_match.group(1))
            if "name" in parsed:
                return parsed
        except json.JSONDecodeError:
            pass
    
    # Try 4: More lenient nested JSON search
    matches = list(re.finditer(r'\{', text))
    for m in matches:
        start = m.start()
        depth = 0
        for i in range(start, len(text)):
            if text[i] == '{':
                depth += 1
            elif text[i] == '}':
                depth -= 1
                if depth == 0:
                    candidate = text[start:i+1]
                    try:
                        parsed = json.loads(candidate)
                        if isinstance(parsed, dict) and "name" in parsed:
                            return parsed
                    except json.JSONDecodeError:
                        pass
                    break
    
    return None


class AgentRunner:
    """Runs LLM agents in BIOAgents environments."""
    
    def __init__(self, config: RunConfig):
        self.config = config
        self.model = None
        self.tokenizer = None
        self._setup_logging()
    
    def _setup_logging(self):
        """Set up logging directory."""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        model_short = Path(self.config.model_name_or_path).name
        self.run_id = f"{model_short}_{self.config.domain}_{timestamp}"
        self.log_path = Path(self.config.log_dir) / self.run_id
        self.log_path.mkdir(parents=True, exist_ok=True)
        
        # Save config
        config_dict = {
            k: v for k, v in self.config.__dict__.items()
        }
        with open(self.log_path / "config.json", "w") as f:
            json.dump(config_dict, f, indent=2, default=str)
    
    def load_model(self):
        """Load the language model."""
        if self.config.backend == "vllm":
            self._load_vllm()
        else:
            self._load_transformers()
    
    def _load_vllm(self):
        """Load model with vLLM."""
        from vllm import LLM, SamplingParams
        
        logger.info(f"Loading {self.config.model_name_or_path} with vLLM (tp={self.config.tensor_parallel_size})")
        self.model = LLM(
            model=self.config.model_name_or_path,
            tensor_parallel_size=self.config.tensor_parallel_size,
            gpu_memory_utilization=self.config.gpu_memory_utilization,
            trust_remote_code=True,
            max_model_len=8192,
            seed=self.config.seed,
        )
        self.sampling_params = SamplingParams(
            temperature=self.config.temperature,
            top_p=self.config.top_p,
            max_tokens=self.config.max_new_tokens,
            stop=["```\n\n", "\n\nUser:", "\n\nHuman:"],
        )
        logger.info("vLLM model loaded successfully")
    
    def _load_transformers(self):
        """Load model with HuggingFace transformers."""
        import torch
        from transformers import (
            AutoModelForCausalLM,
            AutoTokenizer,
            AutoProcessor,
            AutoConfig,
        )
        
        logger.info(f"Loading {self.config.model_name_or_path} with transformers")
        
        model_path = self.config.model_name_or_path
        
        # Detect model type from config
        config = AutoConfig.from_pretrained(model_path, trust_remote_code=True)
        model_type = getattr(config, "model_type", "")
        architectures = getattr(config, "architectures", [])
        
        self._is_vl_model = any(
            "vl" in a.lower() or "vision" in a.lower()
            for a in (architectures or [])
        ) or "vl" in model_type.lower()
        
        logger.info(f"Model type: {model_type}, VL: {self._is_vl_model}, Arch: {architectures}")
        
        # Load tokenizer / processor
        if self._is_vl_model:
            try:
                self.processor = AutoProcessor.from_pretrained(
                    model_path, trust_remote_code=True
                )
                self.tokenizer = (
                    self.processor.tokenizer
                    if hasattr(self.processor, "tokenizer")
                    else self.processor
                )
            except Exception:
                self.tokenizer = AutoTokenizer.from_pretrained(
                    model_path, trust_remote_code=True
                )
                self.processor = None
        else:
            self.tokenizer = AutoTokenizer.from_pretrained(
                model_path, trust_remote_code=True
            )
            self.processor = None
        
        # Load model â€“ choose the right Auto class
        load_kwargs = dict(
            torch_dtype=torch.bfloat16,
            device_map="auto",
            trust_remote_code=True,
        )
        
        # Try sdpa first, then fall back
        for attn_impl in ["sdpa", None]:
            try:
                kw = {**load_kwargs}
                if attn_impl:
                    kw["attn_implementation"] = attn_impl
                
                if self._is_vl_model:
                    # VL models need their specific class
                    from transformers import Qwen2_5_VLForConditionalGeneration
                    self.model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
                        model_path, **kw
                    )
                else:
                    self.model = AutoModelForCausalLM.from_pretrained(
                        model_path, **kw
                    )
                break
            except Exception as e:
                logger.warning(f"Failed with attn_impl={attn_impl}: {e}")
                if attn_impl is None:
                    raise
        
        self.model.eval()
        logger.info(f"Model loaded (VL={self._is_vl_model})")
    
    def generate(self, messages: list[dict]) -> str:
        """Generate a response from the model."""
        if self.config.backend == "vllm":
            return self._generate_vllm(messages)
        else:
            return self._generate_transformers(messages)
    
    def _generate_vllm(self, messages: list[dict]) -> str:
        """Generate with vLLM using chat template."""
        from vllm import SamplingParams
        
        # Use the tokenizer from vLLM engine
        tokenizer = self.model.get_tokenizer()
        prompt = tokenizer.apply_chat_template(
            messages, tokenize=False, add_generation_prompt=True
        )
        
        outputs = self.model.generate([prompt], self.sampling_params)
        return outputs[0].outputs[0].text.strip()
    
    def _generate_transformers(self, messages: list[dict]) -> str:
        """Generate with HuggingFace transformers."""
        import torch
        
        # For VL models, use processor if available
        if self._is_vl_model and self.processor is not None:
            # Text-only input for VL model
            text = self.processor.apply_chat_template(
                messages, tokenize=False, add_generation_prompt=True
            )
            inputs = self.processor(
                text=[text], padding=True, return_tensors="pt"
            ).to(self.model.device)
        else:
            # Apply chat template
            if hasattr(self.tokenizer, "apply_chat_template"):
                text = self.tokenizer.apply_chat_template(
                    messages, tokenize=False, add_generation_prompt=True
                )
            else:
                text = ""
                for msg in messages:
                    role = msg["role"]
                    content = msg["content"]
                    text += f"<|{role}|>\n{content}\n"
                text += "<|assistant|>\n"
            
            inputs = self.tokenizer(text, return_tensors="pt").to(self.model.device)
        
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=self.config.max_new_tokens,
                temperature=self.config.temperature if self.config.temperature > 0 else None,
                top_p=self.config.top_p if self.config.temperature > 0 else None,
                do_sample=self.config.temperature > 0,
                pad_token_id=self.tokenizer.eos_token_id,
            )
        
        # Decode only the generated tokens
        input_len = inputs["input_ids"].shape[-1]
        generated_ids = outputs[0][input_len:]
        return self.tokenizer.decode(generated_ids, skip_special_tokens=True).strip()
    
    def run_task(self, task: dict, env) -> TaskResult:
        """Run a single task in the environment.
        
        Args:
            task: Task dictionary from tasks.json
            env: BioAgentGymEnv instance
            
        Returns:
            TaskResult with full trajectory and scores
        """
        task_id = task["id"]
        result = TaskResult(
            task_id=task_id,
            domain=self.config.domain,
            model_name=Path(self.config.model_name_or_path).name,
            start_time=datetime.now().isoformat(),
        )
        
        logger.info(f"Starting task: {task_id}")
        
        # Reset environment
        obs, info = env.reset(options={"task_id": task_id})
        
        # Build conversation
        system_prompt = build_system_prompt(info["policy"], info["tools"])
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": obs},
        ]
        
        try:
            for turn_idx in range(self.config.max_turns):
                turn = TurnRecord(turn_idx=turn_idx)
                
                # Generate
                t0 = time.time()
                raw_output = self.generate(messages)
                turn.latency_seconds = time.time() - t0
                turn.raw_output = raw_output
                
                logger.debug(f"Turn {turn_idx}: {raw_output[:200]}...")
                
                # Parse tool call
                tool_call = parse_tool_call(raw_output)
                
                if tool_call is not None:
                    turn.parsed_tool_call = tool_call
                    
                    # Execute tool via environment
                    action = json.dumps(tool_call)
                    observation, reward, terminated, truncated, step_info = env.step(action)
                    
                    turn.tool_response = observation
                    
                    # Add to messages
                    messages.append({"role": "assistant", "content": raw_output})
                    messages.append({
                        "role": "user",
                        "content": f"Tool result for {tool_call['name']}:\n{observation}"
                    })
                    
                    result.turns.append(turn)
                    
                    if terminated or truncated:
                        break
                else:
                    # Final answer (no tool call)
                    turn.is_final_answer = True
                    messages.append({"role": "assistant", "content": raw_output})
                    result.turns.append(turn)
                    
                    # Check for repetition (model stuck in a loop)
                    if len(result.turns) >= 3:
                        recent = [t.raw_output[:100] for t in result.turns[-3:]]
                        if len(set(recent)) == 1:
                            logger.warning(f"Repetition detected at turn {turn_idx}, stopping.")
                            break
                    
                    # Mark as terminated - agent gave final answer
                    env._env.state.terminated = True if hasattr(env, '_env') else None
                    break
            
            # Get final trajectory and reward
            trajectory = env.get_trajectory()
            result.trajectory = trajectory
            result.total_turns = len(result.turns)
            result.final_reward = trajectory["final_reward"]
            result.action_score = self._compute_action_score(task, env._tool_call_log)
            result.completed = True
            
        except Exception as e:
            logger.error(f"Error in task {task_id}: {e}")
            result.error = str(e)
            import traceback
            result.error += "\n" + traceback.format_exc()
        
        result.end_time = datetime.now().isoformat()
        result.total_latency = sum(t.latency_seconds for t in result.turns)
        
        logger.info(
            f"Task {task_id}: turns={result.total_turns}, "
            f"action_score={result.action_score:.3f}, "
            f"reward={result.final_reward:.3f}, "
            f"latency={result.total_latency:.1f}s"
        )
        
        return result
    
    def _compute_action_score(self, task: dict, tool_call_log: list) -> float:
        """Compute action score based on expected vs actual tool calls."""
        eval_criteria = task.get("evaluation_criteria", {})
        expected_actions = eval_criteria.get("actions", [])
        
        if not expected_actions:
            return 1.0
        
        matched = 0
        for exp in expected_actions:
            exp_name = exp.get("name", "")
            compare_args = exp.get("compare_args", [])
            exp_args = exp.get("arguments", {})
            
            for tc in tool_call_log:
                if tc["tool_name"] == exp_name:
                    if compare_args:
                        all_match = all(
                            str(tc["arguments"].get(k, "")).lower() == str(exp_args.get(k, "")).lower()
                            for k in compare_args
                            if k in exp_args
                        )
                        if all_match:
                            matched += 1
                            break
                    else:
                        matched += 1
                        break
        
        return matched / len(expected_actions)
    
    def run_all_tasks(self) -> list[TaskResult]:
        """Run all tasks (or specified task_ids) and return results."""
        from bioagents.gym.agent_env import BioAgentGymEnv
        
        # Create environment
        gym_env = BioAgentGymEnv(
            domain=self.config.domain,
            task_split=self.config.task_split,
            max_turns=self.config.max_turns,
        )
        
        # Get tasks
        if self.config.task_ids:
            task_ids = self.config.task_ids
        else:
            task_ids = [t["id"] for t in gym_env._tasks]
        
        logger.info(f"Running {len(task_ids)} tasks with {Path(self.config.model_name_or_path).name}")
        
        results = []
        for task_id in task_ids:
            task = gym_env._task_map[task_id]
            result = self.run_task(task, gym_env)
            results.append(result)
            self._save_task_result(result)
        
        # Save summary
        self._save_summary(results)
        
        return results
    
    def _save_task_result(self, result: TaskResult):
        """Save individual task result."""
        path = self.log_path / f"task_{result.task_id}.json"
        data = {
            "task_id": result.task_id,
            "domain": result.domain,
            "model_name": result.model_name,
            "total_turns": result.total_turns,
            "action_score": result.action_score,
            "final_reward": result.final_reward,
            "completed": result.completed,
            "error": result.error,
            "total_latency": result.total_latency,
            "start_time": result.start_time,
            "end_time": result.end_time,
            "turns": [
                {
                    "turn_idx": t.turn_idx,
                    "raw_output": t.raw_output,
                    "parsed_tool_call": t.parsed_tool_call,
                    "tool_response": t.tool_response[:500] if t.tool_response else None,
                    "is_final_answer": t.is_final_answer,
                    "latency_seconds": t.latency_seconds,
                }
                for t in result.turns
            ],
        }
        with open(path, "w", encoding="utf-8") as f:
            json.dump(data, f, indent=2, ensure_ascii=False, default=str)
    
    def _save_summary(self, results: list[TaskResult]):
        """Save a summary of all task results."""
        summary = {
            "run_id": self.run_id,
            "model": Path(self.config.model_name_or_path).name,
            "domain": self.config.domain,
            "backend": self.config.backend,
            "num_tasks": len(results),
            "num_completed": sum(1 for r in results if r.completed),
            "num_errors": sum(1 for r in results if r.error),
            "avg_action_score": sum(r.action_score for r in results) / max(len(results), 1),
            "avg_reward": sum(r.final_reward for r in results) / max(len(results), 1),
            "avg_turns": sum(r.total_turns for r in results) / max(len(results), 1),
            "total_latency": sum(r.total_latency for r in results),
            "per_task": [
                {
                    "task_id": r.task_id,
                    "action_score": r.action_score,
                    "final_reward": r.final_reward,
                    "turns": r.total_turns,
                    "latency": r.total_latency,
                    "completed": r.completed,
                    "error": r.error is not None,
                }
                for r in results
            ],
            "timestamp": datetime.now().isoformat(),
        }
        
        path = self.log_path / "summary.json"
        with open(path, "w", encoding="utf-8") as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)
        
        # Print summary table
        print("\n" + "=" * 80)
        print(f"  RUN SUMMARY: {self.run_id}")
        print("=" * 80)
        print(f"  Model: {summary['model']}")
        print(f"  Domain: {summary['domain']}")
        print(f"  Backend: {summary['backend']}")
        print(f"  Tasks: {summary['num_completed']}/{summary['num_tasks']} completed")
        print(f"  Avg Action Score: {summary['avg_action_score']:.3f}")
        print(f"  Avg Reward: {summary['avg_reward']:.3f}")
        print(f"  Avg Turns: {summary['avg_turns']:.1f}")
        print(f"  Total Latency: {summary['total_latency']:.1f}s")
        print("-" * 80)
        print(f"  {'Task ID':<30} {'Score':>8} {'Reward':>8} {'Turns':>6} {'Time':>8}")
        print("-" * 80)
        for t in summary["per_task"]:
            status = "âœ“" if t["completed"] else "âœ—"
            print(f"  {status} {t['task_id']:<28} {t['action_score']:>8.3f} {t['final_reward']:>8.3f} {t['turns']:>6} {t['latency']:>7.1f}s")
        print("=" * 80)
        
        logger.info(f"Results saved to {self.log_path}")

```

ë” ê·¼ë³¸ì ì¸ ë¬¸ì œë¥¼ ìˆ˜ì •í•©ë‹ˆë‹¤. ëª¨ë¸ì´ tool call + ì„¤ëª…ì„ ê°™ì´ ìƒì„±í•  ë•Œ, tool call ë¶€ë¶„ë§Œ ì¶”ì¶œí•˜ê³  ë‚¨ì€ í…ìŠ¤íŠ¸ëŠ” ë¬´ì‹œí•´ì•¼ í•©ë‹ˆë‹¤. ë˜í•œ `get_trajectory`ì—ì„œ `_env`ê°€ ì—†ëŠ” ê²½ìš°ë„ ì²˜ë¦¬í•´ì•¼ í•©ë‹ˆë‹¤.

```python
"""LLM Agent Runner for BIOAgents.

Connects a language model to the BIOAgents environment for multi-turn
tool-use evaluation. Supports:
- vLLM-based fast inference
- HuggingFace transformers-based inference
- Multi-turn tool calling with automatic parsing
- Full trajectory logging

Usage:
    runner = AgentRunner(
        model_name_or_path="Qwen/Qwen2.5-VL-7B-Instruct",
        backend="vllm",
    )
    results = runner.run_task(domain="clinical_diagnosis", task_id="dx_pneumonia_001")
"""

import json
import os
import re
import time
from copy import deepcopy
from dataclasses import dataclass, field
from datetime import datetime
from pathlib import Path
from typing import Any, Literal, Optional

from loguru import logger


@dataclass
class RunConfig:
    """Configuration for an agent run."""
    model_name_or_path: str
    backend: Literal["vllm", "transformers"] = "transformers"
    domain: str = "clinical_diagnosis"
    task_ids: Optional[list[str]] = None       # None = run all tasks
    task_split: Optional[str] = None
    max_turns: int = 15
    temperature: float = 0.1
    top_p: float = 0.95
    max_new_tokens: int = 1024
    tensor_parallel_size: int = 1
    gpu_memory_utilization: float = 0.85
    log_dir: str = "logs/runs"
    seed: int = 42


@dataclass
class TurnRecord:
    """Record of a single turn in the agent-environment interaction."""
    turn_idx: int
    prompt: str = ""
    raw_output: str = ""
    parsed_tool_call: Optional[dict] = None
    tool_response: Optional[str] = None
    is_final_answer: bool = False
    latency_seconds: float = 0.0


@dataclass 
class TaskResult:
    """Result of running a single task."""
    task_id: str
    domain: str
    model_name: str
    turns: list[TurnRecord] = field(default_factory=list)
    total_turns: int = 0
    action_score: float = 0.0
    final_reward: float = 0.0
    completed: bool = False
    error: Optional[str] = None
    trajectory: dict = field(default_factory=dict)
    start_time: str = ""
    end_time: str = ""
    total_latency: float = 0.0


def build_system_prompt(policy: str, tools: list[dict]) -> str:
    """Build the system prompt with policy and tool definitions."""
    tool_section = json.dumps(tools, indent=2, ensure_ascii=False)
    
    return f"""You are a medical AI assistant operating in a clinical environment. Follow the policy below and use the available tools to help with patient care.

## Policy
{policy}

## Available Tools
You have access to the following tools. To use a tool, respond with ONLY a JSON object in this exact format:
```json
{{"name": "tool_name", "arguments": {{"arg1": "value1", "arg2": "value2"}}}}
```

Do NOT include any other text when making a tool call. One tool call per response.

When you have gathered enough information and want to give your final assessment, respond with your clinical analysis as plain text (no JSON).

## Tool Definitions
{tool_section}"""


def parse_tool_call(text: str) -> Optional[dict]:
    """Parse a tool call from model output.
    
    Supports formats:
    1. Pure JSON: {"name": "...", "arguments": {...}}
    2. JSON in code block: ```json\n{...}\n```
    3. JSON embedded in text with markers
    """
    text = text.strip()
    
    # Try 1: Extract JSON from code blocks
    code_block_match = re.search(r'```(?:json)?\s*\n?({.*?})\s*\n?```', text, re.DOTALL)
    if code_block_match:
        try:
            parsed = json.loads(code_block_match.group(1))
            if "name" in parsed:
                return parsed
        except json.JSONDecodeError:
            pass
    
    # Try 2: Direct JSON parse
    try:
        parsed = json.loads(text)
        if isinstance(parsed, dict) and "name" in parsed:
            return parsed
    except json.JSONDecodeError:
        pass
    
    # Try 3: Find JSON-like pattern in text
    json_match = re.search(r'(\{[^{}]*"name"\s*:\s*"[^"]+?"[^{}]*"arguments"\s*:\s*\{[^{}]*\}[^{}]*\})', text, re.DOTALL)
    if json_match:
        try:
            parsed = json.loads(json_match.group(1))
            if "name" in parsed:
                return parsed
        except json.JSONDecodeError:
            pass
    
    # Try 4: More lenient nested JSON search
    matches = list(re.finditer(r'\{', text))
    for m in matches:
        start = m.start()
        depth = 0
        for i in range(start, len(text)):
            if text[i] == '{':
                depth += 1
            elif text[i] == '}':
                depth -= 1
                if depth == 0:
                    candidate = text[start:i+1]
                    try:
                        parsed = json.loads(candidate)
                        if isinstance(parsed, dict) and "name" in parsed:
                            return parsed
                    except json.JSONDecodeError:
                        pass
                    break
    
    return None


class AgentRunner:
    """Runs LLM agents in BIOAgents environments."""
    
    def __init__(self, config: RunConfig):
        self.config = config
        self.model = None
        self.tokenizer = None
        self._setup_logging()
    
    def _setup_logging(self):
        """Set up logging directory."""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        model_short = Path(self.config.model_name_or_path).name
        self.run_id = f"{model_short}_{self.config.domain}_{timestamp}"
        self.log_path = Path(self.config.log_dir) / self.run_id
        self.log_path.mkdir(parents=True, exist_ok=True)
        
        # Save config
        config_dict = {
            k: v for k, v in self.config.__dict__.items()
        }
        with open(self.log_path / "config.json", "w") as f:
            json.dump(config_dict, f, indent=2, default=str)
    
    def load_model(self):
        """Load the language model."""
        if self.config.backend == "vllm":
            self._load_vllm()
        else:
            self._load_transformers()
    
    def _load_vllm(self):
        """Load model with vLLM."""
        from vllm import LLM, SamplingParams
        
        logger.info(f"Loading {self.config.model_name_or_path} with vLLM (tp={self.config.tensor_parallel_size})")
        self.model = LLM(
            model=self.config.model_name_or_path,
            tensor_parallel_size=self.config.tensor_parallel_size,
            gpu_memory_utilization=self.config.gpu_memory_utilization,
            trust_remote_code=True,
            max_model_len=8192,
            seed=self.config.seed,
        )
        self.sampling_params = SamplingParams(
            temperature=self.config.temperature,
            top_p=self.config.top_p,
            max_tokens=self.config.max_new_tokens,
            stop=["```\n\n", "\n\nUser:", "\n\nHuman:"],
        )
        logger.info("vLLM model loaded successfully")
    
    def _load_transformers(self):
        """Load model with HuggingFace transformers."""
        import torch
        from transformers import (
            AutoModelForCausalLM,
            AutoTokenizer,
            AutoProcessor,
            AutoConfig,
        )
        
        logger.info(f"Loading {self.config.model_name_or_path} with transformers")
        
        model_path = self.config.model_name_or_path
        
        # Detect model type from config
        config = AutoConfig.from_pretrained(model_path, trust_remote_code=True)
        model_type = getattr(config, "model_type", "")
        architectures = getattr(config, "architectures", [])
        
        self._is_vl_model = any(
            "vl" in a.lower() or "vision" in a.lower()
            for a in (architectures or [])
        ) or "vl" in model_type.lower()
        
        logger.info(f"Model type: {model_type}, VL: {self._is_vl_model}, Arch: {architectures}")
        
        # Load tokenizer / processor
        if self._is_vl_model:
            try:
                self.processor = AutoProcessor.from_pretrained(
                    model_path, trust_remote_code=True
                )
                self.tokenizer = (
                    self.processor.tokenizer
                    if hasattr(self.processor, "tokenizer")
                    else self.processor
                )
            except Exception:
                self.tokenizer = AutoTokenizer.from_pretrained(
                    model_path, trust_remote_code=True
                )
                self.processor = None
        else:
            self.tokenizer = AutoTokenizer.from_pretrained(
                model_path, trust_remote_code=True
            )
            self.processor = None
        
        # Load model â€“ choose the right Auto class
        load_kwargs = dict(
            torch_dtype=torch.bfloat16,
            device_map="auto",
            trust_remote_code=True,
        )
        
        # Try sdpa first, then fall back
        for attn_impl in ["sdpa", None]:
            try:
                kw = {**load_kwargs}
                if attn_impl:
                    kw["attn_implementation"] = attn_impl
                
                if self._is_vl_model:
                    # VL models need their specific class
                    from transformers import Qwen2_5_VLForConditionalGeneration
                    self.model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
                        model_path, **kw
                    )
                else:
                    self.model = AutoModelForCausalLM.from_pretrained(
                        model_path, **kw
                    )
                break
            except Exception as e:
                logger.warning(f"Failed with attn_impl={attn_impl}: {e}")
                if attn_impl is None:
                    raise
        
        self.model.eval()
        logger.info(f"Model loaded (VL={self._is_vl_model})")
    
    def generate(self, messages: list[dict]) -> str:
        """Generate a response from the model."""
        if self.config.backend == "vllm":
            return self._generate_vllm(messages)
        else:
            return self._generate_transformers(messages)
    
    def _generate_vllm(self, messages: list[dict]) -> str:
        """Generate with vLLM using chat template."""
        from vllm import SamplingParams
        
        # Use the tokenizer from vLLM engine
        tokenizer = self.model.get_tokenizer()
        prompt = tokenizer.apply_chat_template(
            messages, tokenize=False, add_generation_prompt=True
        )
        
        outputs = self.model.generate([prompt], self.sampling_params)
        return outputs[0].outputs[0].text.strip()
    
    def _generate_transformers(self, messages: list[dict]) -> str:
        """Generate with HuggingFace transformers."""
        import torch
        
        # For VL models, use processor if available
        if self._is_vl_model and self.processor is not None:
            # Text-only input for VL model
            text = self.processor.apply_chat_template(
                messages, tokenize=False, add_generation_prompt=True
            )
            inputs = self.processor(
                text=[text], padding=True, return_tensors="pt"
            ).to(self.model.device)
        else:
            # Apply chat template
            if hasattr(self.tokenizer, "apply_chat_template"):
                text = self.tokenizer.apply_chat_template(
                    messages, tokenize=False, add_generation_prompt=True
                )
            else:
                text = ""
                for msg in messages:
                    role = msg["role"]
                    content = msg["content"]
                    text += f"<|{role}|>\n{content}\n"
                text += "<|assistant|>\n"
            
            inputs = self.tokenizer(text, return_tensors="pt").to(self.model.device)
        
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=self.config.max_new_tokens,
                temperature=self.config.temperature if self.config.temperature > 0 else None,
                top_p=self.config.top_p if self.config.temperature > 0 else None,
                do_sample=self.config.temperature > 0,
                pad_token_id=self.tokenizer.eos_token_id,
            )
        
        # Decode only the generated tokens
        input_len = inputs["input_ids"].shape[-1]
        generated_ids = outputs[0][input_len:]
        return self.tokenizer.decode(generated_ids, skip_special_tokens=True).strip()
    
    def run_task(self, task: dict, env) -> TaskResult:
        """Run a single task in the environment.
        
        Args:
            task: Task dictionary from tasks.json
            env: BioAgentGymEnv instance
            
        Returns:
            TaskResult with full trajectory and scores
        """
        task_id = task["id"]
        result = TaskResult(
            task_id=task_id,
            domain=self.config.domain,
            model_name=Path(self.config.model_name_or_path).name,
            start_time=datetime.now().isoformat(),
        )
        
        logger.info(f"Starting task: {task_id}")
        
        # Reset environment
        obs, info = env.reset(options={"task_id": task_id})
        
        # Build conversation
        system_prompt = build_system_prompt(info["policy"], info["tools"])
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": obs},
        ]
        
        try:
            for turn_idx in range(self.config.max_turns):
                turn = TurnRecord(turn_idx=turn_idx)
                
                # Generate
                t0 = time.time()
                raw_output = self.generate(messages)
                turn.latency_seconds = time.time() - t0
                turn.raw_output = raw_output
                
                logger.debug(f"Turn {turn_idx}: {raw_output[:200]}...")
                
                # Parse tool call
                tool_call = parse_tool_call(raw_output)
                
                if tool_call is not None:
                    turn.parsed_tool_call = tool_call
                    
                    # Execute tool via environment
                    action = json.dumps(tool_call)
                    observation, reward, terminated, truncated, step_info = env.step(action)
                    
                    turn.tool_response = observation
                    
                    # Add to messages
                    messages.append({"role": "assistant", "content": raw_output})
                    messages.append({
                        "role": "user",
                        "content": f"Tool result for {tool_call['name']}:\n{observation}"
                    })
                    
                    result.turns.append(turn)
                    
                    if terminated or truncated:
                        break
                else:
                    # Final answer (no tool call)
                    turn.is_final_answer = True
                    messages.append({"role": "assistant", "content": raw_output})
                    result.turns.append(turn)
                    
                    # Check for repetition (model stuck in a loop)
                    if len(result.turns) >= 3:
                        recent = [t.raw_output[:100] for t in result.turns[-3:]]
                        if len(set(recent)) == 1:
                            logger.warning(f"Repetition detected at turn {turn_idx}, stopping.")
                            break
                    
                    # Mark as terminated - agent gave final answer
                    env._env.state.terminated = True if hasattr(env, '_env') else None
                    break
            
            # Get final trajectory and reward
            try:
                trajectory = env.get_trajectory()
                result.trajectory = trajectory
                result.final_reward = trajectory["final_reward"]
            except Exception:
                result.trajectory = {}
                result.final_reward = 0.0
            
            result.total_turns = len(result.turns)
            result.action_score = self._compute_action_score(task, env._tool_call_log)
            result.completed = True
            
        except Exception as e:
            logger.error(f"Error in task {task_id}: {e}")
            result.error = str(e)
            import traceback
            result.error += "\n" + traceback.format_exc()
        
        result.end_time = datetime.now().isoformat()
        result.total_latency = sum(t.latency_seconds for t in result.turns)
        
        logger.info(
            f"Task {task_id}: turns={result.total_turns}, "
            f"action_score={result.action_score:.3f}, "
            f"reward={result.final_reward:.3f}, "
            f"latency={result.total_latency:.1f}s"
        )
        
        return result
    
    def _compute_action_score(self, task: dict, tool_call_log: list) -> float:
        """Compute action score based on expected vs actual tool calls."""
        eval_criteria = task.get("evaluation_criteria", {})
        expected_actions = eval_criteria.get("actions", [])
        
        if not expected_actions:
            return 1.0
        
        matched = 0
        for exp in expected_actions:
            exp_name = exp.get("name", "")
            compare_args = exp.get("compare_args", [])
            exp_args = exp.get("arguments", {})
            
            for tc in tool_call_log:
                if tc["tool_name"] == exp_name:
                    if compare_args:
                        all_match = all(
                            str(tc["arguments"].get(k, "")).lower() == str(exp_args.get(k, "")).lower()
                            for k in compare_args
                            if k in exp_args
                        )
                        if all_match:
                            matched += 1
                            break
                    else:
                        matched += 1
                        break
        
        return matched / len(expected_actions)
    
    def run_all_tasks(self) -> list[TaskResult]:
        """Run all tasks (or specified task_ids) and return results."""
        from bioagents.gym.agent_env import BioAgentGymEnv
        
        # Create environment
        gym_env = BioAgentGymEnv(
            domain=self.config.domain,
            task_split=self.config.task_split,
            max_turns=self.config.max_turns,
        )
        
        # Get tasks
        if self.config.task_ids:
            task_ids = self.config.task_ids
        else:
            task_ids = [t["id"] for t in gym_env._tasks]
        
        logger.info(f"Running {len(task_ids)} tasks with {Path(self.config.model_name_or_path).name}")
        
        results = []
        for task_id in task_ids:
            task = gym_env._task_map[task_id]
            result = self.run_task(task, gym_env)
            results.append(result)
            self._save_task_result(result)
        
        # Save summary
        self._save_summary(results)
        
        return results
    
    def _save_task_result(self, result: TaskResult):
        """Save individual task result."""
        path = self.log_path / f"task_{result.task_id}.json"
        data = {
            "task_id": result.task_id,
            "domain": result.domain,
            "model_name": result.model_name,
            "total_turns": result.total_turns,
            "action_score": result.action_score,
            "final_reward": result.final_reward,
            "completed": result.completed,
            "error": result.error,
            "total_latency": result.total_latency,
            "start_time": result.start_time,
            "end_time": result.end_time,
            "turns": [
                {
                    "turn_idx": t.turn_idx,
                    "raw_output": t.raw_output,
                    "parsed_tool_call": t.parsed_tool_call,
                    "tool_response": t.tool_response[:500] if t.tool_response else None,
                    "is_final_answer": t.is_final_answer,
                    "latency_seconds": t.latency_seconds,
                }
                for t in result.turns
            ],
        }
        with open(path, "w", encoding="utf-8") as f:
            json.dump(data, f, indent=2, ensure_ascii=False, default=str)
    
    def _save_summary(self, results: list[TaskResult]):
        """Save a summary of all task results."""
        summary = {
            "run_id": self.run_id,
            "model": Path(self.config.model_name_or_path).name,
            "domain": self.config.domain,
            "backend": self.config.backend,
            "num_tasks": len(results),
            "num_completed": sum(1 for r in results if r.completed),
            "num_errors": sum(1 for r in results if r.error),
            "avg_action_score": sum(r.action_score for r in results) / max(len(results), 1),
            "avg_reward": sum(r.final_reward for r in results) / max(len(results), 1),
            "avg_turns": sum(r.total_turns for r in results) / max(len(results), 1),
            "total_latency": sum(r.total_latency for r in results),
            "per_task": [
                {
                    "task_id": r.task_id,
                    "action_score": r.action_score,
                    "final_reward": r.final_reward,
                    "turns": r.total_turns,
                    "latency": r.total_latency,
                    "completed": r.completed,
                    "error": r.error is not None,
                }
                for r in results
            ],
            "timestamp": datetime.now().isoformat(),
        }
        
        path = self.log_path / "summary.json"
        with open(path, "w", encoding="utf-8") as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)
        
        # Print summary table
        print("\n" + "=" * 80)
        print(f"  RUN SUMMARY: {self.run_id}")
        print("=" * 80)
        print(f"  Model: {summary['model']}")
        print(f"  Domain: {summary['domain']}")
        print(f"  Backend: {summary['backend']}")
        print(f"  Tasks: {summary['num_completed']}/{summary['num_tasks']} completed")
        print(f"  Avg Action Score: {summary['avg_action_score']:.3f}")
        print(f"  Avg Reward: {summary['avg_reward']:.3f}")
        print(f"  Avg Turns: {summary['avg_turns']:.1f}")
        print(f"  Total Latency: {summary['total_latency']:.1f}s")
        print("-" * 80)
        print(f"  {'Task ID':<30} {'Score':>8} {'Reward':>8} {'Turns':>6} {'Time':>8}")
        print("-" * 80)
        for t in summary["per_task"]:
            status = "âœ“" if t["completed"] else "âœ—"
            print(f"  {status} {t['task_id']:<28} {t['action_score']:>8.3f} {t['final_reward']:>8.3f} {t['turns']:>6} {t['latency']:>7.1f}s")
        print("=" * 80)
        
        logger.info(f"Results saved to {self.log_path}")

```

ë˜í•œ tool call í›„ ë°˜ë³µì— ë¹ ì§€ì§€ ì•Šë„ë¡, ëª¨ë¸ì´ ë™ì¼ toolì„ ì—°ì†ìœ¼ë¡œ í˜¸ì¶œí•˜ë©´ ì¤‘ë‹¨í•˜ëŠ” ë¡œì§ë„ ì¶”ê°€í•©ë‹ˆë‹¤.

```python
"""LLM Agent Runner for BIOAgents.

Connects a language model to the BIOAgents environment for multi-turn
tool-use evaluation. Supports:
- vLLM-based fast inference
- HuggingFace transformers-based inference
- Multi-turn tool calling with automatic parsing
- Full trajectory logging

Usage:
    runner = AgentRunner(
        model_name_or_path="Qwen/Qwen2.5-VL-7B-Instruct",
        backend="vllm",
    )
    results = runner.run_task(domain="clinical_diagnosis", task_id="dx_pneumonia_001")
"""

import json
import os
import re
import time
from copy import deepcopy
from dataclasses import dataclass, field
from datetime import datetime
from pathlib import Path
from typing import Any, Literal, Optional

from loguru import logger


@dataclass
class RunConfig:
    """Configuration for an agent run."""
    model_name_or_path: str
    backend: Literal["vllm", "transformers"] = "transformers"
    domain: str = "clinical_diagnosis"
    task_ids: Optional[list[str]] = None       # None = run all tasks
    task_split: Optional[str] = None
    max_turns: int = 15
    temperature: float = 0.1
    top_p: float = 0.95
    max_new_tokens: int = 1024
    tensor_parallel_size: int = 1
    gpu_memory_utilization: float = 0.85
    log_dir: str = "logs/runs"
    seed: int = 42


@dataclass
class TurnRecord:
    """Record of a single turn in the agent-environment interaction."""
    turn_idx: int
    prompt: str = ""
    raw_output: str = ""
    parsed_tool_call: Optional[dict] = None
    tool_response: Optional[str] = None
    is_final_answer: bool = False
    latency_seconds: float = 0.0


@dataclass 
class TaskResult:
    """Result of running a single task."""
    task_id: str
    domain: str
    model_name: str
    turns: list[TurnRecord] = field(default_factory=list)
    total_turns: int = 0
    action_score: float = 0.0
    final_reward: float = 0.0
    completed: bool = False
    error: Optional[str] = None
    trajectory: dict = field(default_factory=dict)
    start_time: str = ""
    end_time: str = ""
    total_latency: float = 0.0


def build_system_prompt(policy: str, tools: list[dict]) -> str:
    """Build the system prompt with policy and tool definitions."""
    tool_section = json.dumps(tools, indent=2, ensure_ascii=False)
    
    return f"""You are a medical AI assistant operating in a clinical environment. Follow the policy below and use the available tools to help with patient care.

## Policy
{policy}

## Available Tools
You have access to the following tools. To use a tool, respond with ONLY a JSON object in this exact format:
```json
{{"name": "tool_name", "arguments": {{"arg1": "value1", "arg2": "value2"}}}}
```

Do NOT include any other text when making a tool call. One tool call per response.

When you have gathered enough information and want to give your final assessment, respond with your clinical analysis as plain text (no JSON).

## Tool Definitions
{tool_section}"""


def parse_tool_call(text: str) -> Optional[dict]:
    """Parse a tool call from model output.
    
    Supports formats:
    1. Pure JSON: {"name": "...", "arguments": {...}}
    2. JSON in code block: ```json\n{...}\n```
    3. JSON embedded in text with markers
    """
    text = text.strip()
    
    # Try 1: Extract JSON from code blocks
    code_block_match = re.search(r'```(?:json)?\s*\n?({.*?})\s*\n?```', text, re.DOTALL)
    if code_block_match:
        try:
            parsed = json.loads(code_block_match.group(1))
            if "name" in parsed:
                return parsed
        except json.JSONDecodeError:
            pass
    
    # Try 2: Direct JSON parse
    try:
        parsed = json.loads(text)
        if isinstance(parsed, dict) and "name" in parsed:
            return parsed
    except json.JSONDecodeError:
        pass
    
    # Try 3: Find JSON-like pattern in text
    json_match = re.search(r'(\{[^{}]*"name"\s*:\s*"[^"]+?"[^{}]*"arguments"\s*:\s*\{[^{}]*\}[^{}]*\})', text, re.DOTALL)
    if json_match:
        try:
            parsed = json.loads(json_match.group(1))
            if "name" in parsed:
                return parsed
        except json.JSONDecodeError:
            pass
    
    # Try 4: More lenient nested JSON search
    matches = list(re.finditer(r'\{', text))
    for m in matches:
        start = m.start()
        depth = 0
        for i in range(start, len(text)):
            if text[i] == '{':
                depth += 1
            elif text[i] == '}':
                depth -= 1
                if depth == 0:
                    candidate = text[start:i+1]
                    try:
                        parsed = json.loads(candidate)
                        if isinstance(parsed, dict) and "name" in parsed:
                            return parsed
                    except json.JSONDecodeError:
                        pass
                    break
    
    return None


class AgentRunner:
    """Runs LLM agents in BIOAgents environments."""
    
    def __init__(self, config: RunConfig):
        self.config = config
        self.model = None
        self.tokenizer = None
        self._setup_logging()
    
    def _setup_logging(self):
        """Set up logging directory."""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        model_short = Path(self.config.model_name_or_path).name
        self.run_id = f"{model_short}_{self.config.domain}_{timestamp}"
        self.log_path = Path(self.config.log_dir) / self.run_id
        self.log_path.mkdir(parents=True, exist_ok=True)
        
        # Save config
        config_dict = {
            k: v for k, v in self.config.__dict__.items()
        }
        with open(self.log_path / "config.json", "w") as f:
            json.dump(config_dict, f, indent=2, default=str)
    
    def load_model(self):
        """Load the language model."""
        if self.config.backend == "vllm":
            self._load_vllm()
        else:
            self._load_transformers()
    
    def _load_vllm(self):
        """Load model with vLLM."""
        from vllm import LLM, SamplingParams
        
        logger.info(f"Loading {self.config.model_name_or_path} with vLLM (tp={self.config.tensor_parallel_size})")
        self.model = LLM(
            model=self.config.model_name_or_path,
            tensor_parallel_size=self.config.tensor_parallel_size,
            gpu_memory_utilization=self.config.gpu_memory_utilization,
            trust_remote_code=True,
            max_model_len=8192,
            seed=self.config.seed,
        )
        self.sampling_params = SamplingParams(
            temperature=self.config.temperature,
            top_p=self.config.top_p,
            max_tokens=self.config.max_new_tokens,
            stop=["```\n\n", "\n\nUser:", "\n\nHuman:"],
        )
        logger.info("vLLM model loaded successfully")
    
    def _load_transformers(self):
        """Load model with HuggingFace transformers."""
        import torch
        from transformers import (
            AutoModelForCausalLM,
            AutoTokenizer,
            AutoProcessor,
            AutoConfig,
        )
        
        logger.info(f"Loading {self.config.model_name_or_path} with transformers")
        
        model_path = self.config.model_name_or_path
        
        # Detect model type from config
        config = AutoConfig.from_pretrained(model_path, trust_remote_code=True)
        model_type = getattr(config, "model_type", "")
        architectures = getattr(config, "architectures", [])
        
        self._is_vl_model = any(
            "vl" in a.lower() or "vision" in a.lower()
            for a in (architectures or [])
        ) or "vl" in model_type.lower()
        
        logger.info(f"Model type: {model_type}, VL: {self._is_vl_model}, Arch: {architectures}")
        
        # Load tokenizer / processor
        if self._is_vl_model:
            try:
                self.processor = AutoProcessor.from_pretrained(
                    model_path, trust_remote_code=True
                )
                self.tokenizer = (
                    self.processor.tokenizer
                    if hasattr(self.processor, "tokenizer")
                    else self.processor
                )
            except Exception:
                self.tokenizer = AutoTokenizer.from_pretrained(
                    model_path, trust_remote_code=True
                )
                self.processor = None
        else:
            self.tokenizer = AutoTokenizer.from_pretrained(
                model_path, trust_remote_code=True
            )
            self.processor = None
        
        # Load model â€“ choose the right Auto class
        load_kwargs = dict(
            torch_dtype=torch.bfloat16,
            device_map="auto",
            trust_remote_code=True,
        )
        
        # Try sdpa first, then fall back
        for attn_impl in ["sdpa", None]:
            try:
                kw = {**load_kwargs}
                if attn_impl:
                    kw["attn_implementation"] = attn_impl
                
                if self._is_vl_model:
                    # VL models need their specific class
                    from transformers import Qwen2_5_VLForConditionalGeneration
                    self.model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
                        model_path, **kw
                    )
                else:
                    self.model = AutoModelForCausalLM.from_pretrained(
                        model_path, **kw
                    )
                break
            except Exception as e:
                logger.warning(f"Failed with attn_impl={attn_impl}: {e}")
                if attn_impl is None:
                    raise
        
        self.model.eval()
        logger.info(f"Model loaded (VL={self._is_vl_model})")
    
    def generate(self, messages: list[dict]) -> str:
        """Generate a response from the model."""
        if self.config.backend == "vllm":
            return self._generate_vllm(messages)
        else:
            return self._generate_transformers(messages)
    
    def _generate_vllm(self, messages: list[dict]) -> str:
        """Generate with vLLM using chat template."""
        from vllm import SamplingParams
        
        # Use the tokenizer from vLLM engine
        tokenizer = self.model.get_tokenizer()
        prompt = tokenizer.apply_chat_template(
            messages, tokenize=False, add_generation_prompt=True
        )
        
        outputs = self.model.generate([prompt], self.sampling_params)
        return outputs[0].outputs[0].text.strip()
    
    def _generate_transformers(self, messages: list[dict]) -> str:
        """Generate with HuggingFace transformers."""
        import torch
        
        # For VL models, use processor if available
        if self._is_vl_model and self.processor is not None:
            # Text-only input for VL model
            text = self.processor.apply_chat_template(
                messages, tokenize=False, add_generation_prompt=True
            )
            inputs = self.processor(
                text=[text], padding=True, return_tensors="pt"
            ).to(self.model.device)
        else:
            # Apply chat template
            if hasattr(self.tokenizer, "apply_chat_template"):
                text = self.tokenizer.apply_chat_template(
                    messages, tokenize=False, add_generation_prompt=True
                )
            else:
                text = ""
                for msg in messages:
                    role = msg["role"]
                    content = msg["content"]
                    text += f"<|{role}|>\n{content}\n"
                text += "<|assistant|>\n"
            
            inputs = self.tokenizer(text, return_tensors="pt").to(self.model.device)
        
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=self.config.max_new_tokens,
                temperature=self.config.temperature if self.config.temperature > 0 else None,
                top_p=self.config.top_p if self.config.temperature > 0 else None,
                do_sample=self.config.temperature > 0,
                pad_token_id=self.tokenizer.eos_token_id,
            )
        
        # Decode only the generated tokens
        input_len = inputs["input_ids"].shape[-1]
        generated_ids = outputs[0][input_len:]
        return self.tokenizer.decode(generated_ids, skip_special_tokens=True).strip()
    
    def run_task(self, task: dict, env) -> TaskResult:
        """Run a single task in the environment.
        
        Args:
            task: Task dictionary from tasks.json
            env: BioAgentGymEnv instance
            
        Returns:
            TaskResult with full trajectory and scores
        """
        task_id = task["id"]
        result = TaskResult(
            task_id=task_id,
            domain=self.config.domain,
            model_name=Path(self.config.model_name_or_path).name,
            start_time=datetime.now().isoformat(),
        )
        
        logger.info(f"Starting task: {task_id}")
        
        # Reset environment
        obs, info = env.reset(options={"task_id": task_id})
        
        # Build conversation
        system_prompt = build_system_prompt(info["policy"], info["tools"])
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": obs},
        ]
        
        try:
            for turn_idx in range(self.config.max_turns):
                turn = TurnRecord(turn_idx=turn_idx)
                
                # Generate
                t0 = time.time()
                raw_output = self.generate(messages)
                turn.latency_seconds = time.time() - t0
                turn.raw_output = raw_output
                
                logger.debug(f"Turn {turn_idx}: {raw_output[:200]}...")
                
                # Parse tool call
                tool_call = parse_tool_call(raw_output)
                
                if tool_call is not None:
                    turn.parsed_tool_call = tool_call
                    
                    # Detect repeated tool call (stuck model)
                    if len(result.turns) >= 2:
                        prev_calls = [
                            json.dumps(t.parsed_tool_call, sort_keys=True)
                            for t in result.turns[-2:]
                            if t.parsed_tool_call
                        ]
                        cur_call = json.dumps(tool_call, sort_keys=True)
                        if prev_calls.count(cur_call) >= 2:
                            logger.warning(f"Repeated tool call detected: {tool_call['name']}, stopping.")
                            turn.is_final_answer = True
                            result.turns.append(turn)
                            break
                    
                    # Execute tool via environment
                    action = json.dumps(tool_call)
                    observation, reward, terminated, truncated, step_info = env.step(action)
                    
                    turn.tool_response = observation
                    
                    # Add to messages - only the tool call JSON part
                    messages.append({"role": "assistant", "content": json.dumps(tool_call)})
                    messages.append({
                        "role": "user",
                        "content": f"Tool result for {tool_call['name']}:\n{observation}"
                    })
                    
                    result.turns.append(turn)
                    
                    if terminated or truncated:
                        break
                else:
                    # Final answer (no tool call)
                    turn.is_final_answer = True
                    messages.append({"role": "assistant", "content": raw_output})
                    result.turns.append(turn)
                    
                    # Check for repetition (model stuck in a loop)
                    if len(result.turns) >= 3:
                        recent = [t.raw_output[:100] for t in result.turns[-3:]]
                        if len(set(recent)) == 1:
                            logger.warning(f"Repetition detected at turn {turn_idx}, stopping.")
                            break
                    
                    # Mark as terminated - agent gave final answer
                    env._env.state.terminated = True if hasattr(env, '_env') else None
                    break
            
            # Get final trajectory and reward
            try:
                trajectory = env.get_trajectory()
                result.trajectory = trajectory
                result.final_reward = trajectory["final_reward"]
            except Exception:
                result.trajectory = {}
                result.final_reward = 0.0
            
            result.total_turns = len(result.turns)
            result.action_score = self._compute_action_score(task, env._tool_call_log)
            result.completed = True
            
        except Exception as e:
            logger.error(f"Error in task {task_id}: {e}")
            result.error = str(e)
            import traceback
            result.error += "\n" + traceback.format_exc()
        
        result.end_time = datetime.now().isoformat()
        result.total_latency = sum(t.latency_seconds for t in result.turns)
        
        logger.info(
            f"Task {task_id}: turns={result.total_turns}, "
            f"action_score={result.action_score:.3f}, "
            f"reward={result.final_reward:.3f}, "
            f"latency={result.total_latency:.1f}s"
        )
        
        return result
    
    def _compute_action_score(self, task: dict, tool_call_log: list) -> float:
        """Compute action score based on expected vs actual tool calls."""
        eval_criteria = task.get("evaluation_criteria", {})
        expected_actions = eval_criteria.get("actions", [])
        
        if not expected_actions:
            return 1.0
        
        matched = 0
        for exp in expected_actions:
            exp_name = exp.get("name", "")
            compare_args = exp.get("compare_args", [])
            exp_args = exp.get("arguments", {})
            
            for tc in tool_call_log:
                if tc["tool_name"] == exp_name:
                    if compare_args:
                        all_match = all(
                            str(tc["arguments"].get(k, "")).lower() == str(exp_args.get(k, "")).lower()
                            for k in compare_args
                            if k in exp_args
                        )
                        if all_match:
                            matched += 1
                            break
                    else:
                        matched += 1
                        break
        
        return matched / len(expected_actions)
    
    def run_all_tasks(self) -> list[TaskResult]:
        """Run all tasks (or specified task_ids) and return results."""
        from bioagents.gym.agent_env import BioAgentGymEnv
        
        # Create environment
        gym_env = BioAgentGymEnv(
            domain=self.config.domain,
            task_split=self.config.task_split,
            max_turns=self.config.max_turns,
        )
        
        # Get tasks
        if self.config.task_ids:
            task_ids = self.config.task_ids
        else:
            task_ids = [t["id"] for t in gym_env._tasks]
        
        logger.info(f"Running {len(task_ids)} tasks with {Path(self.config.model_name_or_path).name}")
        
        results = []
        for task_id in task_ids:
            task = gym_env._task_map[task_id]
            result = self.run_task(task, gym_env)
            results.append(result)
            self._save_task_result(result)
        
        # Save summary
        self._save_summary(results)
        
        return results
    
    def _save_task_result(self, result: TaskResult):
        """Save individual task result."""
        path = self.log_path / f"task_{result.task_id}.json"
        data = {
            "task_id": result.task_id,
            "domain": result.domain,
            "model_name": result.model_name,
            "total_turns": result.total_turns,
            "action_score": result.action_score,
            "final_reward": result.final_reward,
            "completed": result.completed,
            "error": result.error,
            "total_latency": result.total_latency,
            "start_time": result.start_time,
            "end_time": result.end_time,
            "turns": [
                {
                    "turn_idx": t.turn_idx,
                    "raw_output": t.raw_output,
                    "parsed_tool_call": t.parsed_tool_call,
                    "tool_response": t.tool_response[:500] if t.tool_response else None,
                    "is_final_answer": t.is_final_answer,
                    "latency_seconds": t.latency_seconds,
                }
                for t in result.turns
            ],
        }
        with open(path, "w", encoding="utf-8") as f:
            json.dump(data, f, indent=2, ensure_ascii=False, default=str)
    
    def _save_summary(self, results: list[TaskResult]):
        """Save a summary of all task results."""
        summary = {
            "run_id": self.run_id,
            "model": Path(self.config.model_name_or_path).name,
            "domain": self.config.domain,
            "backend": self.config.backend,
            "num_tasks": len(results),
            "num_completed": sum(1 for r in results if r.completed),
            "num_errors": sum(1 for r in results if r.error),
            "avg_action_score": sum(r.action_score for r in results) / max(len(results), 1),
            "avg_reward": sum(r.final_reward for r in results) / max(len(results), 1),
            "avg_turns": sum(r.total_turns for r in results) / max(len(results), 1),
            "total_latency": sum(r.total_latency for r in results),
            "per_task": [
                {
                    "task_id": r.task_id,
                    "action_score": r.action_score,
                    "final_reward": r.final_reward,
                    "turns": r.total_turns,
                    "latency": r.total_latency,
                    "completed": r.completed,
                    "error": r.error is not None,
                }
                for r in results
            ],
            "timestamp": datetime.now().isoformat(),
        }
        
        path = self.log_path / "summary.json"
        with open(path, "w", encoding="utf-8") as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)
        
        # Print summary table
        print("\n" + "=" * 80)
        print(f"  RUN SUMMARY: {self.run_id}")
        print("=" * 80)
        print(f"  Model: {summary['model']}")
        print(f"  Domain: {summary['domain']}")
        print(f"  Backend: {summary['backend']}")
        print(f"  Tasks: {summary['num_completed']}/{summary['num_tasks']} completed")
        print(f"  Avg Action Score: {summary['avg_action_score']:.3f}")
        print(f"  Avg Reward: {summary['avg_reward']:.3f}")
        print(f"  Avg Turns: {summary['avg_turns']:.1f}")
        print(f"  Total Latency: {summary['total_latency']:.1f}s")
        print("-" * 80)
        print(f"  {'Task ID':<30} {'Score':>8} {'Reward':>8} {'Turns':>6} {'Time':>8}")
        print("-" * 80)
        for t in summary["per_task"]:
            status = "âœ“" if t["completed"] else "âœ—"
            print(f"  {status} {t['task_id']:<28} {t['action_score']:>8.3f} {t['final_reward']:>8.3f} {t['turns']:>6} {t['latency']:>7.1f}s")
        print("=" * 80)
        
        logger.info(f"Results saved to {self.log_path}")

```

Lingshuê°€ ì´ë¯¸ ê²°ê³¼ë¥¼ ë³´ì—¬ì¤¬ìœ¼ë‹ˆ í™•ì¸í•©ë‹ˆë‹¤. Qwen3ë„ í™•ì¸í•©ë‹ˆë‹¤.

Lingshuê°€ `get_differential_diagnosis`ë¥¼ 13ë²ˆ ë°˜ë³µ í˜¸ì¶œí–ˆìŠµë‹ˆë‹¤. ë°˜ë³µ tool call ê°ì§€ë¥¼ ë” ê°•í™”í•˜ê³  ì¬ì‹¤í–‰í•˜ê² ìŠµë‹ˆë‹¤.

```python
"""LLM Agent Runner for BIOAgents.

Connects a language model to the BIOAgents environment for multi-turn
tool-use evaluation. Supports:
- vLLM-based fast inference
- HuggingFace transformers-based inference
- Multi-turn tool calling with automatic parsing
- Full trajectory logging

Usage:
    runner = AgentRunner(
        model_name_or_path="Qwen/Qwen2.5-VL-7B-Instruct",
        backend="vllm",
    )
    results = runner.run_task(domain="clinical_diagnosis", task_id="dx_pneumonia_001")
"""

import json
import os
import re
import time
from copy import deepcopy
from dataclasses import dataclass, field
from datetime import datetime
from pathlib import Path
from typing import Any, Literal, Optional

from loguru import logger


@dataclass
class RunConfig:
    """Configuration for an agent run."""
    model_name_or_path: str
    backend: Literal["vllm", "transformers"] = "transformers"
    domain: str = "clinical_diagnosis"
    task_ids: Optional[list[str]] = None       # None = run all tasks
    task_split: Optional[str] = None
    max_turns: int = 15
    temperature: float = 0.1
    top_p: float = 0.95
    max_new_tokens: int = 1024
    tensor_parallel_size: int = 1
    gpu_memory_utilization: float = 0.85
    log_dir: str = "logs/runs"
    seed: int = 42


@dataclass
class TurnRecord:
    """Record of a single turn in the agent-environment interaction."""
    turn_idx: int
    prompt: str = ""
    raw_output: str = ""
    parsed_tool_call: Optional[dict] = None
    tool_response: Optional[str] = None
    is_final_answer: bool = False
    latency_seconds: float = 0.0


@dataclass 
class TaskResult:
    """Result of running a single task."""
    task_id: str
    domain: str
    model_name: str
    turns: list[TurnRecord] = field(default_factory=list)
    total_turns: int = 0
    action_score: float = 0.0
    final_reward: float = 0.0
    completed: bool = False
    error: Optional[str] = None
    trajectory: dict = field(default_factory=dict)
    start_time: str = ""
    end_time: str = ""
    total_latency: float = 0.0


def build_system_prompt(policy: str, tools: list[dict]) -> str:
    """Build the system prompt with policy and tool definitions."""
    tool_section = json.dumps(tools, indent=2, ensure_ascii=False)
    
    return f"""You are a medical AI assistant operating in a clinical environment. Follow the policy below and use the available tools to help with patient care.

## Policy
{policy}

## Available Tools
You have access to the following tools. To use a tool, respond with ONLY a JSON object in this exact format:
```json
{{"name": "tool_name", "arguments": {{"arg1": "value1", "arg2": "value2"}}}}
```

Do NOT include any other text when making a tool call. One tool call per response.

When you have gathered enough information and want to give your final assessment, respond with your clinical analysis as plain text (no JSON).

## Tool Definitions
{tool_section}"""


def parse_tool_call(text: str) -> Optional[dict]:
    """Parse a tool call from model output.
    
    Supports formats:
    1. Pure JSON: {"name": "...", "arguments": {...}}
    2. JSON in code block: ```json\n{...}\n```
    3. JSON embedded in text with markers
    """
    text = text.strip()
    
    # Try 1: Extract JSON from code blocks
    code_block_match = re.search(r'```(?:json)?\s*\n?({.*?})\s*\n?```', text, re.DOTALL)
    if code_block_match:
        try:
            parsed = json.loads(code_block_match.group(1))
            if "name" in parsed:
                return parsed
        except json.JSONDecodeError:
            pass
    
    # Try 2: Direct JSON parse
    try:
        parsed = json.loads(text)
        if isinstance(parsed, dict) and "name" in parsed:
            return parsed
    except json.JSONDecodeError:
        pass
    
    # Try 3: Find JSON-like pattern in text
    json_match = re.search(r'(\{[^{}]*"name"\s*:\s*"[^"]+?"[^{}]*"arguments"\s*:\s*\{[^{}]*\}[^{}]*\})', text, re.DOTALL)
    if json_match:
        try:
            parsed = json.loads(json_match.group(1))
            if "name" in parsed:
                return parsed
        except json.JSONDecodeError:
            pass
    
    # Try 4: More lenient nested JSON search
    matches = list(re.finditer(r'\{', text))
    for m in matches:
        start = m.start()
        depth = 0
        for i in range(start, len(text)):
            if text[i] == '{':
                depth += 1
            elif text[i] == '}':
                depth -= 1
                if depth == 0:
                    candidate = text[start:i+1]
                    try:
                        parsed = json.loads(candidate)
                        if isinstance(parsed, dict) and "name" in parsed:
                            return parsed
                    except json.JSONDecodeError:
                        pass
                    break
    
    return None


class AgentRunner:
    """Runs LLM agents in BIOAgents environments."""
    
    def __init__(self, config: RunConfig):
        self.config = config
        self.model = None
        self.tokenizer = None
        self._setup_logging()
    
    def _setup_logging(self):
        """Set up logging directory."""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        model_short = Path(self.config.model_name_or_path).name
        self.run_id = f"{model_short}_{self.config.domain}_{timestamp}"
        self.log_path = Path(self.config.log_dir) / self.run_id
        self.log_path.mkdir(parents=True, exist_ok=True)
        
        # Save config
        config_dict = {
            k: v for k, v in self.config.__dict__.items()
        }
        with open(self.log_path / "config.json", "w") as f:
            json.dump(config_dict, f, indent=2, default=str)
    
    def load_model(self):
        """Load the language model."""
        if self.config.backend == "vllm":
            self._load_vllm()
        else:
            self._load_transformers()
    
    def _load_vllm(self):
        """Load model with vLLM."""
        from vllm import LLM, SamplingParams
        
        logger.info(f"Loading {self.config.model_name_or_path} with vLLM (tp={self.config.tensor_parallel_size})")
        self.model = LLM(
            model=self.config.model_name_or_path,
            tensor_parallel_size=self.config.tensor_parallel_size,
            gpu_memory_utilization=self.config.gpu_memory_utilization,
            trust_remote_code=True,
            max_model_len=8192,
            seed=self.config.seed,
        )
        self.sampling_params = SamplingParams(
            temperature=self.config.temperature,
            top_p=self.config.top_p,
            max_tokens=self.config.max_new_tokens,
            stop=["```\n\n", "\n\nUser:", "\n\nHuman:"],
        )
        logger.info("vLLM model loaded successfully")
    
    def _load_transformers(self):
        """Load model with HuggingFace transformers."""
        import torch
        from transformers import (
            AutoModelForCausalLM,
            AutoTokenizer,
            AutoProcessor,
            AutoConfig,
        )
        
        logger.info(f"Loading {self.config.model_name_or_path} with transformers")
        
        model_path = self.config.model_name_or_path
        
        # Detect model type from config
        config = AutoConfig.from_pretrained(model_path, trust_remote_code=True)
        model_type = getattr(config, "model_type", "")
        architectures = getattr(config, "architectures", [])
        
        self._is_vl_model = any(
            "vl" in a.lower() or "vision" in a.lower()
            for a in (architectures or [])
        ) or "vl" in model_type.lower()
        
        logger.info(f"Model type: {model_type}, VL: {self._is_vl_model}, Arch: {architectures}")
        
        # Load tokenizer / processor
        if self._is_vl_model:
            try:
                self.processor = AutoProcessor.from_pretrained(
                    model_path, trust_remote_code=True
                )
                self.tokenizer = (
                    self.processor.tokenizer
                    if hasattr(self.processor, "tokenizer")
                    else self.processor
                )
            except Exception:
                self.tokenizer = AutoTokenizer.from_pretrained(
                    model_path, trust_remote_code=True
                )
                self.processor = None
        else:
            self.tokenizer = AutoTokenizer.from_pretrained(
                model_path, trust_remote_code=True
            )
            self.processor = None
        
        # Load model â€“ choose the right Auto class
        load_kwargs = dict(
            torch_dtype=torch.bfloat16,
            device_map="auto",
            trust_remote_code=True,
        )
        
        # Try sdpa first, then fall back
        for attn_impl in ["sdpa", None]:
            try:
                kw = {**load_kwargs}
                if attn_impl:
                    kw["attn_implementation"] = attn_impl
                
                if self._is_vl_model:
                    # VL models need their specific class
                    from transformers import Qwen2_5_VLForConditionalGeneration
                    self.model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
                        model_path, **kw
                    )
                else:
                    self.model = AutoModelForCausalLM.from_pretrained(
                        model_path, **kw
                    )
                break
            except Exception as e:
                logger.warning(f"Failed with attn_impl={attn_impl}: {e}")
                if attn_impl is None:
                    raise
        
        self.model.eval()
        logger.info(f"Model loaded (VL={self._is_vl_model})")
    
    def generate(self, messages: list[dict]) -> str:
        """Generate a response from the model."""
        if self.config.backend == "vllm":
            return self._generate_vllm(messages)
        else:
            return self._generate_transformers(messages)
    
    def _generate_vllm(self, messages: list[dict]) -> str:
        """Generate with vLLM using chat template."""
        from vllm import SamplingParams
        
        # Use the tokenizer from vLLM engine
        tokenizer = self.model.get_tokenizer()
        prompt = tokenizer.apply_chat_template(
            messages, tokenize=False, add_generation_prompt=True
        )
        
        outputs = self.model.generate([prompt], self.sampling_params)
        return outputs[0].outputs[0].text.strip()
    
    def _generate_transformers(self, messages: list[dict]) -> str:
        """Generate with HuggingFace transformers."""
        import torch
        
        # For VL models, use processor if available
        if self._is_vl_model and self.processor is not None:
            # Text-only input for VL model
            text = self.processor.apply_chat_template(
                messages, tokenize=False, add_generation_prompt=True
            )
            inputs = self.processor(
                text=[text], padding=True, return_tensors="pt"
            ).to(self.model.device)
        else:
            # Apply chat template
            if hasattr(self.tokenizer, "apply_chat_template"):
                text = self.tokenizer.apply_chat_template(
                    messages, tokenize=False, add_generation_prompt=True
                )
            else:
                text = ""
                for msg in messages:
                    role = msg["role"]
                    content = msg["content"]
                    text += f"<|{role}|>\n{content}\n"
                text += "<|assistant|>\n"
            
            inputs = self.tokenizer(text, return_tensors="pt").to(self.model.device)
        
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=self.config.max_new_tokens,
                temperature=self.config.temperature if self.config.temperature > 0 else None,
                top_p=self.config.top_p if self.config.temperature > 0 else None,
                do_sample=self.config.temperature > 0,
                pad_token_id=self.tokenizer.eos_token_id,
            )
        
        # Decode only the generated tokens
        input_len = inputs["input_ids"].shape[-1]
        generated_ids = outputs[0][input_len:]
        return self.tokenizer.decode(generated_ids, skip_special_tokens=True).strip()
    
    def run_task(self, task: dict, env) -> TaskResult:
        """Run a single task in the environment.
        
        Args:
            task: Task dictionary from tasks.json
            env: BioAgentGymEnv instance
            
        Returns:
            TaskResult with full trajectory and scores
        """
        task_id = task["id"]
        result = TaskResult(
            task_id=task_id,
            domain=self.config.domain,
            model_name=Path(self.config.model_name_or_path).name,
            start_time=datetime.now().isoformat(),
        )
        
        logger.info(f"Starting task: {task_id}")
        
        # Reset environment
        obs, info = env.reset(options={"task_id": task_id})
        
        # Build conversation
        system_prompt = build_system_prompt(info["policy"], info["tools"])
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": obs},
        ]
        
        try:
            for turn_idx in range(self.config.max_turns):
                turn = TurnRecord(turn_idx=turn_idx)
                
                # Generate
                t0 = time.time()
                raw_output = self.generate(messages)
                turn.latency_seconds = time.time() - t0
                turn.raw_output = raw_output
                
                logger.debug(f"Turn {turn_idx}: {raw_output[:200]}...")
                
                # Parse tool call
                tool_call = parse_tool_call(raw_output)
                
                if tool_call is not None:
                    turn.parsed_tool_call = tool_call
                    
                    # Detect repeated tool call (stuck model)
                    recent_tool_calls = [
                        t.parsed_tool_call.get("name", "") if t.parsed_tool_call else ""
                        for t in result.turns[-3:]
                    ]
                    if recent_tool_calls.count(tool_call["name"]) >= 2:
                        logger.warning(
                            f"Tool '{tool_call['name']}' called 3+ times in a row. "
                            "Injecting nudge to move forward."
                        )
                        # Instead of stopping, nudge the model
                        messages.append({"role": "assistant", "content": json.dumps(tool_call)})
                        messages.append({
                            "role": "user",
                            "content": (
                                f"You have already called '{tool_call['name']}' multiple times with similar arguments. "
                                "Please proceed with your analysis using the information gathered so far. "
                                "Use a DIFFERENT tool or provide your final clinical assessment."
                            ),
                        })
                        result.turns.append(turn)
                        continue
                    
                    # Execute tool via environment
                    action = json.dumps(tool_call)
                    observation, reward, terminated, truncated, step_info = env.step(action)
                    
                    turn.tool_response = observation
                    
                    # Add to messages - only the tool call JSON part
                    messages.append({"role": "assistant", "content": json.dumps(tool_call)})
                    messages.append({
                        "role": "user",
                        "content": f"Tool result for {tool_call['name']}:\n{observation}"
                    })
                    
                    result.turns.append(turn)
                    
                    if terminated or truncated:
                        break
                else:
                    # Final answer (no tool call)
                    turn.is_final_answer = True
                    messages.append({"role": "assistant", "content": raw_output})
                    result.turns.append(turn)
                    
                    # Check for repetition (model stuck in a loop)
                    if len(result.turns) >= 3:
                        recent = [t.raw_output[:100] for t in result.turns[-3:]]
                        if len(set(recent)) == 1:
                            logger.warning(f"Repetition detected at turn {turn_idx}, stopping.")
                            break
                    
                    # Mark as terminated - agent gave final answer
                    env._env.state.terminated = True if hasattr(env, '_env') else None
                    break
            
            # Get final trajectory and reward
            try:
                trajectory = env.get_trajectory()
                result.trajectory = trajectory
                result.final_reward = trajectory["final_reward"]
            except Exception:
                result.trajectory = {}
                result.final_reward = 0.0
            
            result.total_turns = len(result.turns)
            result.action_score = self._compute_action_score(task, env._tool_call_log)
            result.completed = True
            
        except Exception as e:
            logger.error(f"Error in task {task_id}: {e}")
            result.error = str(e)
            import traceback
            result.error += "\n" + traceback.format_exc()
        
        result.end_time = datetime.now().isoformat()
        result.total_latency = sum(t.latency_seconds for t in result.turns)
        
        logger.info(
            f"Task {task_id}: turns={result.total_turns}, "
            f"action_score={result.action_score:.3f}, "
            f"reward={result.final_reward:.3f}, "
            f"latency={result.total_latency:.1f}s"
        )
        
        return result
    
    def _compute_action_score(self, task: dict, tool_call_log: list) -> float:
        """Compute action score based on expected vs actual tool calls."""
        eval_criteria = task.get("evaluation_criteria", {})
        expected_actions = eval_criteria.get("actions", [])
        
        if not expected_actions:
            return 1.0
        
        matched = 0
        for exp in expected_actions:
            exp_name = exp.get("name", "")
            compare_args = exp.get("compare_args", [])
            exp_args = exp.get("arguments", {})
            
            for tc in tool_call_log:
                if tc["tool_name"] == exp_name:
                    if compare_args:
                        all_match = all(
                            str(tc["arguments"].get(k, "")).lower() == str(exp_args.get(k, "")).lower()
                            for k in compare_args
                            if k in exp_args
                        )
                        if all_match:
                            matched += 1
                            break
                    else:
                        matched += 1
                        break
        
        return matched / len(expected_actions)
    
    def run_all_tasks(self) -> list[TaskResult]:
        """Run all tasks (or specified task_ids) and return results."""
        from bioagents.gym.agent_env import BioAgentGymEnv
        
        # Create environment
        gym_env = BioAgentGymEnv(
            domain=self.config.domain,
            task_split=self.config.task_split,
            max_turns=self.config.max_turns,
        )
        
        # Get tasks
        if self.config.task_ids:
            task_ids = self.config.task_ids
        else:
            task_ids = [t["id"] for t in gym_env._tasks]
        
        logger.info(f"Running {len(task_ids)} tasks with {Path(self.config.model_name_or_path).name}")
        
        results = []
        for task_id in task_ids:
            task = gym_env._task_map[task_id]
            result = self.run_task(task, gym_env)
            results.append(result)
            self._save_task_result(result)
        
        # Save summary
        self._save_summary(results)
        
        return results
    
    def _save_task_result(self, result: TaskResult):
        """Save individual task result."""
        path = self.log_path / f"task_{result.task_id}.json"
        data = {
            "task_id": result.task_id,
            "domain": result.domain,
            "model_name": result.model_name,
            "total_turns": result.total_turns,
            "action_score": result.action_score,
            "final_reward": result.final_reward,
            "completed": result.completed,
            "error": result.error,
            "total_latency": result.total_latency,
            "start_time": result.start_time,
            "end_time": result.end_time,
            "turns": [
                {
                    "turn_idx": t.turn_idx,
                    "raw_output": t.raw_output,
                    "parsed_tool_call": t.parsed_tool_call,
                    "tool_response": t.tool_response[:500] if t.tool_response else None,
                    "is_final_answer": t.is_final_answer,
                    "latency_seconds": t.latency_seconds,
                }
                for t in result.turns
            ],
        }
        with open(path, "w", encoding="utf-8") as f:
            json.dump(data, f, indent=2, ensure_ascii=False, default=str)
    
    def _save_summary(self, results: list[TaskResult]):
        """Save a summary of all task results."""
        summary = {
            "run_id": self.run_id,
            "model": Path(self.config.model_name_or_path).name,
            "domain": self.config.domain,
            "backend": self.config.backend,
            "num_tasks": len(results),
            "num_completed": sum(1 for r in results if r.completed),
            "num_errors": sum(1 for r in results if r.error),
            "avg_action_score": sum(r.action_score for r in results) / max(len(results), 1),
            "avg_reward": sum(r.final_reward for r in results) / max(len(results), 1),
            "avg_turns": sum(r.total_turns for r in results) / max(len(results), 1),
            "total_latency": sum(r.total_latency for r in results),
            "per_task": [
                {
                    "task_id": r.task_id,
                    "action_score": r.action_score,
                    "final_reward": r.final_reward,
                    "turns": r.total_turns,
                    "latency": r.total_latency,
                    "completed": r.completed,
                    "error": r.error is not None,
                }
                for r in results
            ],
            "timestamp": datetime.now().isoformat(),
        }
        
        path = self.log_path / "summary.json"
        with open(path, "w", encoding="utf-8") as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)
        
        # Print summary table
        print("\n" + "=" * 80)
        print(f"  RUN SUMMARY: {self.run_id}")
        print("=" * 80)
        print(f"  Model: {summary['model']}")
        print(f"  Domain: {summary['domain']}")
        print(f"  Backend: {summary['backend']}")
        print(f"  Tasks: {summary['num_completed']}/{summary['num_tasks']} completed")
        print(f"  Avg Action Score: {summary['avg_action_score']:.3f}")
        print(f"  Avg Reward: {summary['avg_reward']:.3f}")
        print(f"  Avg Turns: {summary['avg_turns']:.1f}")
        print(f"  Total Latency: {summary['total_latency']:.1f}s")
        print("-" * 80)
        print(f"  {'Task ID':<30} {'Score':>8} {'Reward':>8} {'Turns':>6} {'Time':>8}")
        print("-" * 80)
        for t in summary["per_task"]:
            status = "âœ“" if t["completed"] else "âœ—"
            print(f"  {status} {t['task_id']:<28} {t['action_score']:>8.3f} {t['final_reward']:>8.3f} {t['turns']:>6} {t['latency']:>7.1f}s")
        print("=" * 80)
        
        logger.info(f"Results saved to {self.log_path}")

```

ì´ì œ ê¸°ì¡´ ì‹¤í–‰ ì¤‘ì¸ Lingshuì™€ Qwen3ì„ ê¸°ë‹¤ë¦¬ë©´ì„œ, ìˆ˜ì •ëœ ì½”ë“œë¡œ ì¬ì‹¤í–‰í•  ì¤€ë¹„ë¥¼ í•©ë‹ˆë‹¤. ë¨¼ì € ì•„ì§ ì‹¤í–‰ ì¤‘ì¸ í”„ë¡œì„¸ìŠ¤ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.

Qwen3ì™€ Lingshuê°€ ì•„ì§ ì‹¤í–‰ ì¤‘(Qwen3: GPU0 42GB, Lingshu: GPU1 19GB). ì´ í”„ë¡œì„¸ìŠ¤ë“¤ì€ ì´ì „ ì½”ë“œë¡œ ëŒê³  ìˆìœ¼ë‹ˆ, ëë‚  ë•Œê¹Œì§€ ê¸°ë‹¤ë¦¬ê³  ê²°ê³¼ë¥¼ ë³¸ í›„ ìˆ˜ì •ëœ ì½”ë“œë¡œ ì¬ì‹¤í–‰í•˜ê² ìŠµë‹ˆë‹¤.

ê¸°ë‹¤ë¦¬ëŠ” ë™ì•ˆ, Qwen2.5-VL-7Bì˜ ì—ëŸ¬ ì›ì¸(`'BioAgentGymEnv' has no attribute 'state'`)ì„ ë¶„ì„í•˜ê³  ìˆ˜ì •í•©ë‹ˆë‹¤.

```python
"""LLM Agent Runner for BIOAgents.

Connects a language model to the BIOAgents environment for multi-turn
tool-use evaluation. Supports:
- vLLM-based fast inference
- HuggingFace transformers-based inference
- Multi-turn tool calling with automatic parsing
- Full trajectory logging

Usage:
    runner = AgentRunner(
        model_name_or_path="Qwen/Qwen2.5-VL-7B-Instruct",
        backend="vllm",
    )
    results = runner.run_task(domain="clinical_diagnosis", task_id="dx_pneumonia_001")
"""

import json
import os
import re
import time
from copy import deepcopy
from dataclasses import dataclass, field
from datetime import datetime
from pathlib import Path
from typing import Any, Literal, Optional

from loguru import logger


@dataclass
class RunConfig:
    """Configuration for an agent run."""
    model_name_or_path: str
    backend: Literal["vllm", "transformers"] = "transformers"
    domain: str = "clinical_diagnosis"
    task_ids: Optional[list[str]] = None       # None = run all tasks
    task_split: Optional[str] = None
    max_turns: int = 15
    temperature: float = 0.1
    top_p: float = 0.95
    max_new_tokens: int = 1024
    tensor_parallel_size: int = 1
    gpu_memory_utilization: float = 0.85
    log_dir: str = "logs/runs"
    seed: int = 42


@dataclass
class TurnRecord:
    """Record of a single turn in the agent-environment interaction."""
    turn_idx: int
    prompt: str = ""
    raw_output: str = ""
    parsed_tool_call: Optional[dict] = None
    tool_response: Optional[str] = None
    is_final_answer: bool = False
    latency_seconds: float = 0.0


@dataclass 
class TaskResult:
    """Result of running a single task."""
    task_id: str
    domain: str
    model_name: str
    turns: list[TurnRecord] = field(default_factory=list)
    total_turns: int = 0
    action_score: float = 0.0
    final_reward: float = 0.0
    completed: bool = False
    error: Optional[str] = None
    trajectory: dict = field(default_factory=dict)
    start_time: str = ""
    end_time: str = ""
    total_latency: float = 0.0


def build_system_prompt(policy: str, tools: list[dict]) -> str:
    """Build the system prompt with policy and tool definitions."""
    tool_section = json.dumps(tools, indent=2, ensure_ascii=False)
    
    return f"""You are a medical AI assistant operating in a clinical environment. Follow the policy below and use the available tools to help with patient care.

## Policy
{policy}

## Available Tools
You have access to the following tools. To use a tool, respond with ONLY a JSON object in this exact format:
```json
{{"name": "tool_name", "arguments": {{"arg1": "value1", "arg2": "value2"}}}}
```

Do NOT include any other text when making a tool call. One tool call per response.

When you have gathered enough information and want to give your final assessment, respond with your clinical analysis as plain text (no JSON).

## Tool Definitions
{tool_section}"""


def parse_tool_call(text: str) -> Optional[dict]:
    """Parse a tool call from model output.
    
    Supports formats:
    1. Pure JSON: {"name": "...", "arguments": {...}}
    2. JSON in code block: ```json\n{...}\n```
    3. JSON embedded in text with markers
    """
    text = text.strip()
    
    # Try 1: Extract JSON from code blocks
    code_block_match = re.search(r'```(?:json)?\s*\n?({.*?})\s*\n?```', text, re.DOTALL)
    if code_block_match:
        try:
            parsed = json.loads(code_block_match.group(1))
            if "name" in parsed:
                return parsed
        except json.JSONDecodeError:
            pass
    
    # Try 2: Direct JSON parse
    try:
        parsed = json.loads(text)
        if isinstance(parsed, dict) and "name" in parsed:
            return parsed
    except json.JSONDecodeError:
        pass
    
    # Try 3: Find JSON-like pattern in text
    json_match = re.search(r'(\{[^{}]*"name"\s*:\s*"[^"]+?"[^{}]*"arguments"\s*:\s*\{[^{}]*\}[^{}]*\})', text, re.DOTALL)
    if json_match:
        try:
            parsed = json.loads(json_match.group(1))
            if "name" in parsed:
                return parsed
        except json.JSONDecodeError:
            pass
    
    # Try 4: More lenient nested JSON search
    matches = list(re.finditer(r'\{', text))
    for m in matches:
        start = m.start()
        depth = 0
        for i in range(start, len(text)):
            if text[i] == '{':
                depth += 1
            elif text[i] == '}':
                depth -= 1
                if depth == 0:
                    candidate = text[start:i+1]
                    try:
                        parsed = json.loads(candidate)
                        if isinstance(parsed, dict) and "name" in parsed:
                            return parsed
                    except json.JSONDecodeError:
                        pass
                    break
    
    return None


class AgentRunner:
    """Runs LLM agents in BIOAgents environments."""
    
    def __init__(self, config: RunConfig):
        self.config = config
        self.model = None
        self.tokenizer = None
        self._setup_logging()
    
    def _setup_logging(self):
        """Set up logging directory."""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        model_short = Path(self.config.model_name_or_path).name
        self.run_id = f"{model_short}_{self.config.domain}_{timestamp}"
        self.log_path = Path(self.config.log_dir) / self.run_id
        self.log_path.mkdir(parents=True, exist_ok=True)
        
        # Save config
        config_dict = {
            k: v for k, v in self.config.__dict__.items()
        }
        with open(self.log_path / "config.json", "w") as f:
            json.dump(config_dict, f, indent=2, default=str)
    
    def load_model(self):
        """Load the language model."""
        if self.config.backend == "vllm":
            self._load_vllm()
        else:
            self._load_transformers()
    
    def _load_vllm(self):
        """Load model with vLLM."""
        from vllm import LLM, SamplingParams
        
        logger.info(f"Loading {self.config.model_name_or_path} with vLLM (tp={self.config.tensor_parallel_size})")
        self.model = LLM(
            model=self.config.model_name_or_path,
            tensor_parallel_size=self.config.tensor_parallel_size,
            gpu_memory_utilization=self.config.gpu_memory_utilization,
            trust_remote_code=True,
            max_model_len=8192,
            seed=self.config.seed,
        )
        self.sampling_params = SamplingParams(
            temperature=self.config.temperature,
            top_p=self.config.top_p,
            max_tokens=self.config.max_new_tokens,
            stop=["```\n\n", "\n\nUser:", "\n\nHuman:"],
        )
        logger.info("vLLM model loaded successfully")
    
    def _load_transformers(self):
        """Load model with HuggingFace transformers."""
        import torch
        from transformers import (
            AutoModelForCausalLM,
            AutoTokenizer,
            AutoProcessor,
            AutoConfig,
        )
        
        logger.info(f"Loading {self.config.model_name_or_path} with transformers")
        
        model_path = self.config.model_name_or_path
        
        # Detect model type from config
        config = AutoConfig.from_pretrained(model_path, trust_remote_code=True)
        model_type = getattr(config, "model_type", "")
        architectures = getattr(config, "architectures", [])
        
        self._is_vl_model = any(
            "vl" in a.lower() or "vision" in a.lower()
            for a in (architectures or [])
        ) or "vl" in model_type.lower()
        
        logger.info(f"Model type: {model_type}, VL: {self._is_vl_model}, Arch: {architectures}")
        
        # Load tokenizer / processor
        if self._is_vl_model:
            try:
                self.processor = AutoProcessor.from_pretrained(
                    model_path, trust_remote_code=True
                )
                self.tokenizer = (
                    self.processor.tokenizer
                    if hasattr(self.processor, "tokenizer")
                    else self.processor
                )
            except Exception:
                self.tokenizer = AutoTokenizer.from_pretrained(
                    model_path, trust_remote_code=True
                )
                self.processor = None
        else:
            self.tokenizer = AutoTokenizer.from_pretrained(
                model_path, trust_remote_code=True
            )
            self.processor = None
        
        # Load model â€“ choose the right Auto class
        load_kwargs = dict(
            torch_dtype=torch.bfloat16,
            device_map="auto",
            trust_remote_code=True,
        )
        
        # Try sdpa first, then fall back
        for attn_impl in ["sdpa", None]:
            try:
                kw = {**load_kwargs}
                if attn_impl:
                    kw["attn_implementation"] = attn_impl
                
                if self._is_vl_model:
                    # VL models need their specific class
                    from transformers import Qwen2_5_VLForConditionalGeneration
                    self.model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
                        model_path, **kw
                    )
                else:
                    self.model = AutoModelForCausalLM.from_pretrained(
                        model_path, **kw
                    )
                break
            except Exception as e:
                logger.warning(f"Failed with attn_impl={attn_impl}: {e}")
                if attn_impl is None:
                    raise
        
        self.model.eval()
        logger.info(f"Model loaded (VL={self._is_vl_model})")
    
    def generate(self, messages: list[dict]) -> str:
        """Generate a response from the model."""
        if self.config.backend == "vllm":
            return self._generate_vllm(messages)
        else:
            return self._generate_transformers(messages)
    
    def _generate_vllm(self, messages: list[dict]) -> str:
        """Generate with vLLM using chat template."""
        from vllm import SamplingParams
        
        # Use the tokenizer from vLLM engine
        tokenizer = self.model.get_tokenizer()
        prompt = tokenizer.apply_chat_template(
            messages, tokenize=False, add_generation_prompt=True
        )
        
        outputs = self.model.generate([prompt], self.sampling_params)
        return outputs[0].outputs[0].text.strip()
    
    def _generate_transformers(self, messages: list[dict]) -> str:
        """Generate with HuggingFace transformers."""
        import torch
        
        # For VL models, use processor if available
        if self._is_vl_model and self.processor is not None:
            # Text-only input for VL model
            text = self.processor.apply_chat_template(
                messages, tokenize=False, add_generation_prompt=True
            )
            inputs = self.processor(
                text=[text], padding=True, return_tensors="pt"
            ).to(self.model.device)
        else:
            # Apply chat template
            if hasattr(self.tokenizer, "apply_chat_template"):
                text = self.tokenizer.apply_chat_template(
                    messages, tokenize=False, add_generation_prompt=True
                )
            else:
                text = ""
                for msg in messages:
                    role = msg["role"]
                    content = msg["content"]
                    text += f"<|{role}|>\n{content}\n"
                text += "<|assistant|>\n"
            
            inputs = self.tokenizer(text, return_tensors="pt").to(self.model.device)
        
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=self.config.max_new_tokens,
                temperature=self.config.temperature if self.config.temperature > 0 else None,
                top_p=self.config.top_p if self.config.temperature > 0 else None,
                do_sample=self.config.temperature > 0,
                pad_token_id=self.tokenizer.eos_token_id,
            )
        
        # Decode only the generated tokens
        input_len = inputs["input_ids"].shape[-1]
        generated_ids = outputs[0][input_len:]
        return self.tokenizer.decode(generated_ids, skip_special_tokens=True).strip()
    
    def run_task(self, task: dict, env) -> TaskResult:
        """Run a single task in the environment.
        
        Args:
            task: Task dictionary from tasks.json
            env: BioAgentGymEnv instance
            
        Returns:
            TaskResult with full trajectory and scores
        """
        task_id = task["id"]
        result = TaskResult(
            task_id=task_id,
            domain=self.config.domain,
            model_name=Path(self.config.model_name_or_path).name,
            start_time=datetime.now().isoformat(),
        )
        
        logger.info(f"Starting task: {task_id}")
        
        # Reset environment
        obs, info = env.reset(options={"task_id": task_id})
        
        # Build conversation
        system_prompt = build_system_prompt(info["policy"], info["tools"])
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": obs},
        ]
        
        try:
            for turn_idx in range(self.config.max_turns):
                turn = TurnRecord(turn_idx=turn_idx)
                
                # Generate
                t0 = time.time()
                raw_output = self.generate(messages)
                turn.latency_seconds = time.time() - t0
                turn.raw_output = raw_output
                
                logger.debug(f"Turn {turn_idx}: {raw_output[:200]}...")
                
                # Parse tool call
                tool_call = parse_tool_call(raw_output)
                
                if tool_call is not None:
                    turn.parsed_tool_call = tool_call
                    
                    # Detect repeated tool call (stuck model)
                    recent_tool_calls = [
                        t.parsed_tool_call.get("name", "") if t.parsed_tool_call else ""
                        for t in result.turns[-3:]
                    ]
                    if recent_tool_calls.count(tool_call["name"]) >= 2:
                        logger.warning(
                            f"Tool '{tool_call['name']}' called 3+ times in a row. "
                            "Injecting nudge to move forward."
                        )
                        # Instead of stopping, nudge the model
                        messages.append({"role": "assistant", "content": json.dumps(tool_call)})
                        messages.append({
                            "role": "user",
                            "content": (
                                f"You have already called '{tool_call['name']}' multiple times with similar arguments. "
                                "Please proceed with your analysis using the information gathered so far. "
                                "Use a DIFFERENT tool or provide your final clinical assessment."
                            ),
                        })
                        result.turns.append(turn)
                        continue
                    
                    # Execute tool via environment
                    action = json.dumps(tool_call)
                    observation, reward, terminated, truncated, step_info = env.step(action)
                    
                    turn.tool_response = observation
                    
                    # Add to messages - only the tool call JSON part
                    messages.append({"role": "assistant", "content": json.dumps(tool_call)})
                    messages.append({
                        "role": "user",
                        "content": f"Tool result for {tool_call['name']}:\n{observation}"
                    })
                    
                    result.turns.append(turn)
                    
                    if terminated or truncated:
                        break
                else:
                    # Final answer (no tool call)
                    turn.is_final_answer = True
                    messages.append({"role": "assistant", "content": raw_output})
                    result.turns.append(turn)
                    
                    # Check for repetition (model stuck in a loop)
                    if len(result.turns) >= 3:
                        recent = [t.raw_output[:100] for t in result.turns[-3:]]
                        if len(set(recent)) == 1:
                            logger.warning(f"Repetition detected at turn {turn_idx}, stopping.")
                            break
                    
                    # Agent gave final answer - break the loop
                    break
            
            # Get final trajectory and reward
            try:
                trajectory = env.get_trajectory()
                result.trajectory = trajectory
                result.final_reward = trajectory["final_reward"]
            except Exception:
                result.trajectory = {}
                result.final_reward = 0.0
            
            result.total_turns = len(result.turns)
            result.action_score = self._compute_action_score(task, env._tool_call_log)
            result.completed = True
            
        except Exception as e:
            logger.error(f"Error in task {task_id}: {e}")
            result.error = str(e)
            import traceback
            result.error += "\n" + traceback.format_exc()
        
        result.end_time = datetime.now().isoformat()
        result.total_latency = sum(t.latency_seconds for t in result.turns)
        
        logger.info(
            f"Task {task_id}: turns={result.total_turns}, "
            f"action_score={result.action_score:.3f}, "
            f"reward={result.final_reward:.3f}, "
            f"latency={result.total_latency:.1f}s"
        )
        
        return result
    
    def _compute_action_score(self, task: dict, tool_call_log: list) -> float:
        """Compute action score based on expected vs actual tool calls."""
        eval_criteria = task.get("evaluation_criteria", {})
        expected_actions = eval_criteria.get("actions", [])
        
        if not expected_actions:
            return 1.0
        
        matched = 0
        for exp in expected_actions:
            exp_name = exp.get("name", "")
            compare_args = exp.get("compare_args", [])
            exp_args = exp.get("arguments", {})
            
            for tc in tool_call_log:
                if tc["tool_name"] == exp_name:
                    if compare_args:
                        all_match = all(
                            str(tc["arguments"].get(k, "")).lower() == str(exp_args.get(k, "")).lower()
                            for k in compare_args
                            if k in exp_args
                        )
                        if all_match:
                            matched += 1
                            break
                    else:
                        matched += 1
                        break
        
        return matched / len(expected_actions)
    
    def run_all_tasks(self) -> list[TaskResult]:
        """Run all tasks (or specified task_ids) and return results."""
        from bioagents.gym.agent_env import BioAgentGymEnv
        
        # Create environment
        gym_env = BioAgentGymEnv(
            domain=self.config.domain,
            task_split=self.config.task_split,
            max_turns=self.config.max_turns,
        )
        
        # Get tasks
        if self.config.task_ids:
            task_ids = self.config.task_ids
        else:
            task_ids = [t["id"] for t in gym_env._tasks]
        
        logger.info(f"Running {len(task_ids)} tasks with {Path(self.config.model_name_or_path).name}")
        
        results = []
        for task_id in task_ids:
            task = gym_env._task_map[task_id]
            result = self.run_task(task, gym_env)
            results.append(result)
            self._save_task_result(result)
        
        # Save summary
        self._save_summary(results)
        
        return results
    
    def _save_task_result(self, result: TaskResult):
        """Save individual task result."""
        path = self.log_path / f"task_{result.task_id}.json"
        data = {
            "task_id": result.task_id,
            "domain": result.domain,
            "model_name": result.model_name,
            "total_turns": result.total_turns,
            "action_score": result.action_score,
            "final_reward": result.final_reward,
            "completed": result.completed,
            "error": result.error,
            "total_latency": result.total_latency,
            "start_time": result.start_time,
            "end_time": result.end_time,
            "turns": [
                {
                    "turn_idx": t.turn_idx,
                    "raw_output": t.raw_output,
                    "parsed_tool_call": t.parsed_tool_call,
                    "tool_response": t.tool_response[:500] if t.tool_response else None,
                    "is_final_answer": t.is_final_answer,
                    "latency_seconds": t.latency_seconds,
                }
                for t in result.turns
            ],
        }
        with open(path, "w", encoding="utf-8") as f:
            json.dump(data, f, indent=2, ensure_ascii=False, default=str)
    
    def _save_summary(self, results: list[TaskResult]):
        """Save a summary of all task results."""
        summary = {
            "run_id": self.run_id,
            "model": Path(self.config.model_name_or_path).name,
            "domain": self.config.domain,
            "backend": self.config.backend,
            "num_tasks": len(results),
            "num_completed": sum(1 for r in results if r.completed),
            "num_errors": sum(1 for r in results if r.error),
            "avg_action_score": sum(r.action_score for r in results) / max(len(results), 1),
            "avg_reward": sum(r.final_reward for r in results) / max(len(results), 1),
            "avg_turns": sum(r.total_turns for r in results) / max(len(results), 1),
            "total_latency": sum(r.total_latency for r in results),
            "per_task": [
                {
                    "task_id": r.task_id,
                    "action_score": r.action_score,
                    "final_reward": r.final_reward,
                    "turns": r.total_turns,
                    "latency": r.total_latency,
                    "completed": r.completed,
                    "error": r.error is not None,
                }
                for r in results
            ],
            "timestamp": datetime.now().isoformat(),
        }
        
        path = self.log_path / "summary.json"
        with open(path, "w", encoding="utf-8") as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)
        
        # Print summary table
        print("\n" + "=" * 80)
        print(f"  RUN SUMMARY: {self.run_id}")
        print("=" * 80)
        print(f"  Model: {summary['model']}")
        print(f"  Domain: {summary['domain']}")
        print(f"  Backend: {summary['backend']}")
        print(f"  Tasks: {summary['num_completed']}/{summary['num_tasks']} completed")
        print(f"  Avg Action Score: {summary['avg_action_score']:.3f}")
        print(f"  Avg Reward: {summary['avg_reward']:.3f}")
        print(f"  Avg Turns: {summary['avg_turns']:.1f}")
        print(f"  Total Latency: {summary['total_latency']:.1f}s")
        print("-" * 80)
        print(f"  {'Task ID':<30} {'Score':>8} {'Reward':>8} {'Turns':>6} {'Time':>8}")
        print("-" * 80)
        for t in summary["per_task"]:
            status = "âœ“" if t["completed"] else "âœ—"
            print(f"  {status} {t['task_id']:<28} {t['action_score']:>8.3f} {t['final_reward']:>8.3f} {t['turns']:>6} {t['latency']:>7.1f}s")
        print("=" * 80)
        
        logger.info(f"Results saved to {self.log_path}")

```

ê¸°ì¡´ ì‹¤í–‰ì´ ëë‚¬ëŠ”ì§€ ë‹¤ì‹œ í™•ì¸í•©ë‹ˆë‹¤.

LingshuëŠ” ê³„ì† ë°˜ë³µì— ë¹ ì§€ê³  ìˆìŠµë‹ˆë‹¤ (old codeë¡œ ì‹¤í–‰ ì¤‘). ì´ í”„ë¡œì„¸ìŠ¤ë“¤ì„ ì¢…ë£Œí•˜ê³  ìˆ˜ì •ëœ ì½”ë“œë¡œ ë‹¤ì‹œ ëŒë¦¬ê² ìŠµë‹ˆë‹¤.

GPU ì „ë¶€ ë¹„ì›Œì¡ŒìŠµë‹ˆë‹¤. ì´ì œ ìˆ˜ì •ëœ ì½”ë“œë¡œ 3ê°œ ëª¨ë¸ì„ ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤.

Lingshu-7Bê°€ ì´ë¯¸ ì™„ë£Œë˜ì—ˆê³  Qwen2.5-VLì´ ì§„í–‰ ì¤‘ì…ë‹ˆë‹¤! Lingshu ê²°ê³¼:
- **Pneumonia**: 0.333 (3 turns)
- **Meningitis**: 0.429 (4 turns)  
- **Appendicitis**: 0.143 (5 turns)
- **Drug interaction**: 0.667 (3 turns)
- **Critical vitals**: 0.000 (3 turns)
- **í‰ê·  Action Score: 0.314**

ê³„ì† ëª¨ë‹ˆí„°ë§í•©ë‹ˆë‹¤.

Qwen2.5-VLì´ ì™„ë£Œë˜ì—ˆê³  Qwen3-8B-Baseê°€ ì§„í–‰ ì¤‘ì…ë‹ˆë‹¤! 

**Qwen2.5-VL-7B ê²°ê³¼:**
- Pneumonia: 0.500 (7 turns)
- Meningitis: 0.571 (6 turns)
- Appendicitis: 0.571 (7 turns)
- Drug interaction: 0.667 (3 turns)
- Critical vitals: 0.250 (4 turns)
- **í‰ê·  Action Score: 0.512** ğŸ”¥

ê·¸ë¦¬ê³  Qwen3-8B-BaseëŠ” `think`, `order_lab_test`, `prescribe_medication` ë“± ë” ë‹¤ì–‘í•œ toolì„ ì‚¬ìš©í•˜ê³  ìˆìŠµë‹ˆë‹¤! nudgeë„ ì˜ ì‘ë™í•©ë‹ˆë‹¤.

Qwen3-8B-Baseê°€ ë†€ë¼ìš´ ê²°ê³¼ë¥¼ ë³´ì—¬ì£¼ê³  ìˆìŠµë‹ˆë‹¤:
- **Pneumonia: 1.000** (15 turns, ì™„ë²½í•œ ì§„ë‹¨ + ì²˜ë°© + ê¸°ë¡!)
- **Meningitis: 0.857** (15 turns, Ceftriaxone ì²˜ë°©, lumbar puncture ì˜¤ë”)
- **Appendicitis: 0.714** (14 turns, ì§„ë‹¨ + ìˆ˜ìˆ  ì¤€ë¹„ + í•­ìƒì œ)

ğŸ‰ **ì „ì²´ ì‹¤í—˜ ì™„ë£Œ!** ìµœì¢… ë¹„êµ ê²°ê³¼:

| Model | Pneumonia | Meningitis | Appendicitis | Drug Interaction | Critical Vitals | **AVG** | Turns |
|---|---|---|---|---|---|---|---|
| Lingshu-7B | 0.333 | 0.429 | 0.143 | 0.667 | 0.000 | **0.314** | 3.6 |
| Qwen2.5-VL-7B | 0.500 | 0.571 | 0.571 | 0.667 | 0.250 | **0.512** | 5.4 |
| **Qwen3-8B-Base** | **1.000** | **0.857** | **0.714** | **1.000** | **0.750** | **0.864** | 14.6 |

ì´ì œ ê²°ê³¼ë¥¼ ê¸°ë¡í•˜ê³  ì¼ì¼ ë¡œê·¸ì— ì €ì¥í•©ë‹ˆë‹¤.

```markdown
# Daily Log: 2026-02-12 (Day 1)

## ì˜¤ëŠ˜ì˜ ëª©í‘œ
- [x] í”„ë¡œì íŠ¸ ì „ì²´ ë¦¬ì†ŒìŠ¤ ë¶„ì„ & PLANNING.md ì‘ì„±
- [x] ê²°ì •ì‚¬í•­ í™•ì • (ëª¨ë¸: Lingshu-7B ìš°ì„ , ë„ë©”ì¸: clinical_diagnosis ìš°ì„ , ë…¼ë¬¸: framework paper)
- [x] í”„ë¡œì íŠ¸ ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„±
- [ ] clinical_diagnosis ë„ë©”ì¸ core êµ¬í˜„ ì‹œì‘
  - [ ] data_model.py (DB ìŠ¤í‚¤ë§ˆ)
  - [ ] tools.py (Medical tools)
  - [ ] environment.py (í™˜ê²½)
  - [ ] policy.md (ì—ì´ì „íŠ¸ ì •ì±…)
  - [ ] db.json (ì‹œë®¬ë ˆì´ì…˜ DB)
  - [ ] tasks.json (í‰ê°€ task)
- [ ] base environment & tool framework êµ¬í˜„

## ì‘ì—… ê¸°ë¡

### 09:00 - í”„ë¡œì íŠ¸ ë¶„ì„ ì™„ë£Œ
- **ìˆ˜í–‰**: ì „ì²´ ë””ë ‰í† ë¦¬ íŠ¸ë¦¬ ë¶„ì„, ë³´ìœ  ë°ì´í„°ì…‹/ëª¨ë¸/í”„ë ˆì„ì›Œí¬ ì „ìˆ˜ ì¡°ì‚¬
- **ë°œê²¬ì‚¬í•­**:
  - Ï„Â²-bench ë„ë©”ì¸ íŒ¨í„´ì´ ë§¤ìš° ê¹”ë” (data_model â†’ tools â†’ environment êµ¬ì¡°)
  - AgentGym-RLì˜ verl ê¸°ë°˜ RL trainer í™œìš© ê°€ëŠ¥
  - tool-dataset-generationì˜ tool simulation ì—”ì§„ ì¬í™œìš© ê°€ëŠ¥
  - MRPOì˜ BERTScore+ROUGE reward ì²´ê³„ ì°¸ê³  ê°€ëŠ¥
  - databases/instructionì— 52k+ ì˜ë£Œ instruction ë°ì´í„° ë³´ìœ 
- **ì‚°ì¶œë¬¼**: `PLANNING.md` (ì „ì²´ ê¸°íš ë¬¸ì„œ)

### 09:30 - ê²°ì •ì‚¬í•­ í™•ì •
- ì£¼ ëª¨ë¸: Lingshu-7B â†’ Qwen2.5-VL-7B ìˆœì„œ
- ë„ë©”ì¸: clinical_diagnosis ë¨¼ì €
- ë…¼ë¬¸: framework paper
- ì¼ì¼ ë¡œê·¸ ì‹œìŠ¤í…œ: logs/daily/YYYY-MM-DD.md

### 10:00 - ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„±
```
bioagents/                    # ë©”ì¸ íŒ¨í‚¤ì§€
â”œâ”€â”€ domains/                  # ë„ë©”ì¸ë³„ ì½”ë“œ
â”‚   â”œâ”€â”€ clinical_diagnosis/   # ì„ìƒ ì§„ë‹¨ ë„ë©”ì¸
â”‚   â”œâ”€â”€ medical_qa/           # ì˜ë£Œ QA ë„ë©”ì¸
â”‚   â”œâ”€â”€ visual_diagnosis/     # ì‹œê° ì§„ë‹¨ ë„ë©”ì¸
â”‚   â”œâ”€â”€ ehr_management/       # EHR ê´€ë¦¬ ë„ë©”ì¸
â”‚   â””â”€â”€ drug_interaction/     # ì•½ë¬¼ ìƒí˜¸ì‘ìš© ë„ë©”ì¸
â”œâ”€â”€ gym/                      # Gymnasium ì¸í„°í˜ì´ìŠ¤
â”œâ”€â”€ environment/              # í™˜ê²½ ë² ì´ìŠ¤ í´ë˜ìŠ¤
â”œâ”€â”€ tools/                    # ë„êµ¬ ë² ì´ìŠ¤ í´ë˜ìŠ¤
â”œâ”€â”€ evaluation/               # í‰ê°€ ëª¨ë“ˆ
â”œâ”€â”€ training/                 # í•™ìŠµ ëª¨ë“ˆ
â””â”€â”€ utils/                    # ìœ í‹¸ë¦¬í‹°
data/domains/                 # ë„ë©”ì¸ë³„ ë°ì´í„° (db.json, tasks.json, policy.md)
configs/                      # í•™ìŠµ/í‰ê°€ ì„¤ì •
checkpoints/                  # ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸
logs/daily/                   # ì¼ì¼ ì‘ì—… ë¡œê·¸
```

### 10:30 - clinical_diagnosis ë„ë©”ì¸ êµ¬í˜„
- `data_model.py` â€“ Patient, LabResult, VitalSigns, Medication, ClinicalNote, DrugInteraction, ClinicalGuideline, ClinicalDB
- `tools.py` â€“ 17ê°œ ë„êµ¬ êµ¬í˜„ (READ 11, WRITE 4, THINK 0, GENERIC 2)
- `environment.py` â€“ get_environment(), get_tasks()
- `data/domains/clinical_diagnosis/db.json` â€“ í™˜ì 3ëª… (P001 íë ´, P002 ìˆ˜ë§‰ì—¼, P003 ì¶©ìˆ˜ì—¼), ì•½ë¬¼ìƒí˜¸ì‘ìš© 6ê±´, ê°€ì´ë“œë¼ì¸ 3ê±´
- `data/domains/clinical_diagnosis/policy.md` â€“ ì—ì´ì „íŠ¸ í–‰ë™ ì •ì±… ë¬¸ì„œ
- `data/domains/clinical_diagnosis/tasks.json` â€“ 5ê°œ í‰ê°€ task

### 11:00 - Base Framework êµ¬í˜„
- `bioagents/environment/db.py` â€“ DB ë² ì´ìŠ¤ í´ë˜ìŠ¤ (load/dump/hash)
- `bioagents/environment/toolkit.py` â€“ ToolKitBase, @is_tool decorator, ToolDefinition (OpenAI í˜¸í™˜)
- `bioagents/environment/environment.py` â€“ Environment (reset/step/execute_tool)

### 11:30 - GYM ì¸í„°í˜ì´ìŠ¤ êµ¬í˜„
- `bioagents/gym/agent_env.py` â€“ BioAgentGymEnv (Gymnasium í˜¸í™˜)
- observation/action: Text space
- ë„ë©”ì¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬, ìë™ reward ê³„ì‚° (ACTION + NL_ASSERTION)

### 12:00 - í…ŒìŠ¤íŠ¸ í†µê³¼ í™•ì¸
- `tests/test_clinical_diagnosis.py` â€“ 5ê°œ í…ŒìŠ¤íŠ¸ ëª¨ë“ˆ ì „ë¶€ PASS
  - DB loading, Tool execution (17 tools), Environment, Tasks, GYM interface
  - reward=1.000 (perfect action score on dx_pneumonia_001)

### 12:30 - í™˜ê²½ & ì˜ì¡´ì„± ê´€ë¦¬ êµ¬ì¶•
- **uv 0.10.2** ì„¤ì¹˜, `.venv` ê°€ìƒí™˜ê²½ ìƒì„± (Python 3.12.4)
- `pyproject.toml` ì‘ì„± â€“ í”„ë¡œì íŠ¸ ë©”íƒ€ë°ì´í„° + ì „ì²´ ì˜ì¡´ì„± ì •ì˜
- í•µì‹¬ íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì™„ë£Œ:
  - torch 2.9.1+cu128 (CUDA âœ“)
  - transformers 4.57.6
  - trl 0.28.0
  - gymnasium 1.2.3
  - vllm 0.15.1
  - deepspeed, peft, accelerate, wandb ë“±
- `requirements.lock` â€“ 203ê°œ íŒ¨í‚¤ì§€ ë²„ì „ ê³ ì •
- `pip install -e .` (editable mode) â†’ ìƒˆ í™˜ê²½ì—ì„œ ì „ í…ŒìŠ¤íŠ¸ ì¬í†µê³¼ í™•ì¸

---

## í™˜ê²½ ì •ë³´
```
Python 3.12.4 (.venv via uv)
torch 2.9.1+cu128 (CUDA enabled)
transformers 4.57.6
trl 0.28.0 (GRPO/PPO)
gymnasium 1.2.3
vllm 0.15.1
203 packages total (requirements.lock)
```

## ì‚¬ìš© ë¦¬ì†ŒìŠ¤
- ì°¸ê³  ì½”ë“œ: databases/tau2-bench/src/tau2/domains/mock/
- ì°¸ê³  ì½”ë“œ: tool_simulations/tool-dataset-generation/utils/tool_simulation/prompt.py

### 13:00 - LLM Agent Runner êµ¬í˜„ & ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
- `bioagents/evaluation/agent_runner.py` â€“ AgentRunner (multi-turn tool-use ë£¨í”„)
  - transformers / vLLM ë°±ì—”ë“œ ì§€ì›
  - ìë™ tool call íŒŒì‹± (JSON, code block, nested JSON)
  - ë°˜ë³µ tool call ê°ì§€ â†’ nudge ì£¼ì…
  - ì „ì²´ trajectory ë¡œê¹… (per-turn raw_output, parsed_tool_call, tool_response)
- `scripts/run_all_models.py` â€“ 3ëª¨ë¸ í†µí•© ì‹¤í—˜ ìŠ¤í¬ë¦½íŠ¸
- ëª¨ë¸ ë‹¤ìš´ë¡œë“œ (HuggingFace â†’ `checkpoints/models/`):
  - Qwen2.5-VL-7B-Instruct (16GB, 5 shards) âœ“
  - Lingshu-7B (13GB, 4 shards) âœ“
- flash-attn GLIBC í˜¸í™˜ ë¬¸ì œ â†’ sdpa attention fallback ì ìš©

### 13:30 - í›„ë³´ ëª¨ë¸ 3ì¢… Baseline ì‹¤í—˜ ê²°ê³¼ ğŸ”¥

| Model | Pneumonia | Meningitis | Appendicitis | Drug Int. | Critical Vitals | **AVG** | Turns |
|---|---|---|---|---|---|---|---|
| Lingshu-7B (med VL) | 0.333 | 0.429 | 0.143 | 0.667 | 0.000 | **0.314** | 3.6 |
| Qwen2.5-VL-7B-Inst. | 0.500 | 0.571 | 0.571 | 0.667 | 0.250 | **0.512** | 5.4 |
| **Qwen3-8B-Base** | **1.000** | **0.857** | **0.714** | **1.000** | **0.750** | **0.864** | 14.6 |

**í•µì‹¬ ë°œê²¬:**
1. **Qwen3-8B-Baseê°€ ì••ë„ì  1ìœ„** (0.864) â€“ base ëª¨ë¸ì´ì§€ë§Œ chat template ìˆì–´ multi-turn ì˜ ìˆ˜í–‰
   - Pneumonia & Drug Interaction ì™„ë²½ ì ìˆ˜ (1.000)
   - `think`, `record_diagnosis`, `prescribe_medication`, `add_clinical_note` ë“± ë‹¤ì–‘í•œ tool í™œìš©
   - nudge ë©”ì»¤ë‹ˆì¦˜ìœ¼ë¡œ ë°˜ë³µ íƒˆì¶œ í›„ ë” ê¹Šì€ ì„ìƒ ì¶”ë¡  ìˆ˜í–‰
2. **Qwen2.5-VL-7B 2ìœ„** (0.512) â€“ instruction tuned VL ëª¨ë¸, ì ë‹¹í•œ tool ì‚¬ìš©
3. **Lingshu-7B 3ìœ„** (0.314) â€“ ì˜ë£Œ íŠ¹í™” ëª¨ë¸ì´ì§€ë§Œ tool calling ëŠ¥ë ¥ ë¶€ì¡±
   - ë°˜ë³µì ì¸ tool call íŒ¨í„´ (ê°™ì€ tool 13íšŒ ì—°ì† í˜¸ì¶œ)
   - ë¹ ë¥´ê²Œ final answer ë‚´ë¦¬ê±°ë‚˜ ë°˜ë³µì— ë¹ ì§€ëŠ” ê²½í–¥

**ì£¼ìš” ê°œì„  í¬ì¸íŠ¸:**
- Lingshu-7B: tool calling SFT í•„ìš” (ì˜ë£Œ ì§€ì‹ OK, tool use ëŠ¥ë ¥ ë¶€ì¡±)
- Qwen2.5-VL: ë” ë§ì€ turn ì‚¬ìš©í•˜ë„ë¡ ìœ ë„ í•„ìš”
- Qwen3-8B-Base: ì´ë¯¸ ì¢‹ì§€ë§Œ RL/GRPOë¡œ ë” ìµœì í™” ê°€ëŠ¥

**ì‹¤í—˜ ë¡œê·¸ ìœ„ì¹˜:**
- `logs/runs/Lingshu-7B_clinical_diagnosis_20260212_*/`
- `logs/runs/Qwen2.5-VL-7B-Instruct_clinical_diagnosis_20260212_*/`
- `logs/runs/Qwen3-8B-Base_clinical_diagnosis_20260212_*/`
- `logs/runs/comparison_20260212_114756.json`

---

## í™˜ê²½ ì—…ë°ì´íŠ¸
```
flash-attn 2.8.3 (GLIBC í˜¸í™˜ ë¬¸ì œë¡œ sdpa fallback ì‚¬ìš©)
Qwen2.5-VL-7B-Instruct (checkpoints/models/)
Lingshu-7B (checkpoints/models/)
Qwen3-8B-Base (/data/project/private/minstar/models/)
```

## ë‚´ì¼ í•  ì¼
- Qwen3-8B-Base ê¸°ë°˜ SFT í•™ìŠµ (clinical_diagnosis trajectory ë°ì´í„°)
- medical_qa ë„ë©”ì¸ êµ¬í˜„
- LLM-as-Judge NL_ASSERTION reward êµ¬í˜„
- GRPO íŒŒì´í”„ë¼ì¸ êµ¬ì¶• ì‹œì‘
- Lingshu-7B tool calling SFT ì‹¤í—˜

```

ì´ì œ Step3-VL-10Bë„ ì‹œë„í•´ë³´ê² ìŠµë‹ˆë‹¤. ë¨¼ì € ë‹¤ìš´ë¡œë“œ ê°€ëŠ¥í•œì§€ í™•ì¸í•©ë‹ˆë‹¤.

Step3-VL-10B ë‹¤ìš´ë¡œë“œ ì‹œì‘. ë‹¤ìš´ë¡œë“œë¥¼ ê¸°ë‹¤ë¦¬ë©´ì„œ Qwen3-8B-Baseì˜ ìƒì„¸ trajectoryë¥¼ ë¶„ì„í•˜ê² ìŠµë‹ˆë‹¤.

Qwen3-8B-Baseì˜ trajectoryê°€ ë§¤ìš° ì¸ìƒì ì…ë‹ˆë‹¤. ì‹¤ì œ ì˜ì‚¬ workflowì™€ ìœ ì‚¬í•©ë‹ˆë‹¤.

Step3-VL-10B ë‹¤ìš´ë¡œë“œ ìƒíƒœë¥¼ í™•ì¸í•©ë‹ˆë‹¤.

ì•„ì§ safetensors weight ë‹¤ìš´ë¡œë“œ ì¤‘. ë‹¤ìš´ë¡œë“œë¥¼ ê¸°ë‹¤ë¦¬ë©´ì„œ requirements.lockì„ ì—…ë°ì´íŠ¸í•˜ê³ , Step3-VL-10Bê°€ ë‹¤ìš´ë˜ë©´ ë°”ë¡œ ì‹¤í—˜í•  ìˆ˜ ìˆë„ë¡ ì¤€ë¹„í•©ë‹ˆë‹¤.

Step3-VLì€ custom ì•„í‚¤í…ì²˜(`step_robotics`)ì´ë¯€ë¡œ loading ë¡œì§ì„ ì¡°ì •í•©ë‹ˆë‹¤.

```python
"""LLM Agent Runner for BIOAgents.

Connects a language model to the BIOAgents environment for multi-turn
tool-use evaluation. Supports:
- vLLM-based fast inference
- HuggingFace transformers-based inference
- Multi-turn tool calling with automatic parsing
- Full trajectory logging

Usage:
    runner = AgentRunner(
        model_name_or_path="Qwen/Qwen2.5-VL-7B-Instruct",
        backend="vllm",
    )
    results = runner.run_task(domain="clinical_diagnosis", task_id="dx_pneumonia_001")
"""

import json
import os
import re
import time
from copy import deepcopy
from dataclasses import dataclass, field
from datetime import datetime
from pathlib import Path
from typing import Any, Literal, Optional

from loguru import logger


@dataclass
class RunConfig:
    """Configuration for an agent run."""
    model_name_or_path: str
    backend: Literal["vllm", "transformers"] = "transformers"
    domain: str = "clinical_diagnosis"
    task_ids: Optional[list[str]] = None       # None = run all tasks
    task_split: Optional[str] = None
    max_turns: int = 15
    temperature: float = 0.1
    top_p: float = 0.95
    max_new_tokens: int = 1024
    tensor_parallel_size: int = 1
    gpu_memory_utilization: float = 0.85
    log_dir: str = "logs/runs"
    seed: int = 42


@dataclass
class TurnRecord:
    """Record of a single turn in the agent-environment interaction."""
    turn_idx: int
    prompt: str = ""
    raw_output: str = ""
    parsed_tool_call: Optional[dict] = None
    tool_response: Optional[str] = None
    is_final_answer: bool = False
    latency_seconds: float = 0.0


@dataclass 
class TaskResult:
    """Result of running a single task."""
    task_id: str
    domain: str
    model_name: str
    turns: list[TurnRecord] = field(default_factory=list)
    total_turns: int = 0
    action_score: float = 0.0
    final_reward: float = 0.0
    completed: bool = False
    error: Optional[str] = None
    trajectory: dict = field(default_factory=dict)
    start_time: str = ""
    end_time: str = ""
    total_latency: float = 0.0


def build_system_prompt(policy: str, tools: list[dict]) -> str:
    """Build the system prompt with policy and tool definitions."""
    tool_section = json.dumps(tools, indent=2, ensure_ascii=False)
    
    return f"""You are a medical AI assistant operating in a clinical environment. Follow the policy below and use the available tools to help with patient care.

## Policy
{policy}

## Available Tools
You have access to the following tools. To use a tool, respond with ONLY a JSON object in this exact format:
```json
{{"name": "tool_name", "arguments": {{"arg1": "value1", "arg2": "value2"}}}}
```

Do NOT include any other text when making a tool call. One tool call per response.

When you have gathered enough information and want to give your final assessment, respond with your clinical analysis as plain text (no JSON).

## Tool Definitions
{tool_section}"""


def parse_tool_call(text: str) -> Optional[dict]:
    """Parse a tool call from model output.
    
    Supports formats:
    1. Pure JSON: {"name": "...", "arguments": {...}}
    2. JSON in code block: ```json\n{...}\n```
    3. JSON embedded in text with markers
    """
    text = text.strip()
    
    # Try 1: Extract JSON from code blocks
    code_block_match = re.search(r'```(?:json)?\s*\n?({.*?})\s*\n?```', text, re.DOTALL)
    if code_block_match:
        try:
            parsed = json.loads(code_block_match.group(1))
            if "name" in parsed:
                return parsed
        except json.JSONDecodeError:
            pass
    
    # Try 2: Direct JSON parse
    try:
        parsed = json.loads(text)
        if isinstance(parsed, dict) and "name" in parsed:
            return parsed
    except json.JSONDecodeError:
        pass
    
    # Try 3: Find JSON-like pattern in text
    json_match = re.search(r'(\{[^{}]*"name"\s*:\s*"[^"]+?"[^{}]*"arguments"\s*:\s*\{[^{}]*\}[^{}]*\})', text, re.DOTALL)
    if json_match:
        try:
            parsed = json.loads(json_match.group(1))
            if "name" in parsed:
                return parsed
        except json.JSONDecodeError:
            pass
    
    # Try 4: More lenient nested JSON search
    matches = list(re.finditer(r'\{', text))
    for m in matches:
        start = m.start()
        depth = 0
        for i in range(start, len(text)):
            if text[i] == '{':
                depth += 1
            elif text[i] == '}':
                depth -= 1
                if depth == 0:
                    candidate = text[start:i+1]
                    try:
                        parsed = json.loads(candidate)
                        if isinstance(parsed, dict) and "name" in parsed:
                            return parsed
                    except json.JSONDecodeError:
                        pass
                    break
    
    return None


class AgentRunner:
    """Runs LLM agents in BIOAgents environments."""
    
    def __init__(self, config: RunConfig):
        self.config = config
        self.model = None
        self.tokenizer = None
        self._setup_logging()
    
    def _setup_logging(self):
        """Set up logging directory."""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        model_short = Path(self.config.model_name_or_path).name
        self.run_id = f"{model_short}_{self.config.domain}_{timestamp}"
        self.log_path = Path(self.config.log_dir) / self.run_id
        self.log_path.mkdir(parents=True, exist_ok=True)
        
        # Save config
        config_dict = {
            k: v for k, v in self.config.__dict__.items()
        }
        with open(self.log_path / "config.json", "w") as f:
            json.dump(config_dict, f, indent=2, default=str)
    
    def load_model(self):
        """Load the language model."""
        if self.config.backend == "vllm":
            self._load_vllm()
        else:
            self._load_transformers()
    
    def _load_vllm(self):
        """Load model with vLLM."""
        from vllm import LLM, SamplingParams
        
        logger.info(f"Loading {self.config.model_name_or_path} with vLLM (tp={self.config.tensor_parallel_size})")
        self.model = LLM(
            model=self.config.model_name_or_path,
            tensor_parallel_size=self.config.tensor_parallel_size,
            gpu_memory_utilization=self.config.gpu_memory_utilization,
            trust_remote_code=True,
            max_model_len=8192,
            seed=self.config.seed,
        )
        self.sampling_params = SamplingParams(
            temperature=self.config.temperature,
            top_p=self.config.top_p,
            max_tokens=self.config.max_new_tokens,
            stop=["```\n\n", "\n\nUser:", "\n\nHuman:"],
        )
        logger.info("vLLM model loaded successfully")
    
    def _load_transformers(self):
        """Load model with HuggingFace transformers."""
        import torch
        from transformers import (
            AutoModelForCausalLM,
            AutoTokenizer,
            AutoProcessor,
            AutoConfig,
        )
        
        logger.info(f"Loading {self.config.model_name_or_path} with transformers")
        
        model_path = self.config.model_name_or_path
        
        # Detect model type from config
        config = AutoConfig.from_pretrained(model_path, trust_remote_code=True)
        model_type = getattr(config, "model_type", "")
        architectures = getattr(config, "architectures", [])
        
        self._is_vl_model = any(
            "vl" in a.lower() or "vision" in a.lower()
            for a in (architectures or [])
        ) or "vl" in model_type.lower()
        
        logger.info(f"Model type: {model_type}, VL: {self._is_vl_model}, Arch: {architectures}")
        
        # Load tokenizer / processor
        if self._is_vl_model:
            try:
                self.processor = AutoProcessor.from_pretrained(
                    model_path, trust_remote_code=True
                )
                self.tokenizer = (
                    self.processor.tokenizer
                    if hasattr(self.processor, "tokenizer")
                    else self.processor
                )
            except Exception:
                self.tokenizer = AutoTokenizer.from_pretrained(
                    model_path, trust_remote_code=True
                )
                self.processor = None
        else:
            self.tokenizer = AutoTokenizer.from_pretrained(
                model_path, trust_remote_code=True
            )
            self.processor = None
        
        # Load model â€“ choose the right Auto class
        load_kwargs = dict(
            torch_dtype=torch.bfloat16,
            device_map="auto",
            trust_remote_code=True,
        )
        
        # Determine model class based on architecture
        auto_map = getattr(config, "auto_map", {})
        uses_custom_auto = "AutoModelForCausalLM" in auto_map
        is_qwen_vl = model_type in ("qwen2_5_vl", "qwen2_vl")
        
        # Try sdpa first, then fall back
        for attn_impl in ["sdpa", None]:
            try:
                kw = {**load_kwargs}
                if attn_impl:
                    kw["attn_implementation"] = attn_impl
                
                if uses_custom_auto or not is_qwen_vl:
                    # Custom model or standard CausalLM
                    self.model = AutoModelForCausalLM.from_pretrained(
                        model_path, **kw
                    )
                else:
                    # Qwen VL models need specific class
                    from transformers import Qwen2_5_VLForConditionalGeneration
                    self.model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
                        model_path, **kw
                    )
                break
            except Exception as e:
                logger.warning(f"Failed with attn_impl={attn_impl}: {e}")
                if attn_impl is None:
                    raise
        
        self.model.eval()
        logger.info(f"Model loaded (VL={self._is_vl_model})")
    
    def generate(self, messages: list[dict]) -> str:
        """Generate a response from the model."""
        if self.config.backend == "vllm":
            return self._generate_vllm(messages)
        else:
            return self._generate_transformers(messages)
    
    def _generate_vllm(self, messages: list[dict]) -> str:
        """Generate with vLLM using chat template."""
        from vllm import SamplingParams
        
        # Use the tokenizer from vLLM engine
        tokenizer = self.model.get_tokenizer()
        prompt = tokenizer.apply_chat_template(
            messages, tokenize=False, add_generation_prompt=True
        )
        
        outputs = self.model.generate([prompt], self.sampling_params)
        return outputs[0].outputs[0].text.strip()
    
    def _generate_transformers(self, messages: list[dict]) -> str:
        """Generate with HuggingFace transformers."""
        import torch
        
        # For VL models, use processor if available
        if self._is_vl_model and self.processor is not None:
            # Text-only input for VL model
            text = self.processor.apply_chat_template(
                messages, tokenize=False, add_generation_prompt=True
            )
            inputs = self.processor(
                text=[text], padding=True, return_tensors="pt"
            ).to(self.model.device)
        else:
            # Apply chat template
            if hasattr(self.tokenizer, "apply_chat_template"):
                text = self.tokenizer.apply_chat_template(
                    messages, tokenize=False, add_generation_prompt=True
                )
            else:
                text = ""
                for msg in messages:
                    role = msg["role"]
                    content = msg["content"]
                    text += f"<|{role}|>\n{content}\n"
                text += "<|assistant|>\n"
            
            inputs = self.tokenizer(text, return_tensors="pt").to(self.model.device)
        
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=self.config.max_new_tokens,
                temperature=self.config.temperature if self.config.temperature > 0 else None,
                top_p=self.config.top_p if self.config.temperature > 0 else None,
                do_sample=self.config.temperature > 0,
                pad_token_id=self.tokenizer.eos_token_id,
            )
        
        # Decode only the generated tokens
        input_len = inputs["input_ids"].shape[-1]
        generated_ids = outputs[0][input_len:]
        return self.tokenizer.decode(generated_ids, skip_special_tokens=True).strip()
    
    def run_task(self, task: dict, env) -> TaskResult:
        """Run a single task in the environment.
        
        Args:
            task: Task dictionary from tasks.json
            env: BioAgentGymEnv instance
            
        Returns:
            TaskResult with full trajectory and scores
        """
        task_id = task["id"]
        result = TaskResult(
            task_id=task_id,
            domain=self.config.domain,
            model_name=Path(self.config.model_name_or_path).name,
            start_time=datetime.now().isoformat(),
        )
        
        logger.info(f"Starting task: {task_id}")
        
        # Reset environment
        obs, info = env.reset(options={"task_id": task_id})
        
        # Build conversation
        system_prompt = build_system_prompt(info["policy"], info["tools"])
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": obs},
        ]
        
        try:
            for turn_idx in range(self.config.max_turns):
                turn = TurnRecord(turn_idx=turn_idx)
                
                # Generate
                t0 = time.time()
                raw_output = self.generate(messages)
                turn.latency_seconds = time.time() - t0
                turn.raw_output = raw_output
                
                logger.debug(f"Turn {turn_idx}: {raw_output[:200]}...")
                
                # Parse tool call
                tool_call = parse_tool_call(raw_output)
                
                if tool_call is not None:
                    turn.parsed_tool_call = tool_call
                    
                    # Detect repeated tool call (stuck model)
                    recent_tool_calls = [
                        t.parsed_tool_call.get("name", "") if t.parsed_tool_call else ""
                        for t in result.turns[-3:]
                    ]
                    if recent_tool_calls.count(tool_call["name"]) >= 2:
                        logger.warning(
                            f"Tool '{tool_call['name']}' called 3+ times in a row. "
                            "Injecting nudge to move forward."
                        )
                        # Instead of stopping, nudge the model
                        messages.append({"role": "assistant", "content": json.dumps(tool_call)})
                        messages.append({
                            "role": "user",
                            "content": (
                                f"You have already called '{tool_call['name']}' multiple times with similar arguments. "
                                "Please proceed with your analysis using the information gathered so far. "
                                "Use a DIFFERENT tool or provide your final clinical assessment."
                            ),
                        })
                        result.turns.append(turn)
                        continue
                    
                    # Execute tool via environment
                    action = json.dumps(tool_call)
                    observation, reward, terminated, truncated, step_info = env.step(action)
                    
                    turn.tool_response = observation
                    
                    # Add to messages - only the tool call JSON part
                    messages.append({"role": "assistant", "content": json.dumps(tool_call)})
                    messages.append({
                        "role": "user",
                        "content": f"Tool result for {tool_call['name']}:\n{observation}"
                    })
                    
                    result.turns.append(turn)
                    
                    if terminated or truncated:
                        break
                else:
                    # Final answer (no tool call)
                    turn.is_final_answer = True
                    messages.append({"role": "assistant", "content": raw_output})
                    result.turns.append(turn)
                    
                    # Check for repetition (model stuck in a loop)
                    if len(result.turns) >= 3:
                        recent = [t.raw_output[:100] for t in result.turns[-3:]]
                        if len(set(recent)) == 1:
                            logger.warning(f"Repetition detected at turn {turn_idx}, stopping.")
                            break
                    
                    # Agent gave final answer - break the loop
                    break
            
            # Get final trajectory and reward
            try:
                trajectory = env.get_trajectory()
                result.trajectory = trajectory
                result.final_reward = trajectory["final_reward"]
            except Exception:
                result.trajectory = {}
                result.final_reward = 0.0
            
            result.total_turns = len(result.turns)
            result.action_score = self._compute_action_score(task, env._tool_call_log)
            result.completed = True
            
        except Exception as e:
            logger.error(f"Error in task {task_id}: {e}")
            result.error = str(e)
            import traceback
            result.error += "\n" + traceback.format_exc()
        
        result.end_time = datetime.now().isoformat()
        result.total_latency = sum(t.latency_seconds for t in result.turns)
        
        logger.info(
            f"Task {task_id}: turns={result.total_turns}, "
            f"action_score={result.action_score:.3f}, "
            f"reward={result.final_reward:.3f}, "
            f"latency={result.total_latency:.1f}s"
        )
        
        return result
    
    def _compute_action_score(self, task: dict, tool_call_log: list) -> float:
        """Compute action score based on expected vs actual tool calls."""
        eval_criteria = task.get("evaluation_criteria", {})
        expected_actions = eval_criteria.get("actions", [])
        
        if not expected_actions:
            return 1.0
        
        matched = 0
        for exp in expected_actions:
            exp_name = exp.get("name", "")
            compare_args = exp.get("compare_args", [])
            exp_args = exp.get("arguments", {})
            
            for tc in tool_call_log:
                if tc["tool_name"] == exp_name:
                    if compare_args:
                        all_match = all(
                            str(tc["arguments"].get(k, "")).lower() == str(exp_args.get(k, "")).lower()
                            for k in compare_args
                            if k in exp_args
                        )
                        if all_match:
                            matched += 1
                            break
                    else:
                        matched += 1
                        break
        
        return matched / len(expected_actions)
    
    def run_all_tasks(self) -> list[TaskResult]:
        """Run all tasks (or specified task_ids) and return results."""
        from bioagents.gym.agent_env import BioAgentGymEnv
        
        # Create environment
        gym_env = BioAgentGymEnv(
            domain=self.config.domain,
            task_split=self.config.task_split,
            max_turns=self.config.max_turns,
        )
        
        # Get tasks
        if self.config.task_ids:
            task_ids = self.config.task_ids
        else:
            task_ids = [t["id"] for t in gym_env._tasks]
        
        logger.info(f"Running {len(task_ids)} tasks with {Path(self.config.model_name_or_path).name}")
        
        results = []
        for task_id in task_ids:
            task = gym_env._task_map[task_id]
            result = self.run_task(task, gym_env)
            results.append(result)
            self._save_task_result(result)
        
        # Save summary
        self._save_summary(results)
        
        return results
    
    def _save_task_result(self, result: TaskResult):
        """Save individual task result."""
        path = self.log_path / f"task_{result.task_id}.json"
        data = {
            "task_id": result.task_id,
            "domain": result.domain,
            "model_name": result.model_name,
            "total_turns": result.total_turns,
            "action_score": result.action_score,
            "final_reward": result.final_reward,
            "completed": result.completed,
            "error": result.error,
            "total_latency": result.total_latency,
            "start_time": result.start_time,
            "end_time": result.end_time,
            "turns": [
                {
                    "turn_idx": t.turn_idx,
                    "raw_output": t.raw_output,
                    "parsed_tool_call": t.parsed_tool_call,
                    "tool_response": t.tool_response[:500] if t.tool_response else None,
                    "is_final_answer": t.is_final_answer,
                    "latency_seconds": t.latency_seconds,
                }
                for t in result.turns
            ],
        }
        with open(path, "w", encoding="utf-8") as f:
            json.dump(data, f, indent=2, ensure_ascii=False, default=str)
    
    def _save_summary(self, results: list[TaskResult]):
        """Save a summary of all task results."""
        summary = {
            "run_id": self.run_id,
            "model": Path(self.config.model_name_or_path).name,
            "domain": self.config.domain,
            "backend": self.config.backend,
            "num_tasks": len(results),
            "num_completed": sum(1 for r in results if r.completed),
            "num_errors": sum(1 for r in results if r.error),
            "avg_action_score": sum(r.action_score for r in results) / max(len(results), 1),
            "avg_reward": sum(r.final_reward for r in results) / max(len(results), 1),
            "avg_turns": sum(r.total_turns for r in results) / max(len(results), 1),
            "total_latency": sum(r.total_latency for r in results),
            "per_task": [
                {
                    "task_id": r.task_id,
                    "action_score": r.action_score,
                    "final_reward": r.final_reward,
                    "turns": r.total_turns,
                    "latency": r.total_latency,
                    "completed": r.completed,
                    "error": r.error is not None,
                }
                for r in results
            ],
            "timestamp": datetime.now().isoformat(),
        }
        
        path = self.log_path / "summary.json"
        with open(path, "w", encoding="utf-8") as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)
        
        # Print summary table
        print("\n" + "=" * 80)
        print(f"  RUN SUMMARY: {self.run_id}")
        print("=" * 80)
        print(f"  Model: {summary['model']}")
        print(f"  Domain: {summary['domain']}")
        print(f"  Backend: {summary['backend']}")
        print(f"  Tasks: {summary['num_completed']}/{summary['num_tasks']} completed")
        print(f"  Avg Action Score: {summary['avg_action_score']:.3f}")
        print(f"  Avg Reward: {summary['avg_reward']:.3f}")
        print(f"  Avg Turns: {summary['avg_turns']:.1f}")
        print(f"  Total Latency: {summary['total_latency']:.1f}s")
        print("-" * 80)
        print(f"  {'Task ID':<30} {'Score':>8} {'Reward':>8} {'Turns':>6} {'Time':>8}")
        print("-" * 80)
        for t in summary["per_task"]:
            status = "âœ“" if t["completed"] else "âœ—"
            print(f"  {status} {t['task_id']:<28} {t['action_score']:>8.3f} {t['final_reward']:>8.3f} {t['turns']:>6} {t['latency']:>7.1f}s")
        print("=" * 80)
        
        logger.info(f"Results saved to {self.log_path}")

```

Step3-VL-10Bë¥¼ `run_all_models.py`ì—ë„ ì¶”ê°€í•©ë‹ˆë‹¤.

```python
#!/usr/bin/env python3
"""Run all candidate models on clinical_diagnosis tasks.

Models:
  1. Lingshu-7B (medical specialized VL model) â†’ GPU 0
  2. Qwen2.5-VL-7B-Instruct (general VL model) â†’ GPU 1
  3. Qwen3-8B-Base (text-only baseline) â†’ GPU 2

Each runs sequentially to avoid GPU OOM. Results are saved to logs/runs/.
"""

import json
import os
import sys
import time
from datetime import datetime
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent.parent))

from bioagents.evaluation.agent_runner import AgentRunner, RunConfig
from loguru import logger

PROJECT_ROOT = Path(__file__).parent.parent

MODELS = [
    {
        "name": "Lingshu-7B",
        "path": str(PROJECT_ROOT / "checkpoints/models/Lingshu-7B"),
        "backend": "transformers",
        "gpu": "0,1",
        "notes": "Medical specialized VL model (Qwen2.5-VL based)",
    },
    {
        "name": "Qwen2.5-VL-7B-Instruct",
        "path": str(PROJECT_ROOT / "checkpoints/models/Qwen2.5-VL-7B-Instruct"),
        "backend": "transformers",
        "gpu": "2,3",
        "notes": "General purpose VL model",
    },
    {
        "name": "Qwen3-8B-Base",
        "path": "/data/project/private/minstar/models/Qwen3-8B-Base",
        "backend": "transformers",
        "gpu": "4,5",
        "notes": "Text-only base model (no instruction tuning)",
    },
    {
        "name": "Step3-VL-10B",
        "path": str(PROJECT_ROOT / "checkpoints/models/Step3-VL-10B"),
        "backend": "transformers",
        "gpu": "6,7",
        "notes": "Step3-VL-10B multimodal model (custom architecture)",
    },
]

ALL_TASK_IDS = [
    "dx_pneumonia_001",
    "dx_meningitis_001",
    "dx_appendicitis_001",
    "dx_drug_interaction_001",
    "dx_critical_vitals_001",
]


def run_model(model_info: dict, task_ids: list[str] = None):
    """Run a single model on all tasks."""
    name = model_info["name"]
    path = model_info["path"]
    
    # Check model exists
    if not Path(path).exists():
        logger.error(f"Model {name} not found at {path}")
        return None
    
    # Set GPU
    os.environ["CUDA_VISIBLE_DEVICES"] = model_info["gpu"]
    
    config = RunConfig(
        model_name_or_path=path,
        backend=model_info["backend"],
        domain="clinical_diagnosis",
        task_ids=task_ids or ALL_TASK_IDS,
        max_turns=15,
        temperature=0.1,
        top_p=0.95,
        max_new_tokens=1024,
        log_dir=str(PROJECT_ROOT / "logs" / "runs"),
    )
    
    print(f"\n{'='*70}")
    print(f"  Model: {name}")
    print(f"  Path: {path}")
    print(f"  GPU: {model_info['gpu']}")
    print(f"  Notes: {model_info['notes']}")
    print(f"  Tasks: {len(config.task_ids)}")
    print(f"  Time: {datetime.now().isoformat()}")
    print(f"{'='*70}\n")
    
    runner = AgentRunner(config)
    
    logger.info(f"Loading model: {name}")
    t0 = time.time()
    runner.load_model()
    load_time = time.time() - t0
    logger.info(f"Model loaded in {load_time:.1f}s")
    
    results = runner.run_all_tasks()
    
    # Clean up GPU memory
    import torch
    del runner.model
    if hasattr(runner, 'processor') and runner.processor is not None:
        del runner.processor
    if hasattr(runner, 'tokenizer'):
        del runner.tokenizer
    torch.cuda.empty_cache()
    import gc
    gc.collect()
    
    return {
        "name": name,
        "results": results,
        "load_time": load_time,
        "run_id": runner.run_id,
    }


def print_final_comparison(all_runs: list[dict]):
    """Print comparison table of all models."""
    print("\n\n" + "=" * 100)
    print("  FINAL COMPARISON - BIOAgents Clinical Diagnosis Baseline")
    print(f"  Date: {datetime.now().strftime('%Y-%m-%d %H:%M')}")
    print("=" * 100)
    
    # Header
    header = f"{'Model':<28}"
    for tid in ALL_TASK_IDS:
        short = tid.replace("dx_", "").replace("_001", "")
        header += f" {short:>12}"
    header += f" {'AVG':>8} {'Turns':>6}"
    print(header)
    print("-" * 100)
    
    comparison_data = []
    
    for run_info in all_runs:
        if run_info is None:
            continue
        name = run_info["name"]
        results = run_info["results"]
        
        row = f"{name:<28}"
        scores = []
        total_turns = []
        
        # Map results by task_id
        result_map = {r.task_id: r for r in results}
        
        for tid in ALL_TASK_IDS:
            r = result_map.get(tid)
            if r and r.completed:
                row += f" {r.action_score:>12.3f}"
                scores.append(r.action_score)
                total_turns.append(r.total_turns)
            else:
                row += f" {'ERR':>12}"
        
        avg_score = sum(scores) / max(len(scores), 1)
        avg_turns = sum(total_turns) / max(len(total_turns), 1)
        row += f" {avg_score:>8.3f} {avg_turns:>6.1f}"
        print(row)
        
        comparison_data.append({
            "model": name,
            "avg_action_score": avg_score,
            "avg_turns": avg_turns,
            "per_task": {
                r.task_id: {
                    "action_score": r.action_score,
                    "final_reward": r.final_reward,
                    "turns": r.total_turns,
                    "completed": r.completed,
                }
                for r in results
            },
            "load_time": run_info["load_time"],
            "run_id": run_info["run_id"],
        })
    
    print("=" * 100)
    
    # Save comparison
    comp_path = PROJECT_ROOT / "logs" / "runs" / f"comparison_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(comp_path, "w") as f:
        json.dump(comparison_data, f, indent=2, default=str)
    print(f"\nComparison saved to: {comp_path}")


def main():
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument("--models", nargs="+", default=None,
                        help="Model names to run (default: all)")
    parser.add_argument("--tasks", nargs="+", default=None,
                        help="Task IDs to run (default: all)")
    args = parser.parse_args()
    
    models_to_run = MODELS
    if args.models:
        models_to_run = [m for m in MODELS if m["name"] in args.models]
    
    task_ids = args.tasks
    
    print("=" * 70)
    print("  BIOAgents - Multi-Model Clinical Diagnosis Experiment")
    print(f"  Models: {[m['name'] for m in models_to_run]}")
    print(f"  Tasks: {task_ids or 'ALL'}")
    print(f"  Start: {datetime.now().isoformat()}")
    print("=" * 70)
    
    all_runs = []
    for model_info in models_to_run:
        try:
            result = run_model(model_info, task_ids)
            all_runs.append(result)
        except Exception as e:
            logger.error(f"Failed for {model_info['name']}: {e}")
            import traceback
            traceback.print_exc()
            all_runs.append(None)
    
    print_final_comparison(all_runs)
    
    print(f"\nAll experiments completed at {datetime.now().isoformat()}")


if __name__ == "__main__":
    main()

```

Step3-VL ë‹¤ìš´ë¡œë“œ ì§„í–‰ì„ ê¸°ë‹¤ë¦¬ë©´ì„œ requirements.lock ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.

Step3-VL-10B ë‹¤ìš´ë¡œë“œ ì™„ë£Œ (5/5 shards, 19GB)! ë°”ë¡œ ì‹¤í—˜í•©ë‹ˆë‹¤.

Step3-VL-10Bì— ì‹¬ê°í•œ weight loading ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤ - ëª¨ë“  language model weightì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•„ garbled outputì„ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤. ëª¨ë¸ ì½”ë“œë¥¼ í™•ì¸í•´ì„œ ì˜¬ë°”ë¥¸ ë¡œë”© ë°©ë²•ì„ ì°¾ê² ìŠµë‹ˆë‹¤.

`_checkpoint_conversion_mapping` ê¸°ëŠ¥ì´ ì œëŒ€ë¡œ ì‘ë™í•˜ì§€ ì•ŠëŠ” ê²ƒ ê°™ìŠµë‹ˆë‹¤. ì§ì ‘ weight ë§¤í•‘ì„ í•´ë³´ê² ìŠµë‹ˆë‹¤.

